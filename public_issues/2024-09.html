

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The CoVar Zeitgeist: September, 2024 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../issues/2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../issues/2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../issues/2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The CoVar Zeitgeist: September, 2024</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/public_issues/2024-09.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-covar-zeitgeist-september-2024">
<h1>The CoVar Zeitgeist: September, 2024<a class="headerlink" href="#the-covar-zeitgeist-september-2024" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.arxiv.org/pdf/2407.09468">Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures</a></dt><dd><p>An overview of how to do machine learning in non-Euclidean spaces.  This is a good reference.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11326">Automating Thought of Search: A Journey Towards Soundness and Completeness</a></dt><dd><p>Briefly reviews Thought of Search, a human-in-the-loop method which has a high success rate in using LLMs as planning agents.  This paper automates ToS so no human is necessary, while maintaining the high success rate.  Includes a discussion on how LLMs can reliably generate code.</p>
</dd>
<dt><a class="reference external" href="https://gonenhila.github.io/files/Semantic_Leakage.pdf">Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</a></dt><dd><p>Discovers a new failure case for LLMs called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny’s job is, it will say that Kenny is a school bus driver because school busses are yellow.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11804">Approaching Deep Learning through the Spectral Dynamics of Weights</a></dt><dd><p>Uses spectral weights to analyze deep neural networks. Claims to be able to distinguish “memorizing networks” from “generalizing networks”, as well as identifying “lottery tickets”, i.e. sparse networks with exceptional performance.  Could be a useful tool for network training/diagnostics.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07772">Out-of-Distribution Learning with Human Feedback</a></dt><dd><p>What’s the best way for a model to handle out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from “wild data”, uses human feedback to label them, and then trains a model to both classify and identify OOD objects.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.14837">DIFFUSION MODELS ARE REAL-TIME GAME ENGINES</a></dt><dd><p>A novel neural net, GameNGen, which can succesfully emulate and run a playbale version of the videogame Doom. Cool in and of itself, but there might be something here with having a neural net that will let you dynamically interact/move through 3D world models for reconnassiance/intelligence/training purposes.</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></dt><dd><p>This paper investigates how to best use a finite amount of test-time compute to optimize LLM performacne?.  Results are nuanced but worth reading.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.09025">SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models</a></dt><dd><p>Develops a method to have a spreadsheet be an LLM input.  In doing so, it demonstrates that representing data effectively to an LLM can be complicated.</p>
</dd>
<dt><a class="reference external" href="https://gonenhila.github.io/files/Semantic_Leakage.pdf">Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</a></dt><dd><p>Discovers a new failure case for LLMs called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny’s job is, it will say that Kenny is a school bus driver because school busses are yellow.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10914">To Code, or Not To Code? Exploring Impact of Code in Pre-training</a></dt><dd><p>Investigates the effects of including code in the training data for your LLM, and finds some interesting results: including code helps improve the performance of the LLM on other domains such as NLP reasoning and world knowledge.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.08210">Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models</a></dt><dd><p>Investigates whether LLMs can do reasoning by looking at how they handle necessary and sufficient conditions.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15545">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</a></dt><dd><p>A suite of LLMs to read scientic papers/abstracts and extract useful info in JSON format. Useful for anyone who wants an LLM to summarize information.</p>
</dd>
</dl>
</section>
<section id="vlms">
<h2>VLMs<a class="headerlink" href="#vlms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10188">LONGVILA: SCALING LONG-CONTEXT VISUAL LANGUAGE MODELS FOR LONG VIDEOS</a></dt><dd><p>Another innovation in the VILA family of models, LongVILA is capable of long video understanding.  The methodology is interesting and the paper is worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15998">EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</a></dt><dd><p>A fairly in-depth analysis of the many different ways you can make multimodal LLMs using different vision encoders.  The result is that simple methods can be as effective as complex ones.</p>
</dd>
</dl>
</section>
<section id="object-detection">
<h2>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.02555">MESHANYTHING V2: ARTIST-CREATED MESH GENERATION WITH ADJACENT MESH TOKENIZATION</a></dt><dd><p>A model which takes a variety of inputs such as point clouds, Gaussian Splats, images, or text and generates 3D meshes of the described object.  Could be useful for zero/one/few-shot learning in 3D models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.06190">4D Contrastive Superflows are Dense 3D Representation Learners</a></dt><dd><p>Proposes methods for 3D and 4D foundation models using LiDar and vision.  Potentially useful for 3D and 4D object detection.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2402.18673">Trends, Applications, and Challenges in Human Attention Modelling</a></dt><dd><p>The purpose of object detection is ultimately to aid human perception. This paper investigates how to guide human attention, and can be used to improve visualizations.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10195">SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</a></dt><dd><p>Proposes a model that can take a small set of sparse unposed views of an object and create a 3D mesh that object relatively quickly.  Could be useful for zero/one/few-shot learning using 3D models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10198">MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model</a></dt><dd><p>A novel method for 2D to 3D, sparse image to 3D, and text to 3D generation.  Could be useful for zero/one/few-shot learning using 3D models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.12671">Joint Image De-noising and Enhancement for Satellite-Based SAR</a></dt><dd><p>Develops a science-based algorithm for de-noising and enhancing remote sensing SAR imagery.  This sort of preprocessing is necessary for object detection.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.01384">NOLO: Navigate Only Look Once</a></dt><dd><p>Develops a transformer model that can autonomously navigate a drone based on input EO sensors.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11326">Automating Thought of Search: A Journey Towards Soundness and Completeness</a></dt><dd><p>Briefly reviews Thought of Search, a human-in-the-loop method which has a high success rate in using LLMs as planning agents.  This paper automates ToS so no human is necessary, while maintaining the high success rate.  Includes a discussion on how LLMs can reliably generate code.</p>
</dd>
</dl>
</section>
<section id="tracking">
<h2>Tracking<a class="headerlink" href="#tracking" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.21635">MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction</a></dt><dd><p>Uses relation transformers to do multi-agent tracking in basketball data.  This tracking method can be generalized to other contexts.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01223">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing</a></dt><dd><p>Combines 3D Gaussian splats with VLMs and physics-based models to enable both text-based scened decomposition and physics-based dynamics in a 3D Gaussian splatting model.  Enabling interactivity with 3D world models is a potentially useful capability.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07540">3D Gaussian Editing with A Single Image</a></dt><dd><p>Develops a method that allows you to take a Gaussian splat, compress it to one image, modify that one image, and then generate a novel Gaussian splat corresponding to the changed image.  Enabling interactivity with 3D world models is a potentially useful capability.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.08206">WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting</a></dt><dd><p>Novel 3D Gaussian Splatting approach for underwater scenes which can generalize to foggy/rainy scenes on dry land.  If standard techniques struggle in those settings, this is a good tool.</p>
</dd>
</dl>
</section>
<section id="computational-enhancement">
<h2>Computational Enhancement<a class="headerlink" href="#computational-enhancement" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03703">CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications</a></dt><dd><p>Develops a method to put vision transformers on iPhones. There is a lot of potential in using smart phones for object detection purposes.</p>
</dd>
<dt><a class="reference external" href="https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/">How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model</a></dt><dd><p>NVIDIA takes Llama-3.1 8B and turns it into a 4B parameter model with minimal decrease in performance.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10233">FPCA: FIELD-PROGRAMMABLE PIXEL CONVOLUTIONAL ARRAY FOR EXTREME-EDGE INTELLIGENCE</a></dt><dd><p>This is really really cool. The paper develops a method to have a re-programable circuit behind the pixels on the sensor. So at image capture they can run computations (convolutions). This means you could run super low-power, low-latency CNNs basically as you capture the image. This has been done before, but they’re demonstrating the re-programmable version so you can change the algorithm without remanufacturing the chip.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10189">Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</a></dt><dd><p>Develops a method to distill a tranformer to a SSM model.  The exact methodology is really interesting and worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15237">The Mamba in the Llama: Distilling and Accelerating Hybrid Models</a></dt><dd><p>Takes a transformer and distills it down to an RNN while maintaining performance.</p>
</dd>
</dl>
</section>
<section id="geometric-deep-learning">
<h2>Geometric Deep Learning<a class="headerlink" href="#geometric-deep-learning" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.arxiv.org/pdf/2407.09468">Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures</a></dt><dd><p>An overview of how to do machine learning in non-Euclidean spaces.  This is a good reference.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15894">The Role of Fibration Symmetries in Geometric Deep Learning</a></dt><dd><p>Geometric Deep Learning often relies on global symmetries for inference.  Global symmetries can be rare in practice, however, so this paper instead uses local symmetries to improve inference.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.00657">Disentangling Dense Embeddings with Sparse Autoencoders</a></dt><dd><p>A sparse autoencoder, applied to dense embeddings, can generate sparse embeddings that maintain semantic fidelity.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.04851">Your Classifier Can Be Secretly a Likelihood-Based OOD Detector</a></dt><dd><p>Develops a method in which classifiers can be used to out-of-distribution (OOD) detection.  Results seem promising.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07772">Out-of-Distribution Learning with Human Feedback</a></dt><dd><p>What’s the best way for a model to handle out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from “wild data”, uses human feedback to label them, and then trains a model to both classify and identify OOD objects.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11804">Approaching Deep Learning through the Spectral Dynamics of Weights</a></dt><dd><p>Analyzes deep neural nets from the context of spectral weights.  They claim to be able to distinguish “memorizing networks” from “generalizing networks”, which sounds important, as well as identifying “lottery tickets”, or sparse networks with exceptional performance.  This all sounds pretty grand, so maybe there’s a there there.  Worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.14319">Rethinking Knowledge Transfer in Learning Using Privileged Information</a></dt><dd><p>There exists a training method that attempts to supplement the training process with privieged information (PI) that is available only during training.  This paper investigates this paper and finds that that using PI this way has no theoretical or practical basis.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://link.springer.com/article/10.1007/s11192-024-04968-7">Do grant proposal texts matter for funding decisions? A field experiment</a></dt><dd><p>A dutch study finds that an abstract and CV hold as much weight as a full proposal. Your representation, connections, and elevator pitch are what matter.  I wonder if this generalizes to other countries/institutions?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11527">The Vizier Gaussian Process Bandit Algorithm</a></dt><dd><p>Google talks about some black-box optimization methods they’ve been employing internally for years.  Gaussian process based.  Provides production level code.  If we ever have to do black box optimization (Im not sure we do?) then this is the place to start.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.14837">DIFFUSION MODELS ARE REAL-TIME GAME ENGINES</a></dt><dd><p>A novel neural net, GameNGen, which can succesfully emulate and run a playbale version of the videogame Doom. Cool in and of itself, but there might be something here with having a neural net that will let you dynamically interact/move through 3D world models for reconnassiance/intelligence/training purposes.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/">Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma</a></dt><dd><p>Google adds three new additions to the Gemma 2B family.  They claim its the best thing on the market, etc etc.  <a class="reference external" href="https://arxiv.org/pdf/2408.00118">Lab report</a></p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.21075">Apple Intelligence Foundation Language Models</a></dt><dd><p>Apple’s lab report on its foundation models.  Probably something interesting here if you want to read it.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07009">Imagen 3</a></dt><dd><p>Text to image generation diffusion model from Google.  Maybe there’s a way to do synthetic data generation with this?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03326">LLaVA-OneVision: Easy Visual Task Transfer</a></dt><dd><p>ByteDance releases a family of open LLMs that “push the performance boundaries” in some computer vision tasks.  Using anything released by ByteDane is presumably a hard no for government work, but it comes with a blog detailing development that might be worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11039">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a></dt><dd><p>Meta’s new multi-model foundation model.  Can take text and images as part of the same input, as well as generating images.  Can handle complex(ish) instructions for image editting.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.12569">Sapiens: Foundation for Human Vision Models</a></dt><dd><p>Meta releases a new foundation model for computer vision focussing on humans.  Seems decent, but rather limited in scope and the examples have large numbers of pixels on target, so likely not to be much use for us.</p>
</dd>
</dl>
</section>
<section id="presented-at-covar-seminar">
<h2>Presented at CoVar Seminar<a class="headerlink" href="#presented-at-covar-seminar" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-08-06</dt><dd><p><a class="reference external" href="https://arxiv.org/pdf/2407.21787">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a>
In some paradigms, having an LLM generate an accurate answer is hard but verifying any given answer is easy.  If you are in one of those paradigms, you can have an LLM generate many answers and find the correct one.</p>
</dd>
<dt>2024-08-27</dt><dd><p><a class="reference external" href="https://arxiv.org/html/2402.01786v1">COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations</a>
DEVCOM Army Research Lab has developed a method to use GPT4-Turbo to generate courses of action (COAs) for friendy units using videogame-based simulators.  There is a lot of potential for these types of methods to aid military officers.</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>