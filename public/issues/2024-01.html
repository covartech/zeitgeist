

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2024-01 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2024-02" href="2024-02.html" />
    <link rel="prev" title="2023-12" href="2023-12.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2024-01</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-01.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>2024-01<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="neurips">
<h2>NeurIPS<a class="headerlink" href="#neurips" title="Permalink to this heading">¶</a></h2>
<section id="id2">
<h3><a class="reference external" href="https://neurips2023.vizhub.ai">NeurIPS 2023</a><a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>The conference is in December and this is a cool visualization to see trends. Lots to explore. Maybe next month.</p>
</section>
</section>
<hr class="docutils" />
<section id="gaussian-splatting-and-doctrinaire-related-things">
<h2>Gaussian Splatting and Doctrinaire Related Things<a class="headerlink" href="#gaussian-splatting-and-doctrinaire-related-things" title="Permalink to this heading">¶</a></h2>
<p>Gaussian splatting is very interesting and pretty practical. Lot’s of activity going on. Some of them are getting very close to rhyming with Doctrinaire.</p>
<p>Maybe start with  <a class="reference external" href="https://arxiv.org/abs/2312.13150">Splatter Image: Ultra-Fast Single-View 3D Reconstruction</a>. It’s like the Centernet of NERF. They make a “splatter image” that defines a 3D reconstruction, they can feed it to a Gaussian splatter renderer. From those small number channels they can render other views of that target. It’s kind of awesome. It seems super useful to me, just can’t quite figure out how yet.</p>
<p>Here are some others worth checking out or at least looking at the pictures.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://szymanowiczs.github.io/splatter-image">This paper with video</a></p></li>
<li><p><a class="reference external" href="https://shunyuanzheng.github.io/GPS-Gaussian">Splatting for people moving</a></p></li>
<li><p><a class="reference external" href="https://yuelangx.github.io/gaussianheadavatar/">Splatting for avatar heads</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2312.09147">Splatting with transformers baked in</a></p></li>
</ul>
<p>I think this one <a class="reference external" href="https://arxiv.org/abs/2312.09031">iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching</a> is the splatting paper I was looking for as Doctrinaire.</p>
<hr class="docutils" />
<section id="id3">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.05251.pdf">Reconstructing Hands in 3D with Transformers</a><a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>Implemented a transformer that can look at pictures of hands and output a 3D mesh.  Could be useful for Doctrinaire/TA2/whenever we want to use a CAD model</p>
</section>
</section>
<hr class="docutils" />
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<section id="id4">
<h3><a class="reference external" href="https://arxiv.org/pdf/2304.15004.pdf">Are Emergent Abilities of Large Language Models a Mirage?</a><a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>The authors claimed on twitter that this was best in show at NeurIPS, and no one would lie about that.  Argues that emergent behavior (as model scale increases) is just due to choice of metric rather than any underlying behavior.</p>
</section>
<hr class="docutils" />
<section id="id5">
<h3><a class="reference external" href="https://huggingface.co/blog/moe">Mixtral 8x7B Explained</a><a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<p>Mixtral is an interesting LLM company. They are French and seem to be pretty solid. This is how they do Mixture of Expert models (like GPT4). Instead of using one big model say 56GB we can actually use a mixture of 8 7B models. We end up saving a bit of VRAM space in the process and it works better. Something to keep an eye on.</p>
<p><a class="reference external" href="https://github.com/vllm-project/vllm/commit/b5f882cc98e2c9c6dde7357dbac2ec0c2c57d8cd">VLLM Implementation</a></p>
</section>
<hr class="docutils" />
<section id="id6">
<h3><a class="reference external" href="https://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a><a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h3>
<p>User: Who is Tom Cruise’s mother.</p>
<p>LLM: Mary Lee Pfiefer</p>
<p>User: Who’s son is Mary Lee Pfiefer</p>
<p>LLM: ?!?!? Dunno ?!?!?!</p>
<p>Nobody talks about that so how could the big memorizer memorize it.</p>
</section>
<hr class="docutils" />
<section id="id7">
<h3><a class="reference external" href="https://www.together.ai/blog/stripedhyena-7b">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a><a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p>What’s after transformers? Any ideas? Yeah there are some and sometimes they are better sometimes. It’s based on <a class="reference external" href="https://hazyresearch.stanford.edu/blog/2023-06-08-hyena-safari">signal processing inspired sequence models</a> which means that sometimes you use an FFT. Miles this sounds up your alley.</p>
</section>
<hr class="docutils" />
<section id="id8">
<h3><a class="reference external" href="https://arxiv.org/abs/2311.17035">Scalable Extraction of Training Data from (Production) Language Models</a><a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p>Ask ChatGPT to repeat a word forever. After a while it starts barfing training data.
This is now against OpenAI terms of service.</p>
<blockquote>
<div><p>Our attack circumvents the privacy safeguards by identifying a vulnerability in ChatGPT that causes it to escape its fine-tuning alignment procedure and fall back on its pre-training data.</p>
</div></blockquote>
<p><a class="reference external" href="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html">See Also</a></p>
</section>
<hr class="docutils" />
<section id="id9">
<h3><a class="reference external" href="https://arxiv.org/abs/2311.03348">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a><a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h3>
<p>Getting GPT4 to give you a recipe for meth is very doable. It’s not a GPT problem, it’s a standard LLM problem. This has big implications for Secret and Top Secret LLMs. Jailbreaking them is a security violation / spill.</p>
</section>
<hr class="docutils" />
<section id="id10">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11514.pdf">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a><a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h3>
<p>Apple.  Extremely computationally efficient LLMs</p>
</section>
</section>
<hr class="docutils" />
<section id="images">
<h2>Images<a class="headerlink" href="#images" title="Permalink to this heading">¶</a></h2>
<section id="id11">
<h3><a class="reference external" href="https://yformer.github.io/efficient-sam/">Efficient SAM</a><a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h3>
<p>I tend to think the future of most AI is big foundational models that few people seldomly retrain. Segment Anything (SAM) is the foundational model of image segmentation and its pretty good most of the time. Now fast.</p>
</section>
<hr class="docutils" />
<section id="id12">
<h3><a class="reference external" href="https://arxiv.org/abs/2312.00785">Sequential Modeling Enables Scalable Learning for Large Vision Models</a><a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h3>
<p>Training a foundational vision model that can do many tasks. Uses transformers and image squences. It’s a bit insane. All input must be images so if you want BBoxes you better draw an image with those boxes in it. I didn’t say it was good. I said it was interesting.</p>
<p><a class="reference external" href="https://yutongbai.com/lvm.html">See Also</a></p>
</section>
<section id="id14">
<h3><a class="reference external" href="https://arxiv.org/abs/2312.09158">General Object Foundation Model for Images and Videos at Scale</a><a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h3>
<p>More foundational model vision talk. This thing does it all, object detection, segmentation, tracking. Honestly it looks pretty impressive.</p>
</section>
<hr class="docutils" />
<section id="id15">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.04561.pdf">GenDeF: Learning Generative Deformation Field for Video Generation</a><a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h3>
<p>Video generation via warping one image rather than generating multiple frames in a row.  Unclear how it extends to long videos, but might have some insights for analyzing videos</p>
</section>
</section>
<hr class="docutils" />
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<section id="id16">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.04234.pdf">Graph Convolutions Enrich the Self-Attention in Transforms!</a><a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h3>
<p>We are reaching the limits of self-attention as a mechanism in transformers (???).  Represents self-attention as a graph filter and redesigns from graph signal processing perspective.  Increased performance but also increased complexity</p>
</section>
<hr class="docutils" />
<section id="id17">
<h3><a class="reference external" href="https://arxiv.org/abs/2312.09020">Exploring Transferability for Randomized Smoothing</a><a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<p>If you want a good model you need to train it well with tons of augmentation the first time. When you get a new task and fine tune you will remain robust to augmentation (noise) if if you don’t fine tune with it.</p>
</section>
<hr class="docutils" />
<section id="id18">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.00359.pdf">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a><a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h3>
<p>Interprets learning rate as temperature, and proposes a method for varying the learning rate in a DNN on a layer-by-layer basis.  Significantly outperforms existing SGD methods</p>
</section>
<hr class="docutils" />
<section id="id19">
<h3><a class="reference external" href="https://openreview.net/forum?id=dQkeoGnn68">Understanding the Detrimental Class-level Effects of Data Augmentation</a><a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h3>
<p>ICML.  Analyzes how data augmentation can hurt individual class-level accuracy while improving average class level accuracy.  Data augmentation creates overlap between data distributions associated with different classes</p>
</section>
<hr class="docutils" />
<section id="id20">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.05858.pdf">The Machine Learning Control Method for Counterfactual Forecasting</a><a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h3>
<p>Interesting approach to causal problems.  Learns trend before treatment using ML methods (regression trees?) and forecasts the counterfactual, what would happen to patients in the absence of treatment.  This allows estimation of treatment effects.</p>
</section>
<hr class="docutils" />
<section id="id21">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.07425.pdf">Deep Internal Learning: Deep Learning from a Single Input</a><a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h3>
<p>Review paper.  Covers methods for doing deep internal learning - training a model from a very small amount of inputs - with a focus on CV</p>
</section>
<hr class="docutils" />
<section id="id22">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.06937.pdf">Can a Transformer Represent a Kalman Filter?</a><a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h3>
<p>Yes.  Short paper, focusses on theory - no experiments/implementation.  Also, they left the AISTATs instruction blurb in at the end…</p>
</section>
<hr class="docutils" />
<section id="id23">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.10794.pdf">A Mathematical Perspective on Transformers</a><a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h3>
<p>New mathematical perspective on transformers: “based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time.”</p>
</section>
<hr class="docutils" />
<section id="id24">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11281.pdf">Human mobility is well described by closed-form gravity-like models learned automatically from data</a><a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h3>
<p>Simple, gravity-like machine learning models better describe human mobility than either gravity models or deep learning models</p>
</section>
</section>
<hr class="docutils" />
<section id="reasoning">
<h2>Reasoning<a class="headerlink" href="#reasoning" title="Permalink to this heading">¶</a></h2>
<section id="id25">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.04997.pdf">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a><a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h3>
<p>Review paper.  Does what it says on the tin</p>
</section>
<hr class="docutils" />
<section id="id26">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09219.pdf">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a><a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h3>
<p>Extends knowledge graph methods to nested triples or triples of triples: “(e.g., ((BarackObama, holds position, President), succeed by, (DonaldTrump, holds position, President)))”</p>
</section>
<hr class="docutils" />
<section id="id27">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09995.pdf">SAT-Based Algorithms for Regular Graph Pattern Matching</a><a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h3>
<p>Proposes ReGaPs (regular graph patterns) to do better graph matching - isomorphisms, approximations, subsets, etc, as well as Boolean satisfiability (SAT) encoding, and a simplification technique</p>
</section>
</section>
<hr class="docutils" />
<section id="overhead">
<h2>Overhead<a class="headerlink" href="#overhead" title="Permalink to this heading">¶</a></h2>
<section id="id28">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.06587.pdf">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a><a class="headerlink" href="#id28" title="Permalink to this heading">¶</a></h3>
<p>Uses satellite/high-altitude SAR dataset to assess which buildings were damaged by earthquakes.  Combination of binary classification and anomaly detection.</p>
</section>
<hr class="docutils" />
<section id="id29">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.06940.pdf">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a><a class="headerlink" href="#id29" title="Permalink to this heading">¶</a></h3>
<p>Coauthors from DEVCOM Army Research Lab - might give us insight about what they want.  They analyze SAR classifiers for classification accuracy, runtime performance in terms of inference throughput, and analytical performance in terms of number of parameters, number of layers, model size and number of operations.  No single model rules them all</p>
</section>
<hr class="docutils" />
<section id="id30">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.04548.pdf">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a><a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h3>
<p>Creates a dataset of co-located ground and aerial views.  Finds that supplementing aerial detectors with ground views of the same location at the same time increases performance.</p>
</section>
<hr class="docutils" />
<section id="id31">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09159.pdf">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a><a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h3>
<p>New LWIR overhead aerial dataset. Probably more useful for UAS than TA2, but might be useful if we ever want to do IR capabilities</p>
</section>
</section>
<hr class="docutils" />
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<section id="id32">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09238.pdf">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a><a class="headerlink" href="#id32" title="Permalink to this heading">¶</a></h3>
<p>Uses LLMs to make design dense rewards in Minecraft to make it easier to train AI</p>
</section>
<hr class="docutils" />
<section id="id33">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09187.pdf">Vision-Language Models as a Source of Rewards</a><a class="headerlink" href="#id33" title="Permalink to this heading">¶</a></h3>
<p>Deepmind.  Uses vision-language models (CLIP family) to generate dense rewards for use in training reinforcement learning type things</p>
</section>
<hr class="docutils" />
<section id="id34">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09033.pdf">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a><a class="headerlink" href="#id34" title="Permalink to this heading">¶</a></h3>
<p>Proposes a surprise index to evaluate how autonomous AI makes decisions and evaluates on space-maneuvers.</p>
</section>
<hr class="docutils" />
<section id="id35">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11761.pdf">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a><a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h3>
<p>Minecraft for learning.  Proposes a method for grading natural language descriptions of a Minecraft screenshot</p>
</section>
<hr class="docutils" />
<section id="id36">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11865.pdf">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a><a class="headerlink" href="#id36" title="Permalink to this heading">¶</a></h3>
<p>LLMs play SC2.  Has an SC2 to text to Chain of Summarization pipeline for developing strategies and allowing the LLM to interact</p>
</section>
<hr class="docutils" />
<section id="id37">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.12568.pdf">Scaling Opponent Shaping to High Dimensional Games</a><a class="headerlink" href="#id37" title="Permalink to this heading">¶</a></h3>
<p>Proposes a method for shaping your opponents’ behaviors in multi-agent games to get to better outcomes for everyone</p>
</section>
</section>
<hr class="docutils" />
<section id="stats">
<h2>Stats<a class="headerlink" href="#stats" title="Permalink to this heading">¶</a></h2>
<section id="id38">
<h3><a class="reference external" href="https://arxiv.org/ftp/arxiv/papers/2312/2312.03894.pdf">Zero-Class Poisson for Rare-Event Studies</a><a class="headerlink" href="#id38" title="Permalink to this heading">¶</a></h3>
<p>Proposes a Bayesian zero-count Poisson detector for rare event detection.  Like all rare event stuff you’re pretty data-limited, but it’s a cool field of study</p>
</section>
<hr class="docutils" />
<section id="id39">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.05152.pdf">Probabilistic Reconstruction of Paleodemographic Signals</a><a class="headerlink" href="#id39" title="Permalink to this heading">¶</a></h3>
<p>Bayesian approach to paleodemography with emphasis on uncertainty and a case study on Cyprus.  Cool problem, though I’m not necessarily convinced by their methods</p>
</section>
<hr class="docutils" />
<section id="id40">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.09384.pdf">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a><a class="headerlink" href="#id40" title="Permalink to this heading">¶</a></h3>
<p>Proposes a Gaussian Process Regression model for epidemic analysis.  Derives new measures of uncertainty which make more sense than traditional measures</p>
</section>
<hr class="docutils" />
<section id="id41">
<h3><a class="reference external" href="https://arxiv.org/ftp/arxiv/papers/2312/2312.10494.pdf">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a><a class="headerlink" href="#id41" title="Permalink to this heading">¶</a></h3>
<p>Naval Surface Warfare Center.  Use Bayesian Neural Nets to estimate time to failure for highly reliable weapons systems.</p>
</section>
<hr class="docutils" />
<section id="id42">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11573.pdf">Estimation of individual causal effects in network setup for multiple treatments</a><a class="headerlink" href="#id42" title="Permalink to this heading">¶</a></h3>
<p>Uses graph convolutional networks to estimate individual treatment effects in network meta analysis settings with observational data</p>
</section>
<hr class="docutils" />
<section id="id43">
<h3><a class="reference external" href="https://arxiv.org/pdf/2312.11754.pdf">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a><a class="headerlink" href="#id43" title="Permalink to this heading">¶</a></h3>
<p>Uses a Bayesian model to account for underreporting in storm-induced flooding reports, using data from across multiple storms.  Outperforms baseline models</p>
</section>
</section>
<hr class="docutils" />
<section id="potpourrie">
<h2>Potpourrie<a class="headerlink" href="#potpourrie" title="Permalink to this heading">¶</a></h2>
<section id="id44">
<h3><a class="reference external" href="https://wraltechwire.com/2023/12/01/ai-safety-is-imperative-triangle-thought-leaders-talk-artificial-intelligence-with-senate-panel/">NC Senate AI Panel Report</a><a class="headerlink" href="#id44" title="Permalink to this heading">¶</a></h3>
<p>Not a lot, but interesting to see who was there speaking on “our” behalf. It’s Cynthia Rudin.</p>
<p><a class="reference external" href="https://www.schumer.senate.gov/newsroom/press-releases/statements-from-the-seventh-bipartisan-senate-forum-on-artificial-intelligence">See also</a></p>
</section>
<hr class="docutils" />
<section id="id46">
<h3><a class="reference external" href="https://alexlenail.me/NN-SVG/AlexNet.html">NNSVG</a><a class="headerlink" href="#id46" title="Permalink to this heading">¶</a></h3>
<p>Make pretty SVGs from your NN architecture.
They look nice.</p>
</section>
<hr class="docutils" />
<section id="id47">
<h3><a class="reference external" href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page">Wikifunctions</a><a class="headerlink" href="#id47" title="Permalink to this heading">¶</a></h3>
<p>The Wikipedia of “functions” that can translate inputs to outputs. The open source collection of algorithms/code/functions. A bunch of string operations for now, not much going on, but eventually could be very useful for general AI.</p>
</section>
<hr class="docutils" />
<section id="id48">
<h3><a class="reference external" href="https://ml-explore.github.io/mlx/build/html/index.html">mlX</a><a class="headerlink" href="#id48" title="Permalink to this heading">¶</a></h3>
<p>Apple released computational library kinda like numpy or pytorch for Apple silicon. Because of the unified memory on Apple Silicon, and the inclusion of auto-grad, it’s suitable as a pytorch replacement. Has LLM runtime components that make LLMs go faster than “CPU” based operations.</p>
</section>
<hr class="docutils" />
<section id="id49">
<h3><a class="reference external" href="https://linkeddatafragments.org/specification/triple-pattern-fragments/">Triple Pattern Fragments</a><a class="headerlink" href="#id49" title="Permalink to this heading">¶</a></h3>
<p>The old school pick of the month. W3C is a bunch of committees that govern a lot of open standards. (There are those that say they lost and Google runs the internet now, and they are sorta right but,) Looking at the types of things they have come up with for abstraction and representations is interesting. Lot’s of smart people. We should be more inspired by them.</p>
</section>
<hr class="docutils" />
<section id="id50">
<h3><a class="reference external" href="https://sliders.baulab.info">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a><a class="headerlink" href="#id50" title="Permalink to this heading">¶</a></h3>
<p>In a generative model it is very useful to have knobs to control how to modify an existing image in a specific direction. Make the face older/have glasses etc. This is a way to train those sliders.</p>
</section>
<hr class="docutils" />
<section id="id51">
<h3><a class="reference external" href="https://arxiv.org/abs/2205.02767">Spiking Graph Convolutional Networks</a><a class="headerlink" href="#id51" title="Permalink to this heading">¶</a></h3>
<p>What ever happened to Spiking Networks? Do they do anything good yet? This is for graph convolution from March 2022 and the answer is, not really. Energy efficiency?
Here is another one <a class="reference external" href="https://arxiv.org/abs/2312.09084">Language Modeling on a SpiNNaker 2 Neuromorphic Chip</a> also being a little better on energy for LSTM language models (not even LLMs). Also do some event based camera work, MAYBE just MAYBE there is something there.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2023-12.html" class="btn btn-neutral float-left" title="2023-12" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-02.html" class="btn btn-neutral float-right" title="2024-02" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>