

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2024-06 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2024-07" href="2024-07.html" />
    <link rel="prev" title="2024-05" href="2024-05.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2024-06</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-06.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>2024-06<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.19756">KAN: Kolmogorov–Arnold Networks</a></dt><dd><p>Proposes Kolmogorov-Arnold Networks (KANs) as an alternative to Multi-Layer Perceptrons (MLPs).  First describes what KANs are, and then makes some pretty startling claims - smaller KANs are better than larger MLPs at data fitting and PDE solving (orders of magnitude, both ways), KANs have faster scaling laws than MLPs, are more interpretable, naturally avoid catastrophic forgetting in continual learning, etc etc.  Main drawback is slow training speed.  The examples chosen in this paper are probably favorable to KANs (and are probably closer to “toy” examples than real problems, so we have to see how they generalize), but KANs blow MLPs on them.  This is getting a lot of hype right now and there’s a lot of speculation that KANs might just replace MLPs.  Things usually don’t live up to the hype, but its worth a read.  Probably worth a lunch and learn too.</p>
</dd>
<dt><a class="reference external" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></dt><dd><p>The authors extract interpretable features from LLMs by using a sparse autoencoder.  These are abstract concepts spanning languages and modalities, and can be turned up or down to affect the behavior of the LLM.  Turning up “Golden Gate Bridge” to the max, for instance, makes the LLM think it is the Golden Gate Bridge. Worth a read, may be something like LLM Doctrinaire in here.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.01534">PLAN-SEQ-LEARN: LANGUAGE MODEL GUIDED RL FOR SOLVING LONG HORIZON ROBOTICS TASKS</a></dt><dd><p>Mistral and Carnegie Mellon think you can put LLMs in charge of robots for high-level planning, and let more standard reinforcement learning algorithms take care of the rest.  Does make a decent amount of sense, but you have to be real careful about what the LLM decides to have the robot do.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.15071">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></dt><dd><p>Transformers are bad at implicit reasoning over parametric knowledge, unless you train them beyond the realm of overfitting in which case they become good at it.  The authors all this grokking, but the really dangerous implication is that transformers and LLMs have not been trained enough and are actually underfit!</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2309.12969">Detect Everything with Few Examples</a></dt><dd><p>This is a good idea. It’s basically grounding dino but with a specially classifier layer to label 10-100 examples have everything work without fine tuning. We should really consider using this approach going forward.</p>
</dd>
<dt><a class="reference external" href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/">What We Learned from a Year of Building with LLMs (Part I)</a></dt><dd><p>This is a great summary of experiences and recommendations when trying to build things with LLMs. Well worth the read.</p>
</dd>
</dl>
</section>
<section id="llm-applications">
<h2>LLM Applications<a class="headerlink" href="#llm-applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.02957">Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents</a></dt><dd><p>LLMs simulate a hospital as doctors, staff, patients, etc and trains the doctor-LLMs via this simulated social interaction.  After treating 10,000 cases, the doctor-LLMs achieve “state-of-the-art” performance on the MedQA dataset.  I still wouldn’t want one as my doctor, but it’s an interesting alternative approach to training LLMs.</p>
</dd>
<dt><a class="reference external" href="https://swe-agent.com/paper.pdf">SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING</a></dt><dd><p>A bunch of researchers from Princeton try to make an LLM that can code.  They build a custom agent-computer-interface that greatly enhances performance.  Greatly in this case means 12.5 percent on SWE-bench, so LLMs aren’t stealing our code soon, but the next best performance (according to them) is a RAG approach at 3.8% so this is a big step forwards.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.09596">ENHANCING MARITIME TRAJECTORY FORECASTING VIA H3 INDEX AND CAUSAL LANGUAGE MODELLING (CLM)</a></dt><dd><p>Transforms a spatio-temporal problem in a natural language problem by converting spatial coordinates into a nested hexgrid system, turning these into tokens, and feeding these into Mistral. Outperforms a Kalman filter.  This is a pretty cool way to turn a hard problem into a form that an LLM can understand and then use that LLM to solve the problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.14755">Large language models can be zero-shot anomaly detectors for time series?</a></dt><dd><p>If you configure them right, LLMs can detet anomolies in time series.  This invovle turning time series data into text data and a “prompt-based detection method” for anomoly detection.  Given the sequential nature of transformers, this seems like a cool use case.</p>
</dd>
<dt><a class="reference external" href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/">What We Learned from a Year of Building with LLMs (Part I)</a></dt><dd><p>This is a great summary of experiences and recommendations when trying to build things with LLMs. Well worth the read.</p>
</dd>
</dl>
</section>
<section id="llm-theory">
<h2>LLM Theory<a class="headerlink" href="#llm-theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.19737">Better &amp; Faster Large Language Models via Multi-token Prediction</a></dt><dd><p>Existing LLMs are trained with next-token prediction loss.  This paper trains LLMs by doing loss on the next n tokens using n independent heads.  Seems to improve performance.  I don’t know a lot about LLMs, but I’m surprised no one tried this previously - seems intuitive once it pointed out.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.14860">Not All Language Model Features Are Linear</a></dt><dd><p>Are LLMs linear?  That is, do they do things by manipulating one-dimensional features?  This paper investigates and discovers that the answer is no - some features such as days of week and months of year are strikingly circular.  Further argues that these circular features are the fundamental unit of LLMs in Mistral and Llama.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.17399">Transformers Can Do Arithmetic with the Right Embeddings</a></dt><dd><p>Transformers are bad at arithmetic because they get lost in the sauce - they forget which numbers belong in which digits.  If you give them encodings teling them which numbers are which digits they can do math.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.15071">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></dt><dd><p>Transformers are bad at implicit reasoning over parametric knowledge, unless you train them beyond the realm of overfitting in which case they become good at it.  Teh authors all this grokking, but the really dangerous implication is that transformers and LLMs have not been trained enough</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.20339">Visual Perception by Large Language Model’s Weights</a></dt><dd><p>Existing vision LLMs function by pasting visual and text tokens together.  Instead of doing that, this paper uses an encoder to turn pictures into LLM weight and just puts them directly into the LLM.</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.05133">IDENTIFYING EVERY BUILDING’S FUNCTION IN LARGE-SCALE URBAN AREAS WITH MULTI-MODALITY REMOTE-SENSING DATA</a></dt><dd><p>Uses remote sensing data to classify building uses… in theory.  In practice, uses EO data at 1 GSD for visual representations and night-time data remote sensing data for light use.  Supplements with a lookup table of buliding heights.  Makes a neural net that generates building segmentations and maps their use.  Could imagine the IC being interested in something like this.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.01534">PLAN-SEQ-LEARN: LANGUAGE MODEL GUIDED RL FOR SOLVING LONG HORIZON ROBOTICS TASKS</a></dt><dd><p>Mistral and Carnegie Mellon think you can put LLMs in charge of robots for high-level planning, and let more standard reinforcement learning algorithms take care of the rest.  Does make a decent amount of sense, but you have to be real careful about what the LLM decides to have the robot do.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.01745">Large Language Models for UAVs: Current State and Pathways to the Future</a></dt><dd><p>Review paper covering how to get LLMs onto UAVs at a decently high level.  The idea seems to gaining prominence recently, so might be worth a look.  Ended up not really saying anything beyond LLMs are cool. Maybe the news was that you could use a multimodal LLM and then tell the UAV to follow the bus or something.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.19756">KAN: Kolmogorov–Arnold Networks</a></dt><dd><p>Proposes Kolmogorov-Arnold Networks (KANs) as an alternative to Multi-Layer Perceptrons (MLPs).  First describes what KANs are, and then makes some pretty startling claims - smaller KANs are better than larger MLPs at data fitting and PDE solving (orders of magnitude, both ways), KANs have faster scaling laws than MLPs, are more interpretable, naturally avoid catastrophic forgetting in continual learning, etc etc.  Main drawback is slow training speed.  The examples chosen in this paper are probably favorable to KANs (and are probably closer to “toy” examples than real problems, so we have to see how they generalize), but KANs blow MLPs on them.  This is getting a lot of hype right now and there’s a lot of speculation that KANs might just replace MLPs.  Things usually don’t live up to the hype, but its worth a read.  Probably worth a lunch and learn too.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.07992">MambaOut: Do We Really Need Mamba for Vision?</a></dt><dd><p>Mamba is more suited to long-sequence and autoregressive tasks than it is to vision tasks, but detection and segmentation are somewhat long-sequence.  This paper proposes a new Mamba model, MambaOut, based on this insight which eliminates the state space model and outperforms other Mamba versions on vision tasks.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.07987">The Platonic Representation Hypothesis</a></dt><dd><p>The authors note that, over time and across vision and language modalities, as NNs get deeper they measure distance between datapoints in similar ways.  They tie this in with some philosophy stuff (Platonic forms, anyone?), but the intuition is that all models are attempting to represent reality, and as they grow larger they arrive ever closer to a “true” statistical representation of reality.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.08790">Kolmogorov-Arnold Networks (KANs) for Time Series Analysis</a></dt><dd><p>KANs come for time series (or do they).  This paper shows that 3 and 4 layer KANs outperform 3 and 4 layer MLPs.  This is very much expected behavior and, given training costs, the fair comparison is between a KAN and an MLP much deeper than the KAN.  For time series you’d probably want to compare a transformer or an LSTM.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.12832">Wav-KAN: Wavelet Kolmogorov-Arnold Networks</a></dt><dd><p>KANs but with wavelets instead of splines.  Seems like a decent idea (and avoids a lot of the slow training stuff KANs run into) but doesn’t have a lot of good comparisons.</p>
</dd>
</dl>
</section>
<section id="sensing">
<h2>Sensing<a class="headerlink" href="#sensing" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.06323">OPEN ACCESS BATTLE DAMAGE DETECTION VIA PIXEL-WISE T-TEST ON SENTINEL-1 IMAGERY</a></dt><dd><p>Fast and simple method for detecting battle-damage (really just changepoint detection?) in overhead satellite imagery with an eye towards Ukraine and Gaza.  Seems to work pretty well, rivaling deep-leearning based methodologies.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.06149">DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness</a></dt><dd><p>A tracking system for a USV which operates by using a neural net to estimate the distance and bearing of objects from a camera and record them in GeoTracks.  Feels similar to some of our UAS/MMP work, though much more “throw a neural net at it”.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2403.04700">Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</a></dt><dd><p>Pedestrian Re-ID datasets lack in a few dimensions and thus have long tails. Many trackers don’t work well in the long tails. This paper makes up a few augmentation ideas. Not a bad idea if we start to investigate trained tracking algorithms.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.00676">SUNDAE: Spectrally Pruned Gaussian Fields with Neural Compensation</a></dt><dd><p>Gaussian splatting can be slow and memory intensive.  This paper does some fancy footwork and exploits relationships between primitives to develop a new Gaussian splatting algorithm that is simultaneously less memory intensive and better than old methods.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.19760">Lightplane: Highly-Scalable Components for Neural 3D Fields</a></dt><dd><p>From Meta.  Introduces new method for efficient 2D to 3D Gaussian splatting. Really emphasizes the memory efficiency.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.02005">HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft HoloLens 2</a></dt><dd><p>This paper gets Gaussian splatting up and running on a Hololens.  Results look pretty decent.  Something to keep in mind if we ever get back to working with it again.</p>
</dd>
</dl>
</section>
<section id="reasoning-knowledge-graphs">
<h2>Reasoning/Knowledge Graphs<a class="headerlink" href="#reasoning-knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.04521">UR-GNN (Gaussian Graph Embedding)</a></dt><dd><p>What if we represented embeddings of nodes in a KG using gaussians with mean vector (normal embedding) and variance matrix?  Then we could throw a custom GNN with attention at these embeddings and get it to predict new edges in a probabilistic way.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.18980">THE IMPACT OF COVID-19 ON CO-AUTHORSHIP AND ECONOMICS SCHOLARS’ PRODUCTIVITY</a></dt><dd><p>Analyzes how the pandemic effected collaboration in economics academia.  Before the pandemic, economists were more likely to coauthor with authors of similar productivity; during, things were more mixed. Reminds me a bit of the paper that analyzed marriages amongst the nobility after Prince Alfred died.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.04352">Return to Office and the Tenure Distribution</a></dt><dd><p>How does return to office impact employee tenure?   This study finds that return-to-office causes employees, especially senior employees, to leave in larger-than-expected numbers.  Further, they tend to be replaced by people who are younger/less experienced.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.05596">Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content</a></dt><dd><p>This study conducts a randomized control trial which determines that users change how they interact with recommender systems if they’re told how the recommender system works in an attempt to influence the recommendations they are given.  This is an extremely intuitive result.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.04324">Granite Code Models: A Family of Open Foundation Models for Code Intelligence</a></dt><dd><p>IBM releases a code-focussed LLM.  Decoder only, trained in 116 languages.  Github available.  Reaches (and sometimes exceeds) state-of-the-art performance.  May be smaller than competitors and good at all coding focussed tasks, unlike larger models which have specialized and achieve about the same performance.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.04434">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></dt><dd><p>DeepSeek-AI drops another Mixture-of-Experts LLM.  Total of 236B parameters.  Context length of 128K tokens.  Better performance, lower training cost, etc.  Even with “only” 21B parameters, gets state-of-the-art performance amongst open-source models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.04517">xLSTM: Extended Long Short-Term Memory</a></dt><dd><p>Introduces a new LSTM architecture by making two modifications of traditional LSTMs - exponential gating and novel memory structures - to remedy some of the structural defects of LSTMs compared to transformers.  Looks impressive in simulations compared to transformers and other LSTMs</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.10300">Grounding DINO 1.5: Advance the “Edge” of Open-Set Object Detection</a></dt><dd><p>A new suite of Grounding DINO models which do more or less the same thing as the old one (detect object given language prompts) but comes in two flavors, one of which is better and one of which is faster.</p>
</dd>
<dt><a class="reference external" href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</a></dt><dd><p>Google has released Gemini 1.5.  The lab report they released is 150 pages long so I’m not reading it, but it’s probably suitably impressive.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.09818">Chameleon: Mixed-Modal Early-Fusion Foundation Models</a></dt><dd><p>Meta released an arxiv paper detailing Chameleon, a “family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence.”  The multimodal aspect is pretty cool. The archive paper is dated to May 16th, 2024, but there’s a blog post from July 2023 about it so idk if this is new or not.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2309.12969">Detect Everything with Few Examples</a></dt><dd><p>This is a good idea. It’s basically grounding dino but with a specially classifier layer to label 10-100 examples have everything work without fine tuning. We should really consider using this approach going forward.</p>
</dd>
</dl>
</section>
<section id="lunch-and-learn">
<h2>Lunch and Learn<a class="headerlink" href="#lunch-and-learn" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-05-07 <a class="reference external" href="https://arxiv.org/abs/2403.13327">Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion</a></dt><dd><p><a class="reference external" href="https://voluma.ai/view/jack/test/baltimore">Gaussian Splatting Example: Key Bridge</a>
<a class="reference external" href="https://github.com/nerfstudio-project/nerfstudio/?tab=readme-ov-file#dependencies">NERFStudio</a></p>
</dd>
<dt>2024-05-21</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.11584">THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES FOR REASONING, PLANNING, AND TOOL CALLING: A SURVEY</a></dt><dd><p>Researchers from Neudesic (IBM AI offshoot) look at a bunch of more recent AI Agent architectures and posit that this is the way the field will continue to head (tools instead of RAG).  They identify the importance of reasoning structures and useful tools, as well as the benefits and downsides of multi-agent approaches.  <a class="reference external" href="https://docs.google.com/presentation/d/1RF884dELODX-Yxqbx5J9qJIqEVhcmfLox_jcbc5eG0Q/edit?usp=sharing">Related slides</a>.</p>
</dd>
<dt><a class="reference external" href="https://github.com/kingjulio8238/memary">memary</a></dt><dd><p>Really interesting full implementation of an Agent based off of the ReAct architecture.  Uses a knolwedge graph for relating topics of conversation.  Definitely rough but surprsisingly good readme.</p>
</dd>
</dl>
</dd>
<dt>2024-05-28</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://transformer-circuits.pub/2023/monosemantic-features">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></dt><dd><p>Researchers from Anthropic use sparse autoencoders to isolate monosemantic features with high sensitivity and specificity.</p>
</dd>
<dt><a class="reference external" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></dt><dd><p>Researchers from Anthropic apply their sparse autoencoder approach to interpretability to a ChatGPT-size model, revealing millions of both low-level and high-level features. They find that setting these features to extremely low/high values influences model behavior.</p>
</dd>
</dl>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-05.html" class="btn btn-neutral float-left" title="2024-05" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-07.html" class="btn btn-neutral float-right" title="2024-07" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>