

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The CoVar Zeitgeist: 2024-08 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The CoVar Zeitgeist: 2024-09" href="2024-09.html" />
    <link rel="prev" title="2024-07" href="2024-07.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The CoVar Zeitgeist: 2024-08</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-08.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-covar-zeitgeist-2024-08">
<h1>The CoVar Zeitgeist: 2024-08<a class="headerlink" href="#the-covar-zeitgeist-2024-08" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">AI achieves silver-medal standard solving International Mathematical Olympiad problems</a></dt><dd><p>Deepmind drops another banger.  This time its two reinforcement learning algorithms which, with their powers combined, manage a silver medal at this years International Mathematics Olympiad.  Given that most models in the field struggle with simple problems, this is astounding.  The blog is short and worth a read, but they duct-taped an LLM between the AlphaZero reinforcement learning problem and formal mathematical prover using formal language that could evaluate whether a proof was correct.  It seems like they leveraged that, in the right circumstances, evaluating whether a proof is correct is easy.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.17453">VILA2 : VILA Augmented VILA</a></dt><dd><p>A new VLM from NVIDIA.  The model, though, is less interesting than how they train it.  you can run out of labelled quality data pretty fast, so they have a “self-augment” and a “specialist-augment” step to improve data quality and labels.  They also augment with other models.  Cool idea, and a neat way to generate quality data ex nihilo.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.08972">Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness</a></dt><dd><p>ConvNets with extremely large kernels achieve comparable/superior performance compared to vision transformers.  This paper demonstrates this for robustness and investigates why.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10456">Don’t Throw Away Data: Better Sequence Knowledge Distillation</a></dt><dd><p>From Deepmind.  The problem investigated is how to manage student-teacher cases where you have a big LLM (the teacher) that can perform a specific task (e.g. language translation) but you want a smaller model (the student) to be able to perform the same task.  Finds the most optimal method of teaching the student.  We might be able to use similar methods for cases where LLMs are big but useful in a limited way.  Maybe ODIN?</p>
</dd>
<dt><a class="reference external" href="https://ai.meta.com/research/publications/meta-3d-assetgen-text-to-mesh-generation-with-high-quality-geometry-texture-and-pbr-materials/?utm_source=twitter&amp;utm_medium=organic_social&amp;utm_content=thread&amp;utm_campaign=research">Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials</a></dt><dd><p>Meta presents a novel model for generating 3D objects from text or image inputs.  The examples look incredibly impressive.  Anyone working on recovering CAD models/3D representations of objects should take a look at this.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.15811">Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</a></dt><dd><p>Sony AI trains a sparse 1.16B parameter diffusion transformer over 2.6 days on an 8xH100, with only $1890 in GPU costs.  Lots of fancy footwork went into this paper.  Maybe this means we can entertain the idea of training our own models from scratch?</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.03232">Single Character Perturbations Break LLM Alignment</a></dt><dd><p>LLMs tend to have safeguards built in like “don’t tell people how to build bombs”.  This paper got around these safeguards simply by appending a token of whitespace at the end of input prompts.  This happens because,in training, whitespace prompts the model to make lists and this overrides the “don’t make bombs” instruction.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.03453">On Large Language Models in National Security Applications</a></dt><dd><p>Two professors from the Air Force Institute of Technology talk about the use case of LLMs in the Air Force.  I’m not sure how much is new, but it’s relatively short and probably worth a read for anyone involved in BD or LLMs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10886">SLIP: Securing LLM’s IP Using Weights Decomposition</a></dt><dd><p>Microsoft develops a method to protect LLM weights from being discovered by decomposing them into two matrices, one of which is recoverable and one of which is not.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10456">Don’t Throw Away Data: Better Sequence Knowledge Distillation</a></dt><dd><p>From Deepmind.  The problem investigated is how to manage student-teacher cases where you have a big LLM (the teacher) that can perform a specific task (e.g. language translation) but you want a smaller model (the student) to be able to perform the same task.  Finds the most optimal method of teaching the student.  We might be able to use similar methods for cases where LLMs are big but useful in a limited way.  Maybe ODIN?</p>
</dd>
</dl>
</section>
<section id="vlms">
<h2>VLMs<a class="headerlink" href="#vlms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.03320">InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</a></dt><dd><p>Novel open-source large vision language model.  Can handle text-to-image and image-to-text.  Fairly extensive benchmarking, seems about on par with GPT-4.  Claims to be the best open source VLM.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.15795">AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection</a></dt><dd><p>A novel CLIP variant that functions as a zero-shot anomaly detector/segmenter.  Takes text as an input and semgents whatever was described in the text.  Potentially useful as an “off-the-shelf” tool for a lot of functions.  Maybe a Grounded SAM replacement?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.14500">ViLLa: Video Reasoning Segmentation with Large Language Model</a></dt><dd><p>A novel vision LLM which takes a text prompt as an input and segments the relevant objects in a video, possibly with a text description as well.  Looks pretty powerful in a limited number of examples, but code is coming “soon”.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.17453">VILA2 : VILA Augmented VILA</a></dt><dd><p>A new VLM from NVIDIA.  The model, though, is less interesting than how they train it.  you can run out of labelled quality data pretty fast, so they have a “self-augment” and a “specialist-augment” step to improve data quality and labels.  They also augment with other models.  Cool idea, and a neat way to generate quality data ex nihilo.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.18908">Wolf: Captioning Everything with a World Summarization Framework</a></dt><dd><p>NVIDIA and friends propose a novel VLM whose main focus is on captioning images/videos.  It’s a short jump from captioning to identifying objects in BBOXs, so something like this could be slotted into the Grounded SAM/Grounding DINO pipeline.</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.02394">Similarity Distance-Based Label Assignment for Tiny Object Detection</a></dt><dd><p>A better method for non-max suppression for tiny object detection.  Seems to greatly improve performance of faster R-CNN on AITOD.  Worth looking into for any of our tiny object detection problems.</p>
</dd>
<dt><a class="reference external" href="https://ai.meta.com/research/publications/meta-3d-assetgen-text-to-mesh-generation-with-high-quality-geometry-texture-and-pbr-materials/?utm_source=twitter&amp;utm_medium=organic_social&amp;utm_content=thread&amp;utm_campaign=research">Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials</a></dt><dd><p>Meta presents a novel model for generating 3D objects from text or image inputs.  The examples look incredibly impressive.  Anyone working on recovering CAD models/3D representations of objects should take a look at this.</p>
</dd>
<dt><a class="reference external" href="https://ai.meta.com/research/publications/meta-3d-texturegen-fast-and-consistent-texture-generation-for-3d-objects/?utm_source=twitter&amp;utm_medium=organic_social&amp;utm_content=thread&amp;utm_campaign=research">Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects</a></dt><dd><p>Meta presents a novel model for generating textures for 3D objects.  Probably supposed to work with AssetGen, this also looks suitably impressive.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.12675">Distilling Tiny and Ultra-fast Deep Neural Networks for Autonomous Navigation on Nano-UAVs</a></dt><dd><p>Builds a tiny CNN to function as the brain on a “nano-drone”.  Lots of potential with a swarm of these things for recon/navigation/obstacle detection purposes.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.18892">SHANGUS: Deep Reinforcement Learning Meets Heuristic Optimization for Speedy Frontier-Based Exploration of Autonomous Vehicles in Unknown Spaces</a></dt><dd><p>A deep reinforcement learning framework to control an autonomous vehicle exploring an unknown space.  One of the stated applications is space (!) but it’s easy to imagine wanting this for recon purposes.</p>
</dd>
</dl>
</section>
<section id="reinforcement-learning">
<h2>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10441">Enhancing Building Safety Design for Active Shooter Incidents: Exploration of Building Exit Parameters using Reinforcement Learning-Based Simulation</a></dt><dd><p>Incredibly morbid, but there’s something here that can be adapted to simulating armed combatants that may have a use case.</p>
</dd>
<dt><a class="reference external" href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">AI achieves silver-medal standard solving International Mathematical Olympiad problems</a></dt><dd><p>Deepmind drops another banger.  This time its two reinforcement learning algorithms which, with their powers combined, manage a silver medal at this years International Mathematics Olympiad.  Given that most models in the field struggle with simple problems, this is astounding.  The blog is short and worth a read, but they duct-taped an LLM between the AlphaZero reinforcement learning problem and formal mathematical prover using formal language that could evaluate whether a proof was correct.  It seems like they leveraged that, in the right circumstances, evaluating whether a proof is correct is easy.</p>
</dd>
</dl>
</section>
<section id="fusion">
<h2>Fusion<a class="headerlink" href="#fusion" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.12647">Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial Vehicles Surveillance in Day and Night Dual Visions</a></dt><dd><p>Bulids a bespoke model for RGB/IR sensor fusion for counter-UAS activities at day and night-time.  The results seem convincing, may be worth taking inspiration from.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.13771">Training-Free Model Merging for Multi-target Domain Adaptation</a></dt><dd><p>Investigates how to do fusion on multiple models spanning multiple domains without access to training data.  Not directly relevant to anything we’re doing  (EID would have a fit about all the deep learning), but maybe useful later.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.12663">Is That Rain? Understanding Effects on Visual Odometry Performance for Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge</a></dt><dd><p>Builds a dataset and a (small) detector for detecting whether or not it is raining outside.  We could use this for sensor fusion, or context-aware sensing.</p>
</dd>
</dl>
</section>
<section id="tracking">
<h2>Tracking<a class="headerlink" href="#tracking" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.17272">DenseTrack: Drone-based Crowd Tracking via Density-aware Motion-appearance Synergy</a></dt><dd><p>How to do crowd-tracking from a drone. Neural nets feed into similarity/distance matrices which feed into Hungarians.  Similar to how we do it, except the “tracking net” which they seem to be using instead of IoU metrics or keypoints.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.20055">SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting</a></dt><dd><p>From Deepmind.  Proposes a novel Gaussian Splatting method which can effectively ignore interfering objects.  We’ve noticed on EID that this can lead to weird splats, so ignoring it is quite nice.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.11793">Click-Gaussian: Interactive Segmentation to Any 3D Gaussians</a></dt><dd><p>A 3D Gaussian Splatting renderer/UI that allows the user to segment any object inside the render by clicking on it and adjusting a parameter.  We’re using this (or something morally equivalent) for EID and it’s pretty cool.</p>
</dd>
</dl>
</section>
<section id="gotta-go-fast">
<h2>Gotta Go Fast<a class="headerlink" href="#gotta-go-fast" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.02362">Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA</a></dt><dd><p>A new matrix multiplication method for putting neural nets on FPGAs.  Much more efficient than the baseline.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10960">Fast Matrix Multiplications for Lookup Table-Quantized LLMs</a></dt><dd><p>Another paper in the “make LLM go fast by multiply matrix fast” genre.  Like most of these it goes a bit over my head, but it seems to have quite an interesting approach, using an offline lookup table to supplement its quantized matrix mulitiplication.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.10969">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></dt><dd><p>Another method for making LLMs go fast.  Acts as a method to sparsify the model, and can be applied ontop of either full precision or 1-bit models.  Maintains performance while going much faster.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.12736">CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference</a></dt><dd><p>A bunch of people from USC develop a software for putting vision transformers on FPGAs.  Lots of stuff in here that’s slightly above my head, but the results look legitimate.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.16286">A deeper look at depth pruning of LLMs</a></dt><dd><p>A group at NVIDIA takes a look at various methods for pruning LLMs.  Finds that you can prune a third of Mistral 7B and retain the same performance.  Could be worth a look for our LLM related work.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.12753">LookupViT: Compressing visual information to a limited number of tokens</a></dt><dd><p>From Deepmind.  There’s lots of tokens in images which have very low information content - this paper compresses input tokens to a fixed number of tokens as a method of getting rid of the extraneous tokens.  Improves computational burdens and (sometimes) performance.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.15811">Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</a></dt><dd><p>Sony AI trains a sparse 1.16B parameter diffusion transformer over 2.6 days on an 8xH100, with only $1890 in GPU costs.  Lots of fancy footwork went into this paper.  Maybe this means we can entertain the idea of training our own models from scratch?</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.03512">The Art of the Steal: Purloining Deep Learning Models Developed for an Ultrasound Scanner to a Competitor Machine</a></dt><dd><p>If you put a proprietary DL algorithm on a device, anyone with access to the device can recreate, or “steal” the model weights of the original algorithm by using the device to label a bunch of data and training a new algorithm on that data.  This paper proposes a better way to do that which essentially replicates the performance of the original algorithm.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.08972">Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness</a></dt><dd><p>ConvNets with extremely large kernels achieve comparable/superior performance compared to vision transformers.  This paper demonstrates this for robustness and investigates why.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.04153">Mixture of A Million Experts</a></dt><dd><p>From Deepmind.  Mixture of Experts (MoE) is a promising alternative architecture to transformers which is essentially just an ensemble model.  This one takes it to extremes, with a million experts, and claims to be the best way to do MoEs.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.16647">Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye Images in Autonomous Driving</a></dt><dd><p>Does ATR with fish-eye camera, finds that a deformable CNN (where the kernel depends on the shape of the object) outperforms the non-deformable version of CNNs such as ResNets and U-Nets.  Do we use any fish-eye cameras?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.17654">GENERATIVE LEARNING FOR SIMULATION OF US ARMY VEHICLE FAULTS</a></dt><dd><p>Throws some deep learning methods at the problem of “predicting when US Army vehicles will break down”.  Presumably there’s some army interest behind this, even if it’s not stated explicitly in this paper, because why would you do this otherwise.  Two of the coauthors are at ECE in Duke - might be a collaboration opportunity, might be we know them already.  Sponsored by the Air Force in what I can only assume is inter-department shade.</p>
</dd>
</dl>
</section>
<section id="new-llms">
<h2>New LLMs<a class="headerlink" href="#new-llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.04620">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a></dt><dd><p>New hidden state model with linear complexity in context length.  Seems to outperform both transformers and Mamba in terms of computational time and results.  Impressive if true.</p>
</dd>
<dt><a class="reference external" href="https://mistral.ai/news/codestral-mamba/">Codestral Mamba</a></dt><dd><p>Mistral drops another LLM, this time based on Mamba and with an Apache 2.0 license.  They say it’s good but this particular link is light on resources.</p>
</dd>
<dt><a class="reference external" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">GPT-4o mini: advancing cost-efficient intelligence</a></dt><dd><p>A new GPT model which is very small and very cheap yet better than all GPT models across a range of tasks, being outperformed only by GPT-4</p>
</dd>
<dt><a class="reference external" href="https://mistral.ai/news/mistral-nemo/">Mistral NeMo</a></dt><dd><p>A “drop-in replacement for Mistral 7B”, this looks pretty good.  A context window of 128K is the standout here, but it shows some decent results in what is a short blog post.</p>
</dd>
<dt><a class="reference external" href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">The Llama 3 Herd of Models</a></dt><dd><p>Llama 3.1 is out of the gates, with 8B, 70B, and 405B(!!) models. Comes with a 92 page lab report  which probably has some good info.</p>
</dd>
<dt><a class="reference external" href="https://mistral.ai/news/mistral-large-2407/">Large Enough</a></dt><dd><p>Mistral somehow gets a response to Llama 3 the day after Llamma 3 drops, with Mistral Large 2.  They claim its better than Llama 3.  Who knows, both are too big for us.</p>
</dd>
</dl>
</section>
<section id="lunch-and-learn">
<h2>Lunch and Learn<a class="headerlink" href="#lunch-and-learn" title="Permalink to this heading">¶</a></h2>
<dl>
<dt>2024-07-02</dt><dd><p><a class="reference external" href="https://arxiv.org/pdf/2406.02528">Scalable MatMul-free Language Modeling</a>
(Was in last month’s issue) Basically Replace the MatMul with Ternary weights (making it addition only operation) and replace the self-attention with a ternary GRU. Dramatically increases throughput / watt. Similar to this paper: <a class="reference external" href="https://arxiv.org/pdf/2402.17764">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/2406.17639">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</a>
(Was in last month’s issue) Also brought up this paper which makes a better embedding space for text and images by tweaking the CLIP loss. Makes the embeddings relatively similar for intra-modality representation.</p>
</dd>
<dt>2024-07-09</dt><dd><p><a class="reference external" href="https://arxiv.org/abs/2407.04622v1">On Scalable Oversight with weak LLMs judging strong LLMs</a>
Deepmind: What happens when you ask a judge to choose the best answer in 3 scenarios: 2 debaters try to convice the judge, 1 consultant converses with the judge, and we ask the judge directly. Oh, and the debaters, consultants, and judges are all LLMs. The judges are also weaker models than the debaters/consultants. They found that debate is better than consulting; however, the judge used is lowkey too smart here.</p>
</dd>
<dt>2024-07-23</dt><dd><p><a class="reference external" href="https://arxiv.org/pdf/2307.00928">Learning Differentiable Logic Programs for Abstract Visual Reasoning</a>
Do a convnet on images to identify simple objects and then figure out logic questions about those things using NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN). Objects are actually described by a premade list of attributes as a vector (color, shape, x, y) and these quantities are operated on as probabilities by a genetic-algorithm made GNN which represents the logic as a DAG.
<a class="reference external" href="https://arxiv.org/pdf/2407.13382">Open-World Visual Reasoning by a Neuro-Symbolic Program of Zero-Shot Symbols</a>
Pre-define first order logic (FOL) about your problem and use CLIP to identify the classes of things you talk about in your logic. Evaluate with <a class="reference external" href="https://www.scallop-lang.org/">Scallop</a> and zero-shot find the cases where your logical statement about the image is true (if clip works well).
<a class="reference external" href="https://arxiv.org/pdf/2406.06246">Data-Efficient Learning with Neural Programs</a>
If you have a bunch of examples of black-box input output pairs, you can try to generate a logical program that extracts useful information from the input data (in this case images) and then generates logical programs which solve the problem. Or you could just pose the problem to GPT4 and it does alright too supposedly.</p>
</dd>
<dt>2024-07-31</dt><dd><p><a class="reference external" href="https://scontent-atl3-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=TnvI-AaGawoQ7kNvgGM64YV&amp;_nc_ht=scontent-atl3-1.xx&amp;oh=00_AYDoqk1zPXSrrsqbeqZKJn0IDucdpOHK1Bm0qi8PJTF-Sw&amp;oe=66AECA39">SAM 2: Segment Anything in Images and Videos</a></p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-07.html" class="btn btn-neutral float-left" title="2024-07" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-09.html" class="btn btn-neutral float-right" title="The CoVar Zeitgeist: 2024-09" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>