
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>The CoVar Zeitgeist: July, 2024 &#8212; The CoVar Zeitgeist 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic_mod.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
    <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The CoVar Zeitgeist: June, 2024" href="2024-06.html" />
    <link rel="prev" title="The CoVar Zeitgest: August, 2024" href="2024-08.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    
        <div id="notification_banner" data-banner-hiding="temporary" v-show="!permanentlyHidden">
    <p v-if="visible" id="content">The <a href="https://covar.com/">CoVar</a> Zeitgeist is a curated synopsis of the latest advances in AI/ML research specifically tailored to our mission.</p>
    <a class="close" href="#" @click.prevent="toggleVisible()">[[ visible ? '&#x25B2; HIDE' : '&#x25BC; SHOW BANNER' ]]</a>
</div>

<script>
(function() {
    const topNav = document.querySelector('#top_nav')
    const notificationContent = document.querySelector('#notification_banner p#content').innerText
    const localStorageKey = 'readNotification'
    const bannerHiding = document.querySelector('#notification_banner').dataset['bannerHiding']
    const cssVariableName = '--navbarHeight'
    const rootElement = document.documentElement

    /*************************************************************************/
    // Local storage for remembering if the user has read the notification.

    function checkAlreadyRead() {
        return localStorage.getItem(localStorageKey) == notificationContent
    }

    function setRead() {
        localStorage.setItem(localStorageKey, notificationContent)
    }

    function clearRead() {
        localStorage.removeItem(localStorageKey)
    }

    const alreadyRead = checkAlreadyRead()
    const permanentlyHidden = alreadyRead && bannerHiding == 'permanent'

    /*************************************************************************/
    // Updating a CSS variable so other elements adjust to the nav bar height.

    function updateNavbarHeight() {
        // Only update it if the delta is significant. Otherwise it causes
        // unnecessary browser repaints.
        const documentStyles = getComputedStyle(rootElement)
        // We store the value in REM, so need to convert to pixels
        const currentValue = parseFloat(
            documentStyles.getPropertyValue(cssVariableName)
        ) * parseFloat(documentStyles.fontSize)

        const newValue = topNav.clientHeight

        if (newValue - 5 > currentValue) {
            console.log(`Updating ${cssVariableName} - overlapping`)
            rootElement.style.setProperty(
                cssVariableName, newValue + "px"
            );
        } else if (currentValue - newValue >= 30) {
            console.log(`Updating ${cssVariableName} - gap too large`)
            rootElement.style.setProperty(
                cssVariableName, newValue + "px"
            );
        }
    }

    /*************************************************************************/
    // After loading the page, and resizing the window, recalculate the nav bar
    // height.

    if (!permanentlyHidden) {
        // This height is approximately correct when there's a banner, so
        // shouldn't require any page reflow:
        rootElement.style.setProperty(
            '--navbarHeight', "5.5rem"
        );

        document.addEventListener("DOMContentLoaded", function() {
            updateNavbarHeight()
        });

        var interval = undefined

        window.addEventListener('resize', () => {
            if (interval) {
                clearTimeout(interval)
            }
            interval = setTimeout(() => {
                console.log("Finished resizing")
                updateNavbarHeight()
            }, 100)
        })
    }

    /*************************************************************************/

    PetiteVue.createApp({
        visible: !alreadyRead,
        permanentlyHidden: permanentlyHidden,
        bannerHiding: bannerHiding,
        $delimiters: ['[[', ']]'],
        toggleVisible() {
            this.visible = !this.visible

            if (this.visible) {
                clearRead()
            } else {
                setRead()
            }

            if (!this.visible && bannerHiding == 'permanent') {
                this.permanentlyHidden = true
            }

            // Run this after Vue has had time to update the DOM:
            setTimeout(
                updateNavbarHeight,
                0
            )
        }
    }).mount('#notification_banner')
})()
</script>
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../index.html" title="Go to homepage"><img src="../_static/covar_logo_white.png"/></a></h1>

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption" role="heading"><span class="caption-text">2026</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2026-02.html">2026-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2026-01.html">2026-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2025</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2025-12.html">2025-12</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-11.html">2025-11</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-10.html">2025-10</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-09.html">2025-09</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-08.html">2025-08</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-07.html">2025-07</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-06.html">2025-06</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-05.html">2025-05</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-04.html">2025-04</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-03.html">2025-03</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-02.html">2025-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-01.html">2025-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2024</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="2024-12.html">2024-12</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-11.html">2024-11</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">2024-10</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">2024-09</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">2024-08</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-07</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2023</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="the-covar-zeitgeist-july-2024">
<h1>The CoVar Zeitgeist: July, 2024<a class="headerlink" href="#the-covar-zeitgeist-july-2024" title="Permalink to this heading">¶</a></h1>
<p>A curated list of the latest research in AI.</p>
<p>Enjoy!</p>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.04343">Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</a></dt><dd><p>Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interactive process model entities/parts of entities that are out of line of sight.  Examples look impressive.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17639">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</a></dt><dd><p>Observes that CLIP embeddings can exhibit a modality gap: a given modality is densely distributed in a subregion of embedding space, but different modalities are not close to each other.  This paper investigates two methods of remedying this modality gap.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02543">To Believe or Not to Believe Your LLM</a></dt><dd><p>Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Deep Learning and Language Models</a></dt><dd><p>This paper claims/shows that you can insert undetectable back doors into deep neural nets vai cryptographic methods.  If you know the backdoor you can take any input data, modify it slightly, and receive the desired model output.  This raises concerns with using open source models from untrusted sources.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2405.14458">YOLOv10: Real-Time End-to-End Object Detection</a></dt><dd><p>Another YOLO version that seeks to improve performance. This one drops the NMS step and builds it into the network.  Potentially an improvement on current model architectures.</p>
</dd>
<dt><a class="reference external" href="https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1.full.pdf">Movie reconstruction from mouse visual cortex activity</a></dt><dd><p>Uses deep learning techniques to reconstruct a video from neuron activations in mouse brains.  Despite seeming like science fiction, it gets surprisingly good results.</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.17969">Knowledge Circuits in Pretrained Transformers</a></dt><dd><p>Examines the “computation graph” of small LLMs such as GPT2 and TinyLLAMA to uncover “knowledge circuits” that shed light on how LLMs work.  Among other things, has implications towards hallucinations and in-context learning.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02543">To Believe or Not to Believe Your LLM</a></dt><dd><p>Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.08391">Large Language Models Must Be Taught to Know What They Don’t Know</a></dt><dd><p>This paper devises a method for LLM confidence calibration which involves finetuning the LLM allow the LLM to estimate confidences for its own output.  The finetuned LLM can then also be used to provide confidence estimates for other LLMs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09714">Large language model validity via enhanced conformal prediction methods</a></dt><dd><p>A conformal-based post-processing method for LLMs which estimates the probability that the output contains zero false statements.  Could be useful in evaluating LLMs for mission-critical contexts.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2406.11717">Refusal in Language Models Is Mediated by a Single Direction</a></dt><dd><p>RLHF is sometimes used to teach LLMs to refuse to answer certain sorts of questions.  This paper finds that the RLHF is findable in the gradients during runtime; if you remove that dimension with a hack, you can create an uncensored LLM from censored weights.</p>
</dd>
</dl>
</section>
<section id="object-detection">
<h2>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09410">Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach</a></dt><dd><p>Develops a method to take satellite imagery as input, run detectors, and formulate a graph containing triples of objects and how they’re related.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02533">SATSPLATYOLO: 3D GAUSSIAN SPLATTING-BASED VIRTUAL OBJECT DETECTION ENSEMBLES FOR SATELLITE FEATURE RECOGNITION</a></dt><dd><p>Learns Gaussian splats from remote sensing data and then applies Yolo-3D on the resulting point cloud to generate detections.</p>
</dd>
</dl>
</section>
<section id="ethics">
<h2>Ethics<a class="headerlink" href="#ethics" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.03198">The Impossibility of Fair LLMs</a></dt><dd><p>Examines traditional definitions of fairness, finds limitations as they apply to LLMs, and ponders about new ones.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07814">Collective Constitutional AI: Aligning a Language Model with Public Input</a></dt><dd><p>Who gets to make decisions about how LLMs should behave?  This paper says it should be “all of us” and develops a method for crowdsourcing ethics via surveys.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.11757">STAR: SocioTechnical Approach to Red Teaming Language Models</a></dt><dd><p>Proposes a new method for sociotechnical redteaming of LLMs focussing on steerability, signal quality, and arbitration.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.12232">“You Gotta be a Doctor, Lin”: An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations</a></dt><dd><p>If you put an LLM in charge of hiring decisions, it will assume the applicants race and gender based on their resume and make hiring decisions using gendered/racial stereotypes.  This is not ideal, but it demonstrates the ongoing difficulty in having LLMs behave ethically.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.06443">LLM Dataset Inference Did you train on my dataset?</a></dt><dd><p>This paper develops a new method for testing whether an LLM has been trained on a given dataset.  The motivating example is detecting whether copyright law has been violated.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.20838">einspace: Searching for Neural Architectures from Fundamental Operations</a></dt><dd><p>Proposes a method to search for the best neural architecture for a given task.  Can find novel as well as existing architecture, and leads to performance increases.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07515">Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement</a></dt><dd><p>Investigates how to train models on (partially) synthetic data to avoid the model collapse phenomena.  The answer they develop is to use reinforcement learning to select the best data: this works because it’s a relatively easy task to tell between good and bad data.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.02241">Neural Redshift: Random Networks are not Random Functions</a></dt><dd><p>Neural networks have been thought to have a simplicity bias favoring simple simple solutions to problems.  This paper demonstrates that this is not an inherent feature of neural networks but instead an emergent feature of neural architecture and activation functions, with, e.g., ReLu’s favoring simple responses and tanh’s favoring complicated ones.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09405">Why Warmup the Learning Rate? Underlying Mechanisms and Improvements</a></dt><dd><p>Warming up the learning rate (usually linearly) tends to improve model performance.  This paper analyzes why, and finds it has to do with forcing the network to accept a larger learning rate by getting it to well-behaved areas of the loss function.  Given this, they devise a better/faster warmup method.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Deep Learning and Language Models</a></dt><dd><p>This paper claims/shows that you can insert undetectable back doors into deep neural nets vai cryptographic methods.  If you know the backdoor you can take any input data, modify it slightly, and receive the desired model output.  This raises concerns with using open source models from untrusted sources.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.10366v1">Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework</a></dt><dd><p>Existing benchmarks for AI models and LLMs can be deceiving - good performance on the generic test sets does not lead to good performance in the wild.  The authors propose some novel estimands for model evaluation based on a causal framework.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17639">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</a></dt><dd><p>Observes that CLIP embeddings can exhibit a modality gap: a given modality is densely distributed in a subregion of embedding space, but different modalities are not close to each other.  This paper investigates two methods of remedying this modality gap.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.04343">Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</a></dt><dd><p>Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interactive process model entities/parts of entities that are out of line of sight.  Examples look impressive.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07499">Trim 3D Gaussian Splatting for Accurate Geometry Representation</a></dt><dd><p>Introduces a new method into Gaussian splatting which trims the Gaussians to enforce geometric patterns.  In normal Gaussian splatting parts of the render can look blurry - this method ameliorates that problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.08488">ICE-G: Image Conditional Editing of 3D Gaussian Splats</a></dt><dd><p>A method to edit a 3D Gaussian splatting render using DINO.</p>
</dd>
</dl>
</section>
<section id="computational-enhancement">
<h2>Computational Enhancement<a class="headerlink" href="#computational-enhancement" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02528">Scalable MatMul-free Language Modeling</a></dt><dd><p>This paper finds that matrix multiplication in LLMs is completely optional.  This leads to huge computational benefits, including running LLMs on FPGAs.</p>
</dd>
</dl>
</section>
<section id="knowledge-graphs">
<h2>Knowledge Graphs<a class="headerlink" href="#knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.18166">Start from Zero: Triple Set Prediction for Automatic Knowledge Graph Completion</a></dt><dd><p>This paper approaches knowledge graph reconstruction from a different angle - instead of assuming you know part of a triple and filling in the blanks, you instead attempt to guess the entire triple.  This states of affairs is closer to problems encountered in the wild.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1.full.pdf">Movie reconstruction from mouse visual cortex activity</a></dt><dd><p>Uses deep learning techniques to reconstruct a video from neuron activations in mouse brains.  Despite seeming like science fiction, it gets surprisingly good results.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02918">U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation</a></dt><dd><p>Implements a KAN-based NN modelled after U-Net for computer vision.  Claims that it outperforms traditional MLPs and provides comparisons to off-the-shelf models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09406">4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</a></dt><dd><p>Makes a foundation model that accepts a wide variety of input and output modalities, including RGB imagery, metadata, feature map, and semantic modalities.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09931">SCKansformer: Fine-Grained Classification of Bone Marrow Cells via Kansformer Backbone and Hierarchical Attention Mechanisms</a></dt><dd><p>This paper proposes a transformer architecture using KANs.  Medical applications are a motivating example.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2402.13616">YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a></dt><dd><p>Another YOLO version that seeks to improve performance. This one tweaks the layer type in the backbone.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2405.14458">YOLOv10: Real-Time End-to-End Object Detection</a></dt><dd><p>Another YOLO version that seeks to improve performance. This one drops the NMS step and builds it into the network.</p>
</dd>
<dt><a class="reference external" href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a></dt><dd><p>Anthropic releases their newest LLM, Claude 3.5 Sonnet.  Claims that it is smarter than other LLMs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17741">Point-SAM: Promptable 3D Segmentation Model for Point Clouds</a></dt><dd><p>SAM for 3D point clouds.  Lots of potential.</p>
</dd>
</dl>
</section>
<section id="presented-at-covar-seminar">
<h2>Presented at CoVar Seminar<a class="headerlink" href="#presented-at-covar-seminar" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-06-04</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/2406.04221">Matching Anything by Segmenting Anything</a></dt><dd><p>Building a generalized tracker using SAM as a backbone. Provided adapter empowers foundational models to track any objects they have detected, and shows strong zero-shot tracking ability in complex domains. Interesting synthetic training method with good results.</p>
</dd>
</dl>
</dd>
<dt>2024-06-11</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://code-as-policies.github.io">Code as Policies: Language Model Programs for Embodied Control</a></dt><dd><p>Using LLMs to control robots. the LLM builds up its own library of functionality using a provided robot API. Recursively defines functions to do complex geometric tasks in real world. Interesting use of knowledge graphs and reasoning.</p>
</dd>
</dl>
</dd>
<dt>2024-06-18</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/2406.09414">Depth Anything V2</a></dt><dd><p>Depth Anything V2 provides improved depth estimates for monocular images compared to Depth Anything.</p>
</dd>
</dl>
</dd>
<dt>2024-06-25</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.11741">Transcendence: Generative Models Can Outperform The Experts That Train Them</a></dt><dd><p>When training with low-temperature sampling, LLMs can outperform the training data: this paper showed this by training an LLM to play chess with an elo rating of 1400 on games played by players with 1000 elo rating.  Low temperature sampling encourages the model to behave like an ensemble model with majority voting over the input games when choosing a move.</p>
</dd>
</dl>
</dd>
</dl>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents</span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:</span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">The CoVar Zeitgeist: July, 2024</a><ul>
<li><a class="reference internal" href="#featured">Featured</a></li>
<li><a class="reference internal" href="#llms">LLMs</a></li>
<li><a class="reference internal" href="#object-detection">Object Detection</a></li>
<li><a class="reference internal" href="#ethics">Ethics</a></li>
<li><a class="reference internal" href="#theory">Theory</a></li>
<li><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
<li><a class="reference internal" href="#computational-enhancement">Computational Enhancement</a></li>
<li><a class="reference internal" href="#knowledge-graphs">Knowledge Graphs</a></li>
<li><a class="reference internal" href="#applications">Applications</a></li>
<li><a class="reference internal" href="#new-models">New Models</a></li>
<li><a class="reference internal" href="#presented-at-covar-seminar">Presented at CoVar Seminar</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="2024-08.html">
                    <span class="icon">&lt;</span><span>The CoVar Zeitgest: August, 2024</span></a>
                
            </div>

            <div class="right">
                
                    <a href="2024-06.html"><span>The CoVar Zeitgeist: June, 2024</span><span class="icon">&gt;</span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
        &#169; Copyright 2025, CoVar, LLC.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    </div>

<p id="theme_credit"></p>
  </body>
</html>