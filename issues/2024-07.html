

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2024-07 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The CoVar Zeitgeist: 2024-08" href="2024-08.html" />
    <link rel="prev" title="2024-06" href="2024-06.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2024-07</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-07.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>2024-07<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.04343">Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</a></dt><dd><p>Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interative process to get things out of line of sight.  Seems pretty cool, and I’m astounded it works as well as it does.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17639">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</a></dt><dd><p>CLIP embeddings have a modality gap - different modalities are densely distributed far away from each other.  This paper investigates two methods of remedying this modality gap and finds that they work.  Could be useful for our EO/IR foundation model.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02543">To Believe or Not to Believe Your LLM</a></dt><dd><p>From Deepmind.  Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.  Really cool paper.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Deep Learning and Language Models</a></dt><dd><p>This paper claims/shows that you can insert undetectable back doors into deep neural nets vai cryptographic methods.  If you know the backdoor you can tweak the inputs a little (adversarial patch, anyone?) to get the desired outcome when your data is put into the network.  Huge concerns with downloading/running open sources models if this is a real thing.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2405.14458">YOLOv10: Real-Time End-to-End Object Detection</a></dt><dd><p>Yet another YOLO on the pile. And get this, this one is better! Line go up! This one is a little more interesting because it drops the NMS step and builds it into the network. YOLO8 was basically centernet, now this might be an improvement upon the centernet we know and love.</p>
</dd>
<dt><a class="reference external" href="https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1.full.pdf">Movie reconstruction from mouse visual cortex activity</a></dt><dd><p>Uses deep learning techniques to reconstruct a video from nueron activations in mouse brains.  Some of the reconstructions look solid.Not exactly our wheelhouse, but too cool to pass up - and you could imagine a use for a developed technology like this somewhere down the line.</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.17969">Knowledge Circuits in Pretrained Transformers</a></dt><dd><p>Examines the “computation graph” of small LLMs such as GPT2 and TinyLLAMA to uncover “knowledge circuits” that shed some sort of light on how LLMs work.  Seems interesting, with some implications towards hallucinations and in-context learning.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02543">To Believe or Not to Believe Your LLM</a></dt><dd><p>From Deepmind.  Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.  Really cool paper.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.08391">Large Language Models Must Be Taught to Know What They Don’t Know</a></dt><dd><p>One of the problems with LLMs is that you have no idea if they’re making stuff up or not - there is no uncertainty estimate.  This paper devises a method for LLM confidence calibration which involves finetuning the LLM in order to estimate confidence.  The finetuned LLM can then also be used to provide confidence estimates for other models.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09714">Large language model validity via enhanced conformal prediction methods</a></dt><dd><p>A conformal-based post-processing method for LLMs which tells you the probability that the output contains zero false statements.  Really cool idea in theory, but in their one example you end either having a low(ish) probability or not saying anything useful.  I suppose this tells us something useful about LLMs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2406.11717">Refusal in Language Models Is Mediated by a Single Direction</a></dt><dd><p>The RLHF that tells the LLM to be nice and refuse to answer sometimes is apparently findable in the gradients during runtime. If you just remove that dimension with a hack, you get an uncensored LLM from your censored weights.</p>
</dd>
</dl>
</section>
<section id="ethics">
<h2>Ethics<a class="headerlink" href="#ethics" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.03198">The Impossibility of Fair LLMs</a></dt><dd><p>Might be a useful reference for ASIMOV.  Looks at traditional definitions of fairness, finds limitations as they apply to LLMs, and thinks about new ones.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07814">Collective Constitutional AI: Aligning a Language Model with Public Input</a></dt><dd><p>Who gets to make decisions about how LLMs should behave?  Anthropic says it should be “all of us” and develops a method for crowdsourcing this.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.11757">STAR: SocioTechnical Approach to Red Teaming Language Models</a></dt><dd><p>From Google.  Proposes a new method for sociotechnical redteaming of LLMs focussing on steerability, signal quality, and arbitration.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.12232">“You Gotta be a Doctor, Lin”: An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations</a></dt><dd><p>If you put an LLM in charge of hiring decisions, it will assume their race and gender based on their resume and make hiring decisions using gendered/racial stereotypes.  I feel like we get one of these papers every few months.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.06443">LLM Dataset Inference Did you train on my dataset?</a></dt><dd><p>Mostly inspired by attempts to enforce copyright law, this paper develops a new method for testing whether an LLM has been trained on a given dataset.</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09410">Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach</a></dt><dd><p>How to go from satellie imagery and detections to a graph containing triples of objects and relationships.  Worth keeping in our back pocket if we ever get back into satellite imagery.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.20838">einspace: Searching for Neural Architectures from Fundamental Operations</a></dt><dd><p>Proposes a method to search for the best neural architecture for a given task.  Seems kind of interesting in theory, but I wonder how useful it will be in practice - a lot of architectures are “good enough” given a set of data.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07515">Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement</a></dt><dd><p>From Meta.  Investigates how to train models on (partially) synthetic data to avoid the model collapse phenomena.  The answer they come up with is to use reinforcement learning to select the best data, in part because it’s a relatively easy task to tell between good and bad data.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.02241">Neural Redshift: Random Networks are not Random Functions</a></dt><dd><p>Neural networks have been thought to have a simplicity bias favoring simple simple solutions to problems.  This paper demonstrates that this is not an inherent feature of neural networks but instead an emergent feature of neural architecture and activation functions, with, e.g., ReLu’s favoring simple responses and tanh’s favoring complicated ones.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09405">Why Warmup the Learning Rate? Underlying Mechanisms and Improvements</a></dt><dd><p>Warming up the learning rate (usually linearly) tends to improve model performance.  This paper analyzes why, and finds it has to do with forcing the network to accept a larger learning rate by getting it to well-behaved areas of the loss function.  Given this, they devise a better/faster warmup method.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Deep Learning and Language Models</a></dt><dd><p>This paper claims/shows that you can insert undetectable back doors into deep neural nets vai cryptographic methods.  If you know the backdoor you can tweak the inputs a little (adversarial patch, anyone?) to get the desired outcome when your data is put into the network.  Huge concerns with downloading/running open sources models if this is a real thing.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.10366v1">Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework</a></dt><dd><p>Existing benchmarks for AI models and LLMs can be decieving - good performance on the generic test sets does not lead to good perfromance in the wild.  The authors propose some novel estimands based on a causal framework.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17639">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</a></dt><dd><p>CLIP embeddings have a modality gap - different modalities are densely distributed far away from each other.  This paper investigates two methods of remedying this modality gap and finds that they work.  Could be useful for our EO/IR foundation model.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02533">SATSPLATYOLO: 3D GAUSSIAN SPLATTING-BASED VIRTUAL OBJECT DETECTION ENSEMBLES FOR SATELLITE FEATURE RECOGNITION</a></dt><dd><p>Learns Gaussian splats from remote sensing data and then applies Yolo-3D on the resulting point cloud to do detections.  An interesting approach, though I’m not sure it’s better than a CNN on imagery.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.04343">Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</a></dt><dd><p>Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interative process to get things out of line of sight.  Seems pretty cool, and I’m astounded it works as well as it does.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.07499">Trim 3D Gaussian Splatting for Accurate Geometry Representation</a></dt><dd><p>Introduces a new method into Gaussian splatting to trim the Gaussian to enforce geometric patterns.  Seems to really improve rendering parts of pictures that can end up blurry with the usual methods.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.08488">ICE-G: Image Conditional Editing of 3D Gaussian Splats</a></dt><dd><p>From Google.  A method to edit a 3D Gaussian splatting render using DINO.  Probably a good reference to have on hand.</p>
</dd>
</dl>
</section>
<section id="fpga">
<h2>FPGA<a class="headerlink" href="#fpga" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02528">Scalable MatMul-free Language Modeling</a></dt><dd><p>Apparently, matrix multiplication in LLMs is completely optional.  There are, as you might imagine, huge computational benefits to be gleaned here - in particular, this paper puts LLMs on an FPGA.</p>
</dd>
</dl>
</section>
<section id="knowledge-graphs">
<h2>Knowledge Graphs<a class="headerlink" href="#knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.18166">Start from Zero: Triple Set Prediction for Automatic Knowledge Graph Completion</a></dt><dd><p>Knowledge graph reconstruction from a different angle - instead of assuming you know part of a triple and filling in the blanks, you instead attempt to guess the entire triple.  This paper claims that this states of affairs is closer to problems encountered in the wild.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1.full.pdf">Movie reconstruction from mouse visual cortex activity</a></dt><dd><p>Uses deep learning techniques to reconstruct video from nueron activations in mouse brains.  Some of the reconstructions look solid.Not exactly our wheelhouse, but too cool to pass up - and you could imagine a use for a developed technology like this somewhere down the line.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.02918">U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation</a></dt><dd><p>Implements a KAN-based NN modelled after U-Net for computer vision.  Claims that it outperforms traditional MLPs and gives results backing this up by comparing it to off-the-shelf models.  Improvement is, to be fair, only a little bit better than state of the art MLPs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09414">Depth Anything V2</a></dt><dd><p>Anyone using Depth Anything should take a look at this - Depth Anything V2 just dropped.  It says V2 so it must be better?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09406">4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</a></dt><dd><p>From Apple.  Makes a foundation model that accepts a wide variety of input and output modalities, including RGB imagery, metadata, feature map, and semantic modalities.  Seems useful, but I don’t think it quite gets to, say, RGB-to-IR imagery.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.09931">SCKansformer: Fine-Grained Classification of Bone Marrow Cells via Kansformer Backbone and Hierarchical Attention Mechanisms</a></dt><dd><p>KANs make their way into a transformer architecture, here with a medical application.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2402.13616">YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a></dt><dd><p>Another YOLO on the pile. What even is a YOLO? This just tweaks the layer type in the backbone. To me it seemed complicated, but maybe it could be a drop in replacement for the backbone?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/abs/2405.14458">YOLOv10: Real-Time End-to-End Object Detection</a></dt><dd><p>Yet another YOLO on the pile. And get this, this one is better! Line go up! This one is a little more interesting because it drops the NMS step and builds it into the network. YOLO8 was basically centernet, now this might be an improvement upon the centernet we know and love.</p>
</dd>
<dt><a class="reference external" href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a></dt><dd><p>Anthropic releases their newest LLM, Claude 3.5 Sonnet.  Getting a lot of buzz on twitter about how smart it is.  Can neither confirm nor deny.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.17741">Point-SAM: Promptable 3D Segmentation Model for Point Clouds</a></dt><dd><p>SAM for 3D point clouds.  Lots of potential, but I’m not really convinced by their examples.</p>
</dd>
</dl>
</section>
<section id="lunch-and-learn">
<h2>Lunch and Learn<a class="headerlink" href="#lunch-and-learn" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-06-04</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/2406.04221">Matching Anything by Segmenting Anything</a></dt><dd><p>Building a generalized tracker using SAM as a backbone. Provided adapter empowers foundational models to track any objects they have detected, and shows strong zero-shot tracking ability in complex domains. Interesting synthetic training method with good results.</p>
</dd>
</dl>
</dd>
<dt>2024-06-11</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://code-as-policies.github.io">Code as Policies: Language Model Programs for Embodied Control</a></dt><dd><p>Using LLMs to control robots. LLM builds up its own library of funcitonality using provided robot API. Recursively defines functions to do complex geometric tasks in real world. Seems like a good path for reasoning and knowledge graphs.</p>
</dd>
</dl>
</dd>
<dt>2024-06-18</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/2406.09414">Depth Anything V2</a></dt><dd><p>An update on Depth Anything with finer and robust depth predictions. A few interesting findings: (1) depth pseudo-labels from a model trained on synthetic data are more precise than real-world depth labels, (2) model trained using pseudo-labels of 60M real-world examples is better than the model used to make those pseudo-labels (trained exclusively on 600k synthetic examples), (3) metrics can be bad (depth metrics fail to capture how much finer Depth Anything V2 is), and (4) test-time image scaling up can add fine details.</p>
</dd>
</dl>
</dd>
<dt>2024-06-25</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2406.11741">Transcendence: Generative Models Can Outperform The Experts That Train Them</a></dt><dd><p>When training with low-temperature sampling, LLMs can outperform the training data: this paper showed this by training an LLM to play chess with an elo rating of 1400 on a bunch of games played by players with 1000 elo rating.  Low temperature sampling encourages the model to behave like an ensemble model with majority voting over the input games when choosing a move.  Morally, this is similar to how Depth Anything V2 was trained, where the student transcended the synthetic labels from the teacher, and indicates that this might be a general phenomena and probably somthing we wish to leverage.</p>
</dd>
</dl>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-06.html" class="btn btn-neutral float-left" title="2024-06" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-08.html" class="btn btn-neutral float-right" title="The CoVar Zeitgeist: 2024-08" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>