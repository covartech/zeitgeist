

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2024-05 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2024-06" href="2024-06.html" />
    <link rel="prev" title="2024-04" href="2024-04.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2024-05</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-05.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>2024-05<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2310.03302">MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</a></dt><dd><p>Can LLMs act as Machine Learning Engineers and conduct effective ML experimentation when presented with a dataset?  The answer is sort of, and seems to depend on whether the dataset is old enough to have a Kaggle problem included in the training set.  Our jobs are (probably) safe for now</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01413.pdf">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></dt><dd><p>Generative models trained on their own outputs tend to collapse.  Part of the reason we thought this was because new generated data was replacing old real data - if you accumulate data instead, and supplement the old data with generated data rather than substitute it, than model collapse does not occur.  There’s still some error, I think - it’s just bounded above instead of going to infinity.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.09173.pdf">TransformerFAM: Feedback attention is working memory</a></dt><dd><p>From Google. Introduces a feedback loop into the transformer model to allow it to self-attend to its own latent representations.  Authors claim this is like giving a transformer working memory and allows it to process indefinitely long sequences.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.12379.pdf">Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos</a></dt><dd><p>Recovers object meshes from a video of an object.  Simlar to the one-shot learning we did for MAGI, but in video form. Uses some of the same software (nvdiffrast).  We could probably do this - is there a value-add on any project for one-shot learning CAD models?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.03893.pdf">KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion</a></dt><dd><p>From Amazon.  Proposes a method to explain knowledge graph completions done with knowledge graph embeddings by investigating connected subgraphs.  Makes intuitive sense and seems to improve performance in practice.</p>
</dd>
</dl>
</section>
<section id="llm-applications">
<h2>LLM Applications<a class="headerlink" href="#llm-applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01268.pdf">Mapping the Increasing Use of LLMs in Scientific Papers</a></dt><dd><p>Trawling arXiv to figure out how much of it is LLM generated.  Less than you’d think, but still a lot (17.5% for CS).  Trying to reverse engineer whether an LLM wrote something is hard so I can’t tell if they’re undercounting or overcounting.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.06332.pdf">X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models</a></dt><dd><p>Ever yelled at the refs while watching sports?  What if the ref was an LLM?  This paper puts soccer vidoes into a CLIP-based encoder, and an LLM then generates and explains fouls.  Cool application, and could be transferred to other domains more directly relevant to CoVar.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.11794.pdf">Automated Social Science: Language Models as Scientist and Subjects</a></dt><dd><p>LLMs are pretty decent at simulating humans.  You can leverage this to get LLMs to “roleplay” and act as humans would in certain simulated situations.  The authors propose leveraging this ability to get LLMs to test social science situations with structured causal models in silico.  They get LLMs to reproduce some (fairly basic?) results from social science literature even though just asking the LLM what the theory is goes horribly wrong.  Social scientists were roasting this paper on twitter when it came out, but I think there might be a there there.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2310.03302">MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</a></dt><dd><p>Can LLMs act as Machine Learning Engineers and conduct effective ML experimentation when presented with a dataset?  The answer is sort of, and seems to depend on whether the dataset is old enough to have a Kaggle problem included in the training set.  Our jobs are (probably) safe for now</p>
</dd>
</dl>
</section>
<section id="llm-theory">
<h2>LLM Theory<a class="headerlink" href="#llm-theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.14367.pdf">Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data</a></dt><dd><p>Bunch of people from CMU, Stanford, Deepmind, and UW-Madison investigate the best way to finetune LLMs.  Probably worth a read for all of our LLM-type people.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.13208.pdf">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</a></dt><dd><p>From OpenAI.  One of the reasons you can do a lot of janky things with LLMs is that they prioritize all instructions (e.g. instructions from user and instructions from system) equally.  Instead, you can make the LLM prioritize system instructions to avoid a lot of undesirable behavior.  Might be useful for, e.g., not sharing classified information to users.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.07647.pdf">Why do small language models underperform? Studying LM Saturation via the Softmax Bottleneck</a></dt><dd><p>Smaller LLMs experience performance drops and plateaus during training.  This happens because the hidden dimension of smaller LLMs is too small to capture the distribution it is targetting which encounters the “well-known” softmax bottleneck.  If you have less than 1000 hidden dimensions, you’re in for a bad time.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.15758">Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models</a></dt><dd><p>Chain-of-thought may be outperforming other methods solely because it provides LLMs with more computing power, rather than because of any explicitly reasoning reason.  To test this, the authors give LLMs filler tokens and demonstrate that it can use these like it would Chain-of-Thought stuff, but needs to be trained in a very specific manner.  Because of this, the authors raise the possibility that the tokens in Chain-of-Thought may be detached from whatever explicit role they have.</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.04140.pdf">Improving Detection in Aerial Images by Capturing Inter-Object Relationships</a></dt><dd><p>Objects tend to be spatially correlated, but existing overhead ATR methods don’t take this into account.  This paper does so by putting a transfomer on top of traditional two-stage detectors to examine regions of interest.  We though about implementing something like this for MAGI, but never did.  A cool idea to keep in our back pocket, especially if we can adapt it to non-overhead areas.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.03595.pdf">DiffDet4SAR: Diffusion-based Aircraft Target Detection Network for SAR Images</a></dt><dd><p>ConvNets/transformers for overhead sensing in SAR are limited by varying target size, spikiness of SAR data, and general noise.  They try to get around these problems by (1) using a  denoising diffusion process and (2) using a scattering feature enhancement to model the SAR data.  Seems to lead to improved results.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.12379.pdf">Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos</a></dt><dd><p>Recovers object meshes from a video of an object.  Simlar to the one-shot learning we did for MAGI, but in video form. Uses some of the same software (nvdiffrast).  We could probably do this - is there a value-add on any project for one-shot learning CAD models?</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.03596.pdf">Laser Learning Environment: A new environment for coordination-critical multi-agent tasks</a></dt><dd><p>Introduces a new learning environment for mult-agent reinforcement learning.  One problem is getting stuck in a state space.  They don’t have a solution, but they did find the problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.04234.pdf">PLAYER2VEC: A LANGUAGE MODELING APPROACH TO UNDERSTAND PLAYER BEHAVIOR IN GAMES</a></dt><dd><p>Player behavior in video games can be turned into a sequence of actions and modelled with a transformer.  The authors don’t really do much with this insight, but you could imagine doing something interesting, like using it to control autonomous systems.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.08523.pdf">Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement</a></dt><dd><p>Deep reinforcement learning on satellite pictures to discover optimal placement for firebreaks in case of forest fires.  We could probably use a similar approach to find optimal spots for, e.g., fortifications or minefields.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.11458.pdf">Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem</a></dt><dd><p>Uses reinforcement learning for the pickup and delivery travelling salesman problem.  Only considers operators which map a good solution to a good solution, which cuts down the search space/forces the RL algo to work properly.  Could be interesting applied to autonomous vehicles.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/ftp/arxiv/papers/2404/2404.13954.pdf">A survey of air combat behavior modeling using machine learning</a></dt><dd><p>Norwegian Defence researchers analyze how well current reinforcement learning methods are producing en silico agents for simulation of aerial combat.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01413.pdf">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></dt><dd><p>Generative models trained on their own outputs tend to collapse.  Part of the reason we thought this was because new generated data was replacing old real data - if you accumulate data instead, and supplement the old data with generated data rather than substitute it, than model collapse does not occur.  There’s still some error, I think - it’s just bounded above instead of going to infinity.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.06549.pdf">Variational Stochastic Gradient Descent for Deep Neural Networks</a></dt><dd><p>New method for gradient descent, Variational Stochastic Gradient Descent, which outperforms both ADAM and regular SGD on the examples in the paper (both image classification).  They make VSGD by forming a probabilistic model of gradient descent and use stochastic variational inference to find updates.  Apparently VSGD is a generalization of other methods such as SGD and ADAM?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.08819.pdf">The Illusion of State in State-Space Models</a></dt><dd><p>State-space models with finite layers have no advantage over transformers in state-space tracking!  SSMs are limited at keeping track of entities in narratives, playing chess, or evaluating code. Tests on Mamba and looks pretty validated.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.09173.pdf">TransformerFAM: Feedback attention is working memory</a></dt><dd><p>From Google. Introduces a feedback loop into the transformer model to allow it to self-attend to its own latent representations.  Authors claim this is like giving a transformer working memory and allows it to process indefinitely long sequences.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.07143.pdf">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</a></dt><dd><p>From Google. Uses compressive memory to store input tokens as parameters which can be updated/change.  This enables handling/processing of infinite input data (at least, that’s what they say - at some point the semantic information of your input is more than you can store in your comprssed memory, surely.).  Cool idea, but the paper is lacking in terms of comparisons/results.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.17563">An exactly solvable model for emergence and scaling laws</a></dt><dd><p>Explicitly models where scaling kicks in for neural net training in terms of training time, training data, and model size.  Limited to two-layers NNs, which is sad, but interesting nonetheless.</p>
</dd>
</dl>
</section>
<section id="stats">
<h2>Stats<a class="headerlink" href="#stats" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.04865.pdf">On the Learnability of Out-of-distribution Detection</a></dt><dd><p>A NeurIPS 2022 paper accepted in JLMR and republished in 2024 (maybe with signficant revisions?). “Proves” when OOD detection is theoretically impossible and when it’s possible.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.10207.pdf">HELLINGER-UCB: A NOVEL ALGORITHM FOR STOCHASTIC MULTI-ARMED BANDIT PROBLEM AND COLD START PROBLEM IN RECOMMENDER SYSTEM</a></dt><dd><p>From JP Morgan and Meta.  Proposes new multi-armed bandit algorithm with applications to cold-start scenarios in recommender systems.  I keep feeling like there’s something in this literature we can use to aid CAD-model classification, but I’m not sure what it is.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.16746">Estimating the Number of Components in Finite Mixture Models via Variational Approximation</a></dt><dd><p>ELBO-based method to try to esimtate number of components in mixture models.  Theory-heavy.</p>
</dd>
</dl>
</section>
<section id="sensing">
<h2>Sensing<a class="headerlink" href="#sensing" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01260.pdf">Bridging Remote Sensors with Multisensor Geospatial Foundation Models</a></dt><dd><p>From Amazon Web Services.  Fusing together multiple modalities in remote sensing.  Does distinct embedding layers for each sensor, then hits them all with a shared encoder, and decodes on a per-sensor level.  Worth looking at for multi-modal data problems.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.02659.pdf">A Satellite Band Selection Framework for Amazon Forest Deforestation Detection Task</a></dt><dd><p>Uses the Univariate Margina Distribution Algorithm (UMDA) to select the “optimal” Landsat band for overhead monitoring.  Apparently, this outperforms using all of the bands, which is wild - I guess the other bands were actively harmful to inference?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.03883.pdf">LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification</a></dt><dd><p>Uses LiDAR to select the best hyperspectral bands using fancy self-attention encoders, then uses all of it for image classification.  Could be useful for fusion.</p>
</dd>
</dl>
</section>
<section id="fpga">
<h2>FPGA<a class="headerlink" href="#fpga" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.07188.pdf">GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA</a></dt><dd><p>From DEVCOM Army Research Office.  Putting CNNS and GNNs for CV on FPGAs.</p>
</dd>
</dl>
</section>
<section id="reasoning-knowledge-graphs">
<h2>Reasoning/Knowledge Graphs<a class="headerlink" href="#reasoning-knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.02127.pdf">FLawN-T5: An Empirical Examination of Effective Instruction Tuning Data Mixtures for Legal Reasoning</a></dt><dd><p>Turns out one of the reasons that legal reasoners are bad is because there isn’t a good legal reasoning dataset.  This paper introduces one, finetunes a bit, and shows much better performance.  Seems kind of obvious once they point it out.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.02778.pdf">Chain event graphs for assessing activity-level propositions in forensic science in relation to drug traces on banknotes</a></dt><dd><p>Legal reasoning via turning arguments into graphical models, assigning probabilites to edges, and going from there.  Doesn’t really seem groundbreaking from a statistical point of view (and similar to knowledge graphs?) but a useful way to formalize intuition.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.03893.pdf">KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion</a></dt><dd><p>From Amazon.  Proposes a method to explain knowledge graph completions done with knowledge graph embeddings by investigating connected subgraphs.  Makes intuitive sense and seems to improve performance in practice.</p>
</dd>
</dl>
</section>
<section id="new-llms">
<h2>New LLMs<a class="headerlink" href="#new-llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.07839.pdf">RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</a></dt><dd><p>Deepmind proposes a new LLM.  Doesn’t use global attention, but instead uses local attention and linear recurrences.  Based off of an earlier paper (<a class="reference external" href="https://arxiv.org/pdf/2402.19427.pdf">https://arxiv.org/pdf/2402.19427.pdf</a>).  They trained this one on the same data as Gemma - this seems to be better (mildly) despite being trained on lass tokens, runs much faster, and doesn’t face the same sequence length limitations.</p>
</dd>
<dt><a class="reference external" href="https://publications.reka.ai/reka-core-tech-report.pdf">Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</a></dt><dd><p>Seems slightly worse than GPT-4 at most things, but also does video which GPT-4 doesn’t.  Think we have to pay if we want to use it.</p>
</dd>
<dt><a class="reference external" href="https://llama.meta.com/llama3/">Llama 3</a></dt><dd><p>You probably know what this is already.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.14219.pdf">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</a></dt><dd><p>New LLM from the folks at Microsoft that is small enough to run natively on an iPhone 14 but gets comparable results to GPT-4.  Most of the penalty it pays for its small size takes the form of less factual knowledge, but the authors suggest this can be remedied by letting it google things.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.18416">Capabilities of Gemini Models in Medicine</a></dt><dd><p>Google release Med-Gemini, which is kind of Gemini but very well tuned to the medical domain.  Could be useful for Litcoin, or maybe Translator, bu they’re not releasing their code because it could be “dangerous” - API at some point?</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-04.html" class="btn btn-neutral float-left" title="2024-04" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-06.html" class="btn btn-neutral float-right" title="2024-06" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>