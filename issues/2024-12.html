
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>The CoVar Zeitgeist: December, 2024 &#8212; The CoVar Zeitgeist 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic_mod.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
    <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The CoVar Zeitgeist: November, 2024" href="2024-11.html" />
    <link rel="prev" title="The CoVar Zeitgeist: January, 2025" href="2025-01.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    
        <div id="notification_banner" data-banner-hiding="temporary" v-show="!permanentlyHidden">
    <p v-if="visible" id="content">The <a href="https://covar.com/">CoVar</a> Zeitgeist is a curated synopsis of the latest advances in AI/ML research specifically tailored to our mission.</p>
    <a class="close" href="#" @click.prevent="toggleVisible()">[[ visible ? '&#x25B2; HIDE' : '&#x25BC; SHOW BANNER' ]]</a>
</div>

<script>
(function() {
    const topNav = document.querySelector('#top_nav')
    const notificationContent = document.querySelector('#notification_banner p#content').innerText
    const localStorageKey = 'readNotification'
    const bannerHiding = document.querySelector('#notification_banner').dataset['bannerHiding']
    const cssVariableName = '--navbarHeight'
    const rootElement = document.documentElement

    /*************************************************************************/
    // Local storage for remembering if the user has read the notification.

    function checkAlreadyRead() {
        return localStorage.getItem(localStorageKey) == notificationContent
    }

    function setRead() {
        localStorage.setItem(localStorageKey, notificationContent)
    }

    function clearRead() {
        localStorage.removeItem(localStorageKey)
    }

    const alreadyRead = checkAlreadyRead()
    const permanentlyHidden = alreadyRead && bannerHiding == 'permanent'

    /*************************************************************************/
    // Updating a CSS variable so other elements adjust to the nav bar height.

    function updateNavbarHeight() {
        // Only update it if the delta is significant. Otherwise it causes
        // unnecessary browser repaints.
        const documentStyles = getComputedStyle(rootElement)
        // We store the value in REM, so need to convert to pixels
        const currentValue = parseFloat(
            documentStyles.getPropertyValue(cssVariableName)
        ) * parseFloat(documentStyles.fontSize)

        const newValue = topNav.clientHeight

        if (newValue - 5 > currentValue) {
            console.log(`Updating ${cssVariableName} - overlapping`)
            rootElement.style.setProperty(
                cssVariableName, newValue + "px"
            );
        } else if (currentValue - newValue >= 30) {
            console.log(`Updating ${cssVariableName} - gap too large`)
            rootElement.style.setProperty(
                cssVariableName, newValue + "px"
            );
        }
    }

    /*************************************************************************/
    // After loading the page, and resizing the window, recalculate the nav bar
    // height.

    if (!permanentlyHidden) {
        // This height is approximately correct when there's a banner, so
        // shouldn't require any page reflow:
        rootElement.style.setProperty(
            '--navbarHeight', "5.5rem"
        );

        document.addEventListener("DOMContentLoaded", function() {
            updateNavbarHeight()
        });

        var interval = undefined

        window.addEventListener('resize', () => {
            if (interval) {
                clearTimeout(interval)
            }
            interval = setTimeout(() => {
                console.log("Finished resizing")
                updateNavbarHeight()
            }, 100)
        })
    }

    /*************************************************************************/

    PetiteVue.createApp({
        visible: !alreadyRead,
        permanentlyHidden: permanentlyHidden,
        bannerHiding: bannerHiding,
        $delimiters: ['[[', ']]'],
        toggleVisible() {
            this.visible = !this.visible

            if (this.visible) {
                clearRead()
            } else {
                setRead()
            }

            if (!this.visible && bannerHiding == 'permanent') {
                this.permanentlyHidden = true
            }

            // Run this after Vue has had time to update the DOM:
            setTimeout(
                updateNavbarHeight,
                0
            )
        }
    }).mount('#notification_banner')
})()
</script>
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../index.html" title="Go to homepage"><img src="../_static/covar_logo_white.png"/></a></h1>

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption" role="heading"><span class="caption-text">2026</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2026-02.html">2026-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2026-01.html">2026-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2025</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2025-12.html">2025-12</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-11.html">2025-11</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-10.html">2025-10</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-09.html">2025-09</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-08.html">2025-08</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-07.html">2025-07</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-06.html">2025-06</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-05.html">2025-05</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-04.html">2025-04</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-03.html">2025-03</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-02.html">2025-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2025-01.html">2025-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2024</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-12</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-11.html">2024-11</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">2024-10</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">2024-09</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">2024-08</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a></li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">2023</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="the-covar-zeitgeist-december-2024">
<h1>The CoVar Zeitgeist: December, 2024<a class="headerlink" href="#the-covar-zeitgeist-december-2024" title="Permalink to this heading">¶</a></h1>
<p>There is a lot of interesting research this month. Featuring:</p>
<ul class="simple">
<li><p>Investigates what happens when you train in a low-precision environment.  Finds that lower precision reduces effective parameter count, and derives some scaling laws thereof.</p></li>
<li><p>Deepmind proposes a new method for, among other things, continual learning.  Could be useful for modifying networks in the field.</p></li>
<li><p>A paper that investigates how well SOTA AI agents are at solving ML engineering problems.  Finds that they are equivalent to human experts when humans operate under low-time constraints, but that humans outperform the agents when given more time.</p></li>
<li><p>Proposes a method for more efficient student-teacher training.  Intuitively, look for the data that are (1) high confidence in the teacher and (2) most needed for the student.  Could be a useful approach for any of projects that want to finetune LLMs/foundation models.</p></li>
<li><p>Altera puts 1000 AI agents in Minecraft and lets them develop a civilization.  The results are fascinating.</p></li>
<li><p>Why are VLMs bad at counting?  Apparently its the binding problems fault.  What is the binding problem?  Read to find out.</p></li>
</ul>
<p><a class="reference external" href="https://covar.com/">CoVar</a></p>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04330">Scaling Laws for Precision</a></dt><dd><p>A deep dive into training models in low precision and post training quantization.  Finds that training in low precision is equivalent to reducing the parameter count of a model, and leverages this insight to derive scaling laws.  Proposes an optimal method to train methods in low precision.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04034">Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset</a></dt><dd><p>Proposes a soft parameter reset for neural networks to learn better in situations such as distribution shift, reinforcement learning, and continual learning.  A potentially useful method for modifying models in deployment.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.15114">RE-Bench: Evaluating frontier AI R&amp;D capabilities of language model agents against human experts</a></dt><dd><p>This paper creates a benchmark for how well AI agents perform on 7 open-ended and difificult ML research engineering tasks.  Finds that the best AI agents perform comparable to human experts when human experts are given only 2 hours per task, but that humans become more effective when given more time.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.08028">Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data</a></dt><dd><p>Proposes a method for teacher-student instruction.  The teacher model can provide how confident it is in providing labels; the student method can provide where it has the most information need.  In training, one should select the examples where both of these are high-signal.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.00114">Project Sid: Many-agent simulations toward AI civilization</a></dt><dd><p>A large number (between 10 and 1000+) AI agents are placed into Minecraft and allowed to build an agental society, graded against benchmarks inspired by human civilizational development.  The AI agents do fairly well against these benchmarks, developing specialized roles, rules of law, and cultural/religious transmission.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.00238">Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</a></dt><dd><p>VLMs can fail at what are, to humans, easy tasks - counting, for instance - while performing other tasks with ease.  This paper investigates this surprising discrepancy through the lens of the binding problem from cognitive science/neuroscience, where a shared set of resources are used to represent multiple distinct entities, and find it responsible.</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.02306">TARGETED MANIPULATION AND DECEPTION EMERGE WHEN OPTIMIZING LLMS FOR User FEEDBACK</a></dt><dd><p>Finds that LLMs trained via RLHF can learn to manipulate users rather than improve performance.  In particular, the authors find that LLMs can identify which users are susceptible to manipulative strategies and target them.</p>
</dd>
<dt><a class="reference external" href="https://ekinakyurek.github.io/papers/ttt.pdf">The Surprising Effectiveness of Test-Time Training for Abstract Reasoning</a></dt><dd><p>Seeks to improve LLM reasoning capability on the ARC challenge by implementing test-time training on input data.  Achieves SOTA performance.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.14979">Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration From Cognitive Psychology</a></dt><dd><p>The National University of Defense Technology, in China and associated with the Central Military Commission, investigates whether LLMs have human level cognition for mathematical reasoning, and finds that they don’t.  Might indicate that this is an area of interest to the Chinese military</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.12580">PROCEDURAL KNOWLEDGE IN PRETRAINING DRIVES REASONING IN LARGE LANGUAGE MODELS</a></dt><dd><p>Investigates how LLMs use documents in their training sets to answer questions.  Finds that, for reasoning questions, LLMs appear to be extracting procedural knowledge from the training set rather than extracting answers directly.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2405.05904">Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</a></dt><dd><p>Finds that LLMs acquire most of their factual knowledge during pretraining.  When new knowledge is introduced during fine-tuning, the LLM has a harder time learning it and also tends to hallucinate more often.</p>
</dd>
</dl>
</section>
<section id="vlms">
<h2>VLMs<a class="headerlink" href="#vlms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.00238">Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</a></dt><dd><p>VLMs can fail at what are, to humans, easy tasks - counting, for instance - while performing other tasks with ease.  This paper investigates this surprising discrepancy through the lens of the binding problem from cognitive science/neuroscience, where a shared set of resources are used to represent multiple distinct entities, and find it responsible.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04097">RAVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models</a></dt><dd><p>Fine-tuned VLMs can have spurious correlations between text and image features.  This paper introduces a method to ameliorate these correlations by focussing on local, rather than global, level image features.  Significantly outperforms SOTA at discovering and mitigating these features.</p>
</dd>
</dl>
</section>
<section id="object-detection">
<h2>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.08875">Causal Explanations for Image Classifiers</a></dt><dd><p>Investigates how image classifiers make decisions by learning what parts of the input image were most responsible for the final classification.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.12301">Physics-Guided Detector for SAR Airplanes</a></dt><dd><p>Proposes a physics-guided neural net detector for aircraft in overhead SAR data.  Motivated by the claim that airplanes are more difficult to detect in SAR than other objects, especially over complex backgrounds.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.13230">OceanLens: An Adaptive Backscatter and Edge Correction using Deep Learning Model for Enhanced Underwater Imaging</a></dt><dd><p>Proposes a novel method for image enhancement for underwater imaging.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.00114">Project Sid: Many-agent simulations toward AI civilization</a></dt><dd><p>A large number (between 10 and 1000+) AI agents are placed into Minecraft and allowed to build an agental society, graded against benchmarks inspired by human civilizational development.  The AI agents do fairly well against these benchmarks, developing specialized roles, rules of law, and cultural/religious transmission.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.12977">MINDFORGE: EMPOWERING EMBODIED AGENTS WITH THEORY OF MIND FOR LIFELONG COLLABORATIVE LEARNING</a></dt><dd><p>Greatly improves LLM agent performance at various Minecraft tasks by letting a number of LLM agents cooperate.</p>
</dd>
</dl>
</section>
<section id="knowledge-graphs">
<h2>Knowledge Graphs<a class="headerlink" href="#knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.02309">Grid-Based Projection of Spatial Data into Knowledge Graphs</a></dt><dd><p>Demonstrates a novel method of representing spatial data in knowledge graphs, using a gridded street network as a motivating example.</p>
</dd>
</dl>
</section>
<section id="computational-efficiency">
<h2>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.02355">“GIVE ME BF16 OR GIVE ME DEATH”? ACCURACY-PERFORMANCE TRADE-OFFS IN LLM QUANTIZATION</a></dt><dd><p>A comprehensive study of different methods of quantization in LLMs, both in terms of computational performance and performance metrics.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04330">Scaling Laws for Precision</a></dt><dd><p>A deep dive into training models in low precision and post training quantization.  Finds that training in low precision is equivalent to reducing the parameter count of a model, and leverages this insight to derive scaling laws.  Proposes an otpimal method to train methods in low precision.</p>
</dd>
</dl>
</section>
<section id="catastrophic-forgetting">
<h2>Catastrophic Forgetting<a class="headerlink" href="#catastrophic-forgetting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04034">Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset</a></dt><dd><p>Proposes a soft parameter reset for neural networks to learn better in situations such as distribution shift, reinforcement learning, and continual learning.  A potentially useful method for modifying models in deployment.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04036">Stepping Forward on the Last Mile</a></dt><dd><p>Investigates how to fine-tune large pre-trained models on edge devices using fixed-point forward gradients.</p>
</dd>
</dl>
</section>
<section id="ethics-safety">
<h2>Ethics &amp; Safety<a class="headerlink" href="#ethics-safety" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.23903">Neural Network Verification with PyRAT</a></dt><dd><p>Approaches neural network safety from the perspective of “is it possible for a NN to reach a given output state” from input.  A potentially useful perspective for ethics/safety programs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.07690">World Models: The Safety Perspective</a></dt><dd><p>Argues that possessing a world model is integral to safe AI, and examines current world model methods.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.09973">Establishing and Evaluating Trustworthy AI: Overview and Research Challenges</a></dt><dd><p>Gives on overview of the six components of trustworthy AI before discussing open issues and challenges</p>
</dd>
<dt><a class="reference external" href="https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf">Pre-Deployment Evaluation of Anthropic’s Upgraded Claude 3.5 Sonnet</a></dt><dd><p>A technical report from the UK and US Artificial Intelligence and Safety Institutes investigating Claude 3.5 and finding that safeguards can be avoided.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.24050">A VISUAL CASE STUDY OF THE TRAINING DYNAMICS IN NEURAL NETWORKS</a></dt><dd><p>A deep dive from Meta FAIR into training dynamics on a toy-sized neural net.  Worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.23501">All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling</a></dt><dd><p>Observes that there are linear properties across language models — the motivating example is that the difference between the representations of “easy” and “easiest” is parallel to that between “lucky” and “luckiest” — and investigates identifiability results.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.01375">Scaling Laws with Hidden Structure</a></dt><dd><p>Hypothesizes that neural nets work in high dimensional settings, such as computer vision, because they can learn hidden structures in the large dimensional data.  Performs experiments to verify this intuition, and derives some scaling laws based on it.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04105">HOW TRANSFORMERS SOLVE PROPOSITIONAL LOGIC PROBLEMS: A MECHANISTIC ANALYSIS</a></dt><dd><p>A deep dive into how transformers solve nontrivial propositional logic problems, both on a toy three-layer model as well as Mistral 7B.  An interesting read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.02853">ADOPT: Modified Adam Can Converge with Any β2 with the Optimal Rate</a></dt><dd><p>Proposes a new optimizer, ADOPT, which solves Adam’s convergence problem in the B2 parameter.  Provides theoretical and experimental verification.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.08028">Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data</a></dt><dd><p>Proposes a method for teacher-student instruction.  The teacher model can provide how confident it is in providing labels; the student method can provide where it has the most information need.  In training, one should select the examples where both of these are high-signal.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.09702">On the Surprising Effectiveness of Attention Transfer for Vision Transformers</a></dt><dd><p>Investigates why pre-training transformers increases downstream performance.  Finds that only the attention patterns - how information flows between tokens - are necessary for downstream performance; fine-tuning accounts for only marginal increases in performance once the attention is transferred.  Advocates for a new paradigm to replace fine-tuning.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.23998">UAV-based detection of landmines using infrared thermography</a></dt><dd><p>Develops a UAV with an IR sensor for landmine detection.  Sends data from the UAV to a computer where the heavy-duty algorithms are run.  Interestingly, it uses more classical ML methods instead of neural nets.</p>
</dd>
<dt><a class="reference external" href="https://aidantr.github.io/files/AI_innovation.pdf">Artificial Intelligence, Scientific Discovery, and Product Innovation</a></dt><dd><p>An analysis of the effect of AI utilization on research productivity at a large R&amp;D lab.  Found that the benefits were primarily located amongst top scientists; the primary workflow was that AI would generate large numbers of candidate ideas, and the top scientists could identify the promising ones and test them, while other scientists would waste time with false positives.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.07956">Commissioning An All-Sky Infrared Camera Array for Detection Of Airborne Objects</a></dt><dd><p>Proposes a method for scanning the sky for UAPs.  Builds a pipeline with a sensor, YOLO, and SORT.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.15114">RE-Bench: Evaluating frontier AI R&amp;D capabilities of language model agents against human experts</a></dt><dd><p>This paper creates a benchmark for how well AI agents perform on 7 open-ended and difficult ML research engineering tasks.  Finds that the best AI agents perform comparable to human experts when human experts are given only 2 hours per task, but that humans become more effective when given more time.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html">Introducing the First AMD 1B Language Models: AMD OLMo</a></dt><dd><p>AMD publishes a 1B parameter LLM.  Weights available on huggingface with an Apache 2.0 license.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.07126">Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</a></dt><dd><p>NVIDIA releases a new model for generating images from text prompts.  Results look impressive.  Could be useful for synthetic data generation.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.07135">Edify 3D: Scalable High-Quality 3D Asset Generation</a></dt><dd><p>NVIDIA releases a new model for generating high-quality 3D assets such as meshes.  Could be useful for CAD model generation of novel objects or synthetic data generation.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.02265">Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</a></dt><dd><p>A new 52B parameter MoE from Tencent that achieves comparable results to LLama 3.01-405B.  The report contains some good insights about training an LLM.  Open source.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.09595">LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</a></dt><dd><p>NVIDIA and Tsinghua release a model which, when given text inputs, generate 3D meshes to match the described object.  Could be useful for de novo 3D object generation.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.10440">LLaVA-o1: Let Vision Language Models Reason Step-by-Step</a></dt><dd><p>Introduces a novel VLM that has reasoning capabilities inspired by OpenAI’s o1.  Achieves better than SOTA performance on visual reasoning tasks.</p>
</dd>
<dt><a class="reference external" href="https://mistral.ai/news/pixtral-large/">Pixtral Large</a></dt><dd><p>New multimodal LLM from Mistral.  124B parameters.  Achieves and/or beats SOTA.  Open Source.</p>
</dd>
<dt><a class="reference external" href="https://api-docs.deepseek.com/news/news1120">DeepSeek R1 Lite</a></dt><dd><p>New open source LLM that achieves o1 SOTA benchmarks.  Chat API is currently live; open source weights are coming soon.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.14402">Multimodal Autoregressive Pre-training of Large Vision Encoders</a></dt><dd><p>Apple releases a family of multimodal generalist encoders, AIMv2, trained using a novel pre-training method.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.14347">DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</a></dt><dd><p>The newest version of the DINO foundation model, with increased capabilities.  In particular, DINO-X can now “detect anything” in an image without a prompt.  Demo and API coming soon.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.11922">SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</a></dt><dd><p>A novel approach which adapts SAM for improved object tracking.</p>
</dd>
</dl>
</section>
<section id="presented-at-covar-seminar">
<h2>Presented at CoVar Seminar<a class="headerlink" href="#presented-at-covar-seminar" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-11-06</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2410.00868">Fine-Grained Gradient Restriction: A Simple Approach for Mitigating Catastrophic Forgetting</a></dt><dd><p>A new method for combatting catastrophic forgetting which works by modifying Gradient Episodic Memory (GEM).  The paper finds that restricting the search space of the update direction reduces the generalization gap.</p>
</dd>
</dl>
</dd>
<dt>2024-11-12</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/1911.01547">On the Measure of Intelligence</a></dt><dd><p>Claims thats intelligence is skill acquisition efficiency and has created the ARC challenge as a mechanism to quantify intelligence of AI systems. ARC is a benchmark where every item in the dataset is a completely different task complete with a handful of example inputs and outputs. Results of the challenge indicate that LLMs have poor skill acquisition efficiency even with in-context learning. The best approaches do test-time fine-tuning.</p>
</dd>
</dl>
</dd>
<dt>2024-11-18</dt><dd><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2411.04732">Convolutional Differentiable Logic Gate Networks</a></dt><dd><p>Extends differentiable logic gate networks to convolutions, enabling the direct learning of the logic gate networks necessary to implement convolutional networks on CIFAR-10.  This is a cool approach which runs in nanoseconds on an FPGA on CIFAR.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2210.08277">Deep Differentiable Logic Gate Networks</a></dt><dd><p>The paper which proposed logic gate networks.</p>
</dd>
</dl>
</dd>
</dl>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents</span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:</span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">The CoVar Zeitgeist: December, 2024</a><ul>
<li><a class="reference internal" href="#featured">Featured</a></li>
<li><a class="reference internal" href="#llms">LLMs</a></li>
<li><a class="reference internal" href="#vlms">VLMs</a></li>
<li><a class="reference internal" href="#object-detection">Object Detection</a></li>
<li><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li><a class="reference internal" href="#knowledge-graphs">Knowledge Graphs</a></li>
<li><a class="reference internal" href="#computational-efficiency">Computational Efficiency</a></li>
<li><a class="reference internal" href="#catastrophic-forgetting">Catastrophic Forgetting</a></li>
<li><a class="reference internal" href="#ethics-safety">Ethics &amp; Safety</a></li>
<li><a class="reference internal" href="#theory">Theory</a></li>
<li><a class="reference internal" href="#applications">Applications</a></li>
<li><a class="reference internal" href="#new-models">New Models</a></li>
<li><a class="reference internal" href="#presented-at-covar-seminar">Presented at CoVar Seminar</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="2025-01.html">
                    <span class="icon">&lt;</span><span>The CoVar Zeitgeist: January, 2025</span></a>
                
            </div>

            <div class="right">
                
                    <a href="2024-11.html"><span>The CoVar Zeitgeist: November, 2024</span><span class="icon">&gt;</span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
        &#169; Copyright 2025, CoVar, LLC.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    </div>

<p id="theme_credit"></p>
  </body>
</html>