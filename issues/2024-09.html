

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The CoVar Zeitgeist: 2024-09 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The CoVar Zeitgeist: 2024-10" href="2024-10.html" />
    <link rel="prev" title="The CoVar Zeitgeist: 2024-08" href="2024-08.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-04.html">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-04.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The CoVar Zeitgeist: 2024-09</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-09.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-covar-zeitgeist-2024-09">
<h1>The CoVar Zeitgeist: 2024-09<a class="headerlink" href="#the-covar-zeitgeist-2024-09" title="Permalink to this heading">¶</a></h1>
<section id="featured">
<h2>Featured<a class="headerlink" href="#featured" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.arxiv.org/pdf/2407.09468">Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures</a></dt><dd><p>Cool overview with lots of pictures. Doing ML in non-vector spaces is basically what we need to do.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11326">Automating Thought of Search: A Journey Towards Soundness and Completeness</a></dt><dd><p>From IBM.  Briefly reviews Thought of Search, a human-in-the-loop method which apparently has a 100% success rate in using LLMs as planning agents.  This paper automates ToS so no human is necessary, while maintaining 100% success.  The sucess may in part be explained by the test set, which seems to include five (somewhat simple?) toy problem sets.  Still impressive, and includes a section about getting LLMs to code reliably.</p>
</dd>
<dt><a class="reference external" href="https://gonenhila.github.io/files/Semantic_Leakage.pdf">Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</a></dt><dd><p>Finds a new way LLMs can go horribly wrong, called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny’s job is, it will say that Kenny is a school bus driver because school busses are yellow.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11804">Approaching Deep Learning through the Spectral Dynamics of Weights</a></dt><dd><p>Analyzes deep neural nets from the context of spectral weights.  They claim to be able to distinguish “memorizing networks” from “generalizing networks”, which sounds important, as well as identifying “lottery tickets”, or sparse networks with exceptional performance.  This all sounds pretty grand, so maybe there’s a there there.  Worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07772">Out-of-Distribution Learning with Human Feedback</a></dt><dd><p>How do you deal with out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from “wild data” (a great name), uses human feedback to label them, and then trains to both classify and identify OOD objects.  It’s a cool method, and one we might be able to use in a “how to use the minimun amount of labels to best improve model performance” sort of way.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.14837">DIFFUSION MODELS ARE REAL-TIME GAME ENGINES</a></dt><dd><p>Can it run Doom?  Some researchers at Google have made a neural net, GameNGen, for which the answer is yes.  Seems like this is just a fun science experiment at first, but there might be something here with having a neural net that will let you dynamically interact/move through 3D world models (Gaussian splatted?) for reconnassiance/intelligence/training purposes.  No word on whether it can run Crysis.</p>
</dd>
</dl>
</section>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></dt><dd><p>Deepmind investigates how best to use a finite amount of test-time compute to get an LLM to give the best answer.  Results are nuanced and the paper seems worth reading.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.09025">SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models</a></dt><dd><p>Microsoft figures out how to give a spreadsheet to an LLM. Not really a problem we have, but somewhat interesting in that, representing data effectively to an LLM is not always super straightforward.</p>
</dd>
<dt><a class="reference external" href="https://gonenhila.github.io/files/Semantic_Leakage.pdf">Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</a></dt><dd><p>Finds a new way LLMs can go horribly wrong, called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny’s job is, it will say that Kenny is a school bus driver because school busses are yellow.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10914">To Code, or Not To Code? Exploring Impact of Code in Pre-training</a></dt><dd><p>From Cohere.  Investigates the effects of including code in the training data for your LLM, and finds some pretty interesting results - mostly that including code helps improve the performance of your LLM on other tasks such as NLP reasoning and world knowledge.  Begs the question of “why”, but they don’t really answer that.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.08210">Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models</a></dt><dd><p>Investigates whether LLMs can do reasoning by looking at how they handle necessary and sufficient conditions.  Lots of formalism</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15545">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</a></dt><dd><p>A suite of LLMs to read scientic papers/abstracts and extract useful info in JSON format.  Seems useful for LitCoin.</p>
</dd>
</dl>
</section>
<section id="vlms">
<h2>VLMs<a class="headerlink" href="#vlms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10188">LONGVILA: SCALING LONG-CONTEXT VISUAL LANGUAGE MODELS FOR LONG VIDEOS</a></dt><dd><p>NVIDIA continues to chip away at VLMs, this time giving VILA extra long context length.  Lots of good stuff in here about training, datasets, performance, etc.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15998">EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</a></dt><dd><p>NVIDIA and friends do a fairly in-depth analysis of a bunch of different ways you can make mulitmodal LLMs with a number of different vision encoders.  Finds that simple methods work just as well as more complex ones.</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.02555">MESHANYTHING V2: ARTIST-CREATED MESH GENERATION WITH ADJACENT MESH TOKENIZATION</a></dt><dd><p>A model which takes “anything” (point clouds, Gaussian Splats, images, text, etc) and generates 3D meshes of the described object.  Could be useful.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.06190">4D Contrastive Superflows are Dense 3D Representation Learners</a></dt><dd><p>Cross train vision and lidar to make a pointcloud. Uses a vision transform and claims state of the art performance. Could potentially inspire some of our work to make 3D scenes from vision only.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2402.18673">Trends, Applications, and Challenges in Human Attention Modelling</a></dt><dd><p>Our job with AiTR is ultimately to aid human perception. Should we really be drawing boxes? With the new glow work coming up I think we should focus on understanding human attention so that we can better direct it.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10195">SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</a></dt><dd><p>Proposes a method to take one to a small number of unposed images of an object, and create a 3D mesh in 20 seconds.  Seems decent, lots of pictures of videogame characters.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10198">MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model</a></dt><dd><p>A novel method for 2D to 3D, sparse image to 3D, and text to 3D generation.  The abstract kind of reads like a madlibs of methods, but the results seem decent.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.12671">Joint Image De-noising and Enhancement for Satellite-Based SAR</a></dt><dd><p>Develops a novel algorithm for de-noising and enhancing remote sensing SAR imagery.  Actually seems like it might use science instead of machine learning mumbo jumbo.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.01384">NOLO: Navigate Only Look Once</a></dt><dd><p>Develops a transformer model to control navigation on a drone based on input video/images.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11326">Automating Thought of Search: A Journey Towards Soundness and Completeness</a></dt><dd><p>From IBM.  Briefly reviews Thought of Search, a human-in-the-loop method which apparently has a 100% success rate in using LLMs as planning agents.  This paper automates ToS so no human is necessary, while maintaining 100% success.  The sucess may in part be explained by the test set, which seems to include five (somewhat simple?) toy problem sets.  Still impressive, and includes a section about getting LLMs to code reliably.</p>
</dd>
</dl>
</section>
<section id="tracking">
<h2>Tracking<a class="headerlink" href="#tracking" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.21635">MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction</a></dt><dd><p>Uses relation transformers to do multi-agent tracking in basketball data.  This kind of makes sense since tracking involves sequences and transformers are good at that.</p>
</dd>
</dl>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2404.01223">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing</a></dt><dd><p>Combines 3D Gaussian splats with VLMs and physics-based models to enable text-based scened decomposition and to simulate physics-based dynamics in a 3D Gaussian splat.  Duct-tapes a whole bunch of models together to get to a cool looking result</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07540">3D Gaussian Editing with A Single Image</a></dt><dd><p>Develops a method that allows you to take a Gaussian splat, compress it to one image, modify that one image (photoshop?), and then generate a novel Gaussian splat corresponding to the changed image.  Seems cool, not sure what the use case is.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.08206">WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting</a></dt><dd><p>Novel 3D Gaussian Splatting approach for underwater scenes.  Generalizes to foggy/rainy scenes on dry land.  Could be a good tool to have in our toolbox</p>
</dd>
</dl>
</section>
<section id="gotta-go-fast">
<h2>Gotta Go Fast<a class="headerlink" href="#gotta-go-fast" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03703">CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications</a></dt><dd><p>How to put vision transformers on an iPhone.  Hilariously, they cite a paper from 2009 showing vision transformer results.  Were transformers even a thing in 2009?</p>
</dd>
<dt><a class="reference external" href="https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/">How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model</a></dt><dd><p>NVIDIA takes Llama-3.1 8B and turns it into a 4B parameter model with minimal decrease in performance somehow.  I guess NVIDIA is just better at this than Meta?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10233">FPCA: FIELD-PROGRAMMABLE PIXEL CONVOLUTIONAL ARRAY FOR EXTREME-EDGE INTELLIGENCE</a></dt><dd><p>This is really really cool. They came up with a way to have a re-programable circuit behind the pixels on the sensor. So at image capture they can run computations (convolutions). This means you could run super low-power, low-latency CNNs basically as you capture the image. This has been done before, but they’re showing off a the re-programmable version so you can change the algorithm without remanufacturing the chip.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.10189">Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</a></dt><dd><p>Figures out a way to distill a tranformer down to a SSM model.  Actually seems like a kind of cool process.Claims to be the best of its kind on the open source market, etc etc</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15237">The Mamba in the Llama: Distilling and Accelerating Hybrid Models</a></dt><dd><p>Takes a transformer and distills it down to an RNN without losing performance.  It’s kind of interesting that this works better than training an RNN from scratch - or maybe the transformer was just trained better than the best RNN.</p>
</dd>
</dl>
</section>
<section id="geometric-deep-learning">
<h2>Geometric Deep Learning<a class="headerlink" href="#geometric-deep-learning" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.arxiv.org/pdf/2407.09468">Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures</a></dt><dd><p>Cool overview with lots of pictures. Doing ML in non-vector spaces is basically what we need to do.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.15894">The Role of Fibration Symmetries in Geometric Deep Learning</a></dt><dd><p>Geometric Deep Learning often relies on global symmetries for inference.  Global symmetries can be rare in practice, however, so this paper instead uses local symmetries - called fibration symmetries to make it seem fancy? - to improve inference.  A cool idea, but it has less performance comparisons than I would like.</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.00657">Disentangling Dense Embeddings with Sparse Autoencoders</a></dt><dd><p>If you have dense embeddings, you can hit them with a sparse autoencoder and have sparse embeddings that maintain semantic fidelity.  Feels like there is something useful here, but can’t quite put my finger on what.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.05147">Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2</a></dt><dd><p>Deepmind explores some ways that sparse autoencoders are useful, with a look at the Gemma 2 family of models.  Seems like a decent overview</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.04851">Your Classifier Can Be Secretly a Likelihood-Based OOD Detector</a></dt><dd><p>Another paper in the “classifiers do OOD detection” bin.  Results seem decently convincing?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07772">Out-of-Distribution Learning with Human Feedback</a></dt><dd><p>How do you deal with out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from “wild data” (a great name), uses human feedback to label them, and then trains to both classify and identify OOD objects.  It’s a cool method, and one we might be able to use in a “how to use the minimun amount of labels to best improve model performance” sort of way.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11804">Approaching Deep Learning through the Spectral Dynamics of Weights</a></dt><dd><p>Analyzes deep neural nets from the context of spectral weights.  They claim to be able to distinguish “memorizing networks” from “generalizing networks”, which sounds important, as well as identifying “lottery tickets”, or sparse networks with exceptional performance.  This all sounds pretty grand, so maybe there’s a there there.  Worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.14319">Rethinking Knowledge Transfer in Learning Using Privileged Information</a></dt><dd><p>Some people have tried an interesting training method where you have privieged information (PI) that is available only during training.  This paper says that using PI this way has no theoretical or practical basis and should never be done.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://link.springer.com/article/10.1007/s11192-024-04968-7">Do grant proposal texts matter for funding decisions? A field experiment</a></dt><dd><p>A dutch study finds that an abstract and CV hold as much weight as a full proposal. Your representation, connections, and elevator pitch are what matter.  I wonder if this generalizes to other countries/institutions?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11527">The Vizier Gaussian Process Bandit Algorithm</a></dt><dd><p>Google talks about some black-box optimization methods they’ve been employing internally for years.  Gaussian process based.  Provides production level code.  If we ever have to do black box optimization (Im not sure we do?) then this is the place to start.</p>
</dd>
</dl>
</section>
<section id="new-models">
<h2>New Models<a class="headerlink" href="#new-models" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/">Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma</a></dt><dd><p>Google adds three new additions to the Gemma 2B family.  They claim its the best thing on the market, etc etc.  <a class="reference external" href="https://arxiv.org/pdf/2408.00118">Lab report</a></p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2407.21075">Apple Intelligence Foundation Language Models</a></dt><dd><p>Apple’s lab report on its foundation models.  Probably something interesting here if you want to read it.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.07009">Imagen 3</a></dt><dd><p>Text to image generation diffusion model from Google.  Maybe there’s a way to do synthetic data generation with this?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.03326">LLaVA-OneVision: Easy Visual Task Transfer</a></dt><dd><p>ByteDance releases a family of open LLMs that “push the performance boundaries” in some computer vision tasks.  Using anything released by ByteDane is presumably a hard no for government work, but it comes with a blog detailing development that might be worth a read.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.11039">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a></dt><dd><p>Meta’s new multi-model foundation model.  Can take text and images as part of the same input, as well as generating images.  Can handle complex(ish) instructions for image editting.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2408.12569">Sapiens: Foundation for Human Vision Models</a></dt><dd><p>Meta releases a new foundation model for computer vision focussing on humans.  Seems decent, but rather limited in scope and the examples have large numbers of pixels on target, so likely not to be much use for us.</p>
</dd>
</dl>
</section>
<section id="lunch-and-learn">
<h2>Lunch and Learn<a class="headerlink" href="#lunch-and-learn" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>2024-08-06</dt><dd><p><a class="reference external" href="https://arxiv.org/pdf/2407.21787">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a>
Generating accurate answers is hard, but verifying an answer is (sometimes) easy.  If you are living in a world where verifying an answer is easy, you can have an LLM generate a ton of answers and find th correct one.  Greatly improves performance.</p>
</dd>
<dt>2024-08-27</dt><dd><p><a class="reference external" href="https://arxiv.org/html/2402.01786v1">COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations</a>
DEVCOM Army Research Lab used GPT4-Turbo to play PySC2 (Armyified StarCraft battle simulator) and it came up with either a movement or attack for each blue unit and they called that a COA. It’s alright because of cutting edge model but only predicts one action for each unit (no updates over time).</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-08.html" class="btn btn-neutral float-left" title="The CoVar Zeitgeist: 2024-08" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-10.html" class="btn btn-neutral float-right" title="The CoVar Zeitgeist: 2024-10" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>