

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2024-04 &mdash; CoVar Readers Digest 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Lato" />
      <link rel="stylesheet" type="text/css" href="../_static/pytorch_theme.css?v=30bd9c6c" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=adfe253f" />

  
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=af2ce170"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2024-05" href="2024-05.html" />
    <link rel="prev" title="2024-03" href="2024-03.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/covar_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sections/01-Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023-12.html">2023-12</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id2">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id3">Toy Models of Superposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id4">BirdNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id5">Exponentially Faster Language Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id6">Don’t Say What You Don’t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id7">Accelerating 3D Deep Learning with PyTorch3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id8">Estimation of NIIRS, for High Resolution Satellite Images, Using the Simplified GIQE</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id9">Supersizing Transformers: Going Beyond RAG with Extended minds for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id10">Language Models can be Logical Solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id11">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id12">LLMs cannot find reasoning errors, but can correct them!</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id13">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id14">Orca 2: Teaching Small Language Models How to Reason</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id15">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id16">The ‘eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li>
<li class="toctree-l2"><a class="reference internal" href="2023-12.html#id17">ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-01.html">2024-01</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#neurips">NeurIPS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id2">NeurIPS 2023</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#gaussian-splatting-and-doctrinaire-related-things">Gaussian Splatting and Doctrinaire Related Things</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id3">Reconstructing Hands in 3D with Transformers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#llms">LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id4">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id5">Mixtral 8x7B Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id6">The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id7">Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id8">Scalable Extraction of Training Data from (Production) Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id9">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id10">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#images">Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id11">Efficient SAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id12">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id14">General Object Foundation Model for Images and Videos at Scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id15">GenDeF: Learning Generative Deformation Field for Video Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#theory">Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id16">Graph Convolutions Enrich the Self-Attention in Transforms!</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id17">Exploring Transferability for Randomized Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id18">Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id19">Understanding the Detrimental Class-level Effects of Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id20">The Machine Learning Control Method for Counterfactual Forecasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id21">Deep Internal Learning: Deep Learning from a Single Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id22">Can a Transformer Represent a Kalman Filter?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id23">A Mathematical Perspective on Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id24">Human mobility is well described by closed-form gravity-like models learned automatically from data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#reasoning">Reasoning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id25">Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id26">NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id27">SAT-Based Algorithms for Regular Graph Pattern Matching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#overhead">Overhead</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id28">QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id29">Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id30">Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id31">WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#autonomy">Autonomy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id32">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id33">Vision-Language Models as a Source of Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id34">Using Surprise Index for Competency Assessment in Autonomous Decision-Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id35">MineObserver 2.0: A Deep Learning &amp; In-Game Framework for Assessing Natural Language Descriptions of Minecraft Imagery</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id36">Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id37">Scaling Opponent Shaping to High Dimensional Games</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#stats">Stats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id38">Zero-Class Poisson for Rare-Event Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id39">Probabilistic Reconstruction of Paleodemographic Signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id40">Modeling and Predicting Epidemic Spread: A Gaussian Process Regression Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id41">Do Bayesian Neural Networks Weapon System Improve Predictive Maintenance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id42">Estimation of individual causal effects in network setup for multiple treatments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id43">A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2024-01.html#potpourrie">Potpourrie</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id44">NC Senate AI Panel Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id46">NNSVG</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id47">Wikifunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id48">mlX</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id49">Triple Pattern Fragments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id50">Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="2024-01.html#id51">Spiking Graph Convolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-02.html">2024-02</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#build-it">Build It</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-02.html#potpurrie">Potpurrie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-03.html">2024-03</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#tooling">Tooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#position-papers">Position Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-03.html#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2024-04</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lvlms">LVLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sports-analytics">Sports Analytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#computer-science">Computer Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-labelling">Data Labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logistics-operations-research">Logistics/Operations Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-05.html">2024-05</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#stats">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-05.html#new-llms">New LLMs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-06.html">2024-06</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-applications">LLM Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#llm-theory">LLM Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#sensing">Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#reasoning-knowledge-graphs">Reasoning/Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-06.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-07.html">2024-07</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#ethics">Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#fpga">FPGA</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#knowledge-graphs">Knowledge Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-07.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-08.html">The CoVar Zeitgeist: 2024-08</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#fusion">Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#new-llms">New LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-08.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-09.html">The CoVar Zeitgeist: 2024-09</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#autonomy">Autonomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gaussian-splatting">Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#geometric-deep-learning">Geometric Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-09.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2024-10.html">The CoVar Zeitgeist: 2024-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#featured">Featured</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#llms">LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#vlms">VLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#doctrinaire">Doctrinaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#reasoning">Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#tracking">Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#gotta-go-fast">Gotta Go Fast</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#adversarial">Adversarial</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#theory">Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#new-models">New Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2024-10.html#lunch-and-learn">Lunch and Learn</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CoVar Readers Digest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2024-04</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/issues/2024-04.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>2024-04<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="llms">
<h2>LLMs<a class="headerlink" href="#llms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03186.pdf">Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</a></dt><dd><p>LLM plays Red Dead Redemption II.  Takes compuer screen and audio as inputs, and outputes keyboard/mouse actions, like a human would.  Something like this might be a good way to mimic what a human does in other areas?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2402.12483.pdf">Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?</a></dt><dd><p>LLMs are really good at multiple choice questions.  How good are they are multiplce choice if they just get the answers, and not the questions?  Really good as it turns out.  Overtrained to the training data, or picking up some weird underlying structure?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.04366.pdf">Enhancing Court View Generation with Knowledge Injection and Guidance</a></dt><dd><p>Have you ever wanted an LLM to be a judge in a court case?  This paper injects legal knowledge into pretrained GPT 2 using some sort of encoder to try to do this.  They outperform the metrics, but I wouldn’t trust thsi.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03853.pdf">ShortGPT: Layers in Large Language Models are More Redundant Than You Expect</a></dt><dd><p>Pruning method for LLMs.  Seems to maintain SOTA(ish) performancs across a range of benchmarks while pruning quite significantly.  Prunes by measuring the influence of each level and getting rid of the not important ones</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.14624.pdf">MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</a></dt><dd><p>New dataset for LLMs in math, and a Chain-of-thought evaluation strategy.  Finds that math diagrams do not improve LLM performance on math questions - sometimes, they make the LLMs worse!  Apparently LLMs are just going off of text data.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.16843.pdf">Do LLM Agents Have Regret? A Case Study in Online Learning and Games</a></dt><dd><p>In an online setting, how do LLMs do with respect to regret?  Sometimes they’re no-regret, sometimes they’re not - even in some simple cases.  I wouldn’t rely on one of these intead of, e.g.,  a multi-armed bandit but maybe they can get there?</p>
</dd>
<dt><a class="reference external" href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">Introducing DBRX: A New State-of-the-Art Open LLM</a></dt><dd><p>New LLM just dropped.  Claims to be better and more efficient than LLaMa2-70B, Mixtral, and Grok-1.  Some very funny y-axis choices on graphs.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.18802.pdf">LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS</a></dt><dd><p>From Deepmind.  LLMs can do factchecking by breaking claims down into factual statements and evaluating each claim with a multi-step reasoning process and the ability to query Google Search. Better and faster than crowdsourced human factchecking which seems too good to be true.  Seems dependent on Google Search, but aren’t we all.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09629.pdf">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></dt><dd><p>Every piece of text has some implicit reasoning in it - turns out we can make LLMs be better by forcing them to explicitly implicitly reason when generating new text.  The paper has to do some fancy footwork to stick the landing, but seems to manage it.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.06634.pdf">Stealing Part of a Production Language Model</a></dt><dd><p>From Deepmind/OpenAI.  (Kind of looks like Deepmind just decided to do this and then talked to OpenAI about it later, though.)  For less than $20 you can steal a whole bunch of embeddings from an LLM using the API.  Apprently they patched it so you can’t do this anymore, but maybe there’s some other stuff you can do?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.04732.pdf">How Far Are We from Intelligent Visual Deductive Reasoning?</a></dt><dd><p>From Apple.  Pretty far as it turns out - LLMs tend not to reason very well when presented with visual instead of textual evidence.  They investigate why a little bit.  Tracks with the “math llms are bad at figures” paper.</p>
</dd>
</dl>
</section>
<section id="lvlms">
<h2>LVLMs<a class="headerlink" href="#lvlms" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.12966.pdf">Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</a></dt><dd><p>Large vision language models (LVLMs) can see an increase in performance if you prompt them to focus in on an area of interest.  Maybe they get confused by large images?</p>
</dd>
</dl>
</section>
<section id="doctrinaire">
<h2>Doctrinaire<a class="headerlink" href="#doctrinaire" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.02136.pdf">Point2Building: Reconstructing Buildings from Airborne LiDAR Point Clouds</a></dt><dd><p>Makes a method to autoregressivly 3D meshes from LiDAR point clouds of buildings.  Can probably be adapted to things like tanks?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03527.pdf">LDSF: Lightweight Dual-Stream Framework for SAR Target Recognition by Coupling Local Electromagnetic Scattering Features and Global Visual Features</a></dt><dd><p>A Chinese lab from the “National University of Defense Technology” implements a method for incorproating physrics (EM backscattering) into algorithms for ATR with SAR data.  Demonstrates method on MSTAR.  Worth a look, if only to see what the other guy’s up to.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.05381.pdf">Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery</a></dt><dd><p>Few-shot detection in remote sensing data using vision transformers.  Classes are represented by prototypes in some embediding space - the model can learn/fit these prototypes.  Morally similar to doctrinaire/MAGI stuff.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.07815.pdf">Chronos: Learning the Language of Time Series</a></dt><dd><p>From Amazon, Time-series prediction is predicting the next thing in a sequence.  Language models are transformers, which just predict the next thing in a sequence.  What if we just used language models for time-series prediction?  Works out pretty well, especially in a zero-shot context.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09014.pdf">Urban mapping in Dar es Salaam using AJIVE</a></dt><dd><p>Mapping poverty in Dar Es Salem using remote sensing imagery, cell phone data, and surveys.  Cool problem, though they seem to just take an off-the-shelf dimension reduction solution and throw it at the problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09651.pdf">Precision Agriculture: Crop Mapping using Machine Learning and Sentinel-2 Satellite Imagery</a></dt><dd><p>Segmenting lavendar fields using Sentinel-2 data.  Finds that logistic regression across bands (they have 12 bands) can outperfrom deep CNNs.  I find this kind of surprising, though lavendar is purple and so very visually distinct.</p>
</dd>
</dl>
</section>
<section id="autonomy">
<h2>Autonomy<a class="headerlink" href="#autonomy" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.00692.pdf">Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks</a></dt><dd><p>How to coordinate EO satellites to observe enough stuff using dynamic graph neural nets.  More constrained, but similar to drone swarm type problems.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.02289.pdf">Physics-Informed Neural Networks with Skip Connections for Modeling and Control of Gas-Lifted Oil Wells</a></dt><dd><p>Physics-informed neural networks for modelling ODEs/PDEs and controlling dynamic systems like oil wells.  They say introducing skip connections helps improve performance which feels right but also maybe like an old result?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03172.pdf">Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination</a></dt><dd><p>From Tencent.  A problem in multi-agent reinforcement learning is coordination - this paper attempts to solve that problem by imaging a common goal and guiding agents to that path.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.10054.pdf">Control and Automation for Industrial Production Storage Zone: Generation of Optimal Route Using Image Processing</a></dt><dd><p>Finding optimal routes for multiple agents based off of imagery.  Details the whole autonomy pipeline for this.  Could be a useful reference if we ever want to do something similar.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.13741.pdf">Hyper Strategy Logic</a></dt><dd><p>How to get multiple agents to cooperate in a multi-agent system?  Apparently one answer is using “hyperlogic” to coordinate.  Interesting problem (and 10 points for the name), but the paper looks very theoretical.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.14796.pdf">Planning and Acting While the Clock Ticks</a></dt><dd><p>Traditional autonomous planning agents have planned everything before executing anything.  Sometimes you can’t do this - you have to initiate some tasks before you plan everything.  This paper does this by putting a decision rule into an existing planner.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.15341.pdf">Collaborative AI Teaming in Unknown Environments via Active Goal Deduction</a></dt><dd><p>How to get AIs that don’t know each other to work together in the presence of goal uncertainty and imperfect info about other agents?  Try to learn what the other agents preference are and acted based on that in such a manner as to further your own goals.  Tests it in Starcraft II, and seems to work</p>
</dd>
</dl>
</section>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.07688.pdf">Maxwell’s Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons</a></dt><dd><p>From Deepmind.  Examines dead neurons, and proposes that maybe its a good thing.  Proposes a method for training a neural net that controls the number of dead neurons, which leads to network sparsity.  Seems like a cool way to do sparsity during training?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09613.pdf">Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</a></dt><dd><p>From Deepmind.  When fed cyclical sequences of documents, LLMs don’t experience catastrophic forgetting but rather anticipatory recovery.  They offer a few hypotheses for why this is happening, but they don’t really know.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.10519.pdf">Frozen Feature Augmentation for Few-Shot Image Classification</a></dt><dd><p>From Deepmind.  Applies image/feature augmentations in froze feature space to improve the performance of a relativley simple model duct-taped on top of a frozen deep learning model.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.11532.pdf">Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)</a></dt><dd><p>Traditional methods of evaluating out-of-distribution detection may be too optimistic, and we should use conformal prediction instead.  If we had more time/effort for MAGI I’d look into this.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.11497.pdf">Do CLIPs Always Generalize Better than ImageNet Models?</a></dt><dd><p>Constrcuts a dataset with lots of spurious correlations to evaluate CLIP and ImageNet style models fro zero-shot prediction.  Found ImageNets did better in this circumstance.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2310.00873.pdf">Deep Neural Networks Tend To Extrapolate Predictably</a></dt><dd><p>As data gets OOD, neural nets tend to produce solutions which default to the solution which, when treated as constant, minimizes loss over the training set.  This is very similar to how a Gaussian Process behaves - I wonder if there are structural similarities between the models or if this is a generic thing for nonparametric methods?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.16459.pdf">On the rates of convergence for learning with convolutional neural networks</a></dt><dd><p>Theory heavy.  Derives rates of convergence for estimators based on CNNs.  It’s reassuring that someone has done this, but I’m glad it wasn’t me.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.17887.pdf">The Unreasonable Ineffectiveness of the Deeper Layers</a></dt><dd><p>From Meta (and others).  Investigates pruning LLMs and finds you can prune up to half(!) of layers without much degradation in performance.  Seems crazy, but we should maybe try this if it holds up.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.05440.pdf">Is Cosine-Similarity of Embeddings Really About Similarity?</a></dt><dd><p>From Netflix.  Studies how well cosine similarity does in some pretty simple linear matrix factorization models (probably inspired by probability matrix factorization/recommender system stuff).  How well it works depends on training/regularization.  Buyer beware.</p>
</dd>
</dl>
</section>
<section id="stats">
<h2>Stats<a class="headerlink" href="#stats" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.00639.pdf">Hierarchical Bayesian Models to Mitigate Systematic Disparities in Prediction with Proxy Outcomes</a></dt><dd><p>Andrew Gelman is a co-author.  Deals with label bias - e.g. you’re given a diagnosis and not the underlying condition, and this is confounded with other stuff.  Seems relevant for P(object|detection) sort of problems?</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03007.pdf">Scalable Bayesian inference for the generalized linear mixed model</a></dt><dd><p>Stochastic gradient descent MCMC for Bayesian GLMMs.  Significantly faster than Gibbs sampling, but not compared to frequentist methods.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.04058.pdf">PLANT-CAPTURE METHODS FOR ESTIMATING POPULATION SIZE FROM UNCERTAIN PLANT CAPTURES</a></dt><dd><p>Develops methods for counting how large a population is based on a capture-recapture model.  Cool stats, lots of applications.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09604.pdf">EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES</a></dt><dd><p>Uses extremal graphical models to model conditional independence structure for extreme multivariate data.  Cool approach for extereme events problems.</p>
</dd>
</dl>
</section>
<section id="sports-analytics">
<h2>Sports Analytics<a class="headerlink" href="#sports-analytics" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.04873.pdf">The SIDO Performance Model for League of Legends</a></dt><dd><p>Collaboration between a UW prof and TSM Parth for modelling player performance in league of legends.  Very interesting/complex problem.  Looks to be a giant Bayesian mixed effects model, which makes sense.  Not sure if it generalizes to the professional setting, since it relies on having lots of data/players mixing somewhat frequently</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.12977.pdf">SportsNGEN: Sustained Generation of Multi-player Sports Gameplay</a></dt><dd><p>A transformer decoder can be trained on sports data (tennis and soccer) to simulate matches/games.  They say coaches can use it to evaluate counterfactuals, but since its a black box I’m not sure what type of insights you can glean.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.13821.pdf">Offensive Lineup Analysis in Basketball with Clustering Players Based on Shooting Style and Offensive Role</a></dt><dd><p>Attempts to analyze style-of-play in basketball by clustering based on tracking data and “advanced statistics”.  Then trained some Bayesian stuff on top to predict which players would go work well together.  Interesting idea/approach, but not terribly complicated.</p>
</dd>
</dl>
</section>
<section id="sensing">
<h2>Sensing<a class="headerlink" href="#sensing" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.17820.pdf">Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge</a></dt><dd><p>Bayesian hierarchical models (including GPs!) to predict which trains are passing over the Stanfordshire Bridge based on telemetry data.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.18549.pdf">A communication-efficient, online changepoint detection method for monitoring distributed sensor networks</a></dt><dd><p>How to do changepoint detection from an array of distributed sensors while minimizing communicaition costs?  Feels like it could be a relevant problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.19376.pdf">NIGHT - Non-Line-of-Sight Imaging from Indirect Time of Flight Data</a></dt><dd><p>Can you sense an object behind a corner and out of line of sight?  Apparently you can using Time of Flight sensors and using opposing walls as mirrors.  It’s a cool party trick, maybe one we can sell.</p>
</dd>
</dl>
</section>
<section id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.00975.pdf">Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance</a></dt><dd><p>Model wind-turbine perfromance over time using ensembles of FNN and LSTMs.  Each wind turbine is unique enough to require tailoring at the individual level.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03948.pdf">Estimating the household secondary attack rate with the Incomplete Chain Binomial model</a></dt><dd><p>Discrete-time SIR model for infectious diseases, but explicitly modelling the spread within each household  using an Incomplete Chain Binomial model.  I’d never heard of that before and it’s nice to learn things.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.06860.pdf">A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa</a></dt><dd><p>Using LSTM/convolutional neural nets to do spatiotemporal modelling of locusts swarms.  Specifically looking to predict breeding grounds - probably with an eye towards some sort of policy intervention.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.11564.pdf">Spatio-temporal point process intensity estimation using zero-deflated subsampling applied to a lightning strikes dataset in France</a></dt><dd><p>If you’ve ever wanted to predict lightning, this is the paper for youl  Nothing too groundbreaking, but an interesting applied problem.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.12858.pdf">Settlement Mapping for Population Density Modelling in Disease Risk Spatial Analysis</a></dt><dd><p>Wants to use population density to model disease risk.  Gets at it by estimating settlement maps from satellites using off-the-shelf software (CNNs for settlement segmentation) and adminstrative data to generate more accurate density estimates before using those estimates in downstream analysis.  Nothing too fancy, but a cool pipeline.</p>
</dd>
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.19572.pdf">Swarm Characteristics Classification Using Neural Networks</a></dt><dd><p>The Naval Postgraduate School analyzes drone swarms.  Supervised neural nets can apparently analyze and predict drone swarm behavior very well (granted, only two dimensions).  Swarm behavior/tactics were simulated, so to some extent this is just showing that neural nets work as emulators (something somethign universal approximation theorem), but it shows that there’s something to the concept.  If they can do this, we can do this.</p>
</dd>
</dl>
</section>
<section id="computer-science">
<h2>Computer Science<a class="headerlink" href="#computer-science" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://www.eecs.umich.edu/courses/eecs584/static_files/papers/p3372-pedreira.pdf">Velox: Meta’s Unified Execution Engine</a></dt><dd><p>From Meta, and already in use internally.  Over my head technically, but seems to be a fancy new way to store, access, and use data of every type in one place.  Might be convenient.</p>
</dd>
</dl>
</section>
<section id="data-labelling">
<h2>Data Labelling<a class="headerlink" href="#data-labelling" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.03208.pdf">Active Statistical Inference</a></dt><dd><p>Active learning for choosing which datapoints to label next.  Could be useful for situations where we have many more datapoints than labels (e.g. MAGI)</p>
</dd>
</dl>
</section>
<section id="logistics-operations-research">
<h2>Logistics/Operations Research<a class="headerlink" href="#logistics-operations-research" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.09361.pdf">A Multi-population Integrated Approach for Capacitated Location Routing</a></dt><dd><p>Looking for the best way to get a bunch of stuff from a set of depots to a population of users.  Seems to be considering depot configurations.  Worth a look if we ever want to break into logistics</p>
</dd>
</dl>
</section>
<section id="knowledge-graphs">
<h2>Knowledge Graphs<a class="headerlink" href="#knowledge-graphs" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.06936.pdf">Counterfactual Reasoning with Knowledge Graph Embeddings</a></dt><dd><p>Counterfactual reasoning on knowledge graphs.  Feels like there’s a cool idea in here somehwere but the paper doesn’t quite find it.</p>
</dd>
</dl>
</section>
<section id="reasoning">
<h2>Reasoning<a class="headerlink" href="#reasoning" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.16628.pdf">A comparison of graphical methods in the case of the murder of Meredith Kercher</a></dt><dd><p>An application of 3 different graphical reasoning models to the Amanda Knox case.  More of a case study than anything, but still interesting.</p>
</dd>
</dl>
</section>
<section id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/pdf/2403.05422v1.pdf">EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV</a></dt><dd><p>Lots of labelled UAV pictures from 50, 70, and 90m.  Made for adversarial stuff, but looks good anyway.</p>
</dd>
<dt><a class="reference external" href="https://ieeexplore.ieee.org/document/9109702">Multisized Object Detection Using Spaceborne Optical Imagery</a></dt><dd><p>Remote sensing with lots of classes - definitely includes planes</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2024-03.html" class="btn btn-neutral float-left" title="2024-03" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2024-05.html" class="btn btn-neutral float-right" title="2024-05" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CoVar, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>