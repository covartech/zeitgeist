2024-05
=======

Featured
--------
`Improving Detection in Aerial Images by Capturing Inter-Object Relationships <https://arxiv.org/pdf/2404.04140.pdf>`_
    Objects tend to be spatially corrected, but existing overhead ATR methods don't take this into account.  This paper does so by putting a transfomer on top of traditional two-stage detectors to examine regions of interest.  We though about implementing something like this for MAGI, but never did.  A cool idea to keep in our back pocket, especially if we can adapt it to non-overhead areas.

`Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs <https://arxiv.org/pdf/2403.04801.pdf>`_
    Instead of trying to prompt an LLM to recover something about it (in this case whether/how much training data it has memorized) you can have another LLM attack it instead.  Could probably expand this methodology to other vectors.

`RecurrentGemma: Moving Past Transformers for Efficient Open Language Models <https://arxiv.org/pdf/2404.07839.pdf>`_
    Deepmind proposes a new LLM.  Doesn't use global attention, but instead uses local attention and linear recurrences.  Based off of an earlier paper (https://arxiv.org/pdf/2402.19427.pdf) which proposes a novel model architecture.  They trained this one on the same data as Gemma - this seems to be better (mildly) despite being trained on lass tokens, runs much faster, and doesn't face the same sequence length limitations. 

`Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data <https://arxiv.org/pdf/2404.01413.pdf>`_
    Generative models trained on their own outputs tend to collapse.  Part of the reason we thought this was because new generated data was replacing old real data - if you accumulate data instead, and supplement the old data with generated data rather than substitute it, than model collapse does not occur.  There's still some error, I think - it's just bounded above instead of going to infinity.

`X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models <https://arxiv.org/pdf/2404.06332.pdf>`_
    Ever yelled at the refs while watching sports?  What if the ref was an LLM?  This paper puts soccer vidoes into a CLIP-based encoder, and an LLM then generates and explains fouls.  Cool application, and could be transferred to other domains more directly relevant to CoVar.

LLMs
----

`Localizing Paragraph Memorization in Language Models <https://arxiv.org/pdf/2403.19851.pdf>`_
    From Google.  LLMs can memorize whole paragraphs of text from training data and vomit them when prompted to do so.  This paper investigates this phenomena.  Worth keeping an eye on, especially when we're dealing with classified/controlled training data.

`TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model <https://arxiv.org/pdf/2404.01273.pdf>`_
    Uses LLMs to predict counterfactual data for clinical trials for use in assessing drugs.  Cool idea, but I would never want to take medicine tested this way.

`Mapping the Increasing Use of LLMs in Scientific Papers <https://arxiv.org/pdf/2404.01268.pdf>`_
    Trawling arXiv to figure out how much of it is LLM generated.  Less than you'd think, but still a lot (17.5% for CS).  Trying to reverse engineer whether an LLM wrote something is hard so I can't tell if they're undercounting or overcounting.

`X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models <https://arxiv.org/pdf/2404.06332.pdf>`_
    Ever yelled at the refs while watching sports?  What if the ref was an LLM?  This paper puts soccer vidoes into a CLIP-based encoder, and an LLM then generates and explains fouls.  Cool application, and could be transferred to other domains more directly relevant to CoVar.

`Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models <https://arxiv.org/pdf/2404.06209.pdf>`_
    From Microsoft.  LLMs have  tendency to memorize tabular training data wholesale, but still perform decently well in zero/few shot environments.  

`Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs <https://arxiv.org/pdf/2403.04801.pdf>`_
    Instead of trying to prompt an LLM to recover something about it (in this case whether/how much training data it has memorized) you can have another LLM attack it instead.  Could probably expand this methodology to other vectors.

`RecurrentGemma: Moving Past Transformers for Efficient Open Language Models <https://arxiv.org/pdf/2404.07839.pdf>`_
    Deepmind proposes a new LLM.  Doesn't use global attention, but instead uses local attention and linear recurrences.  Based off of an earlier paper (https://arxiv.org/pdf/2402.19427.pdf).  They trained this one on the same data as Gemma - this seems to be better (mildly) despite being trained on lass tokens, runs much faster, and doesn't face the same sequence length limitations. 

Doctrinaire
-----------
`FLIGHT SCOPE: A DEEP COMPREHENSIVE ASSESSMENT OF AIRCRAFT DETECTION ALGORITHMS IN SATELLITE IMAGERY <https://arxiv.org/pdf/2404.02877.pdf>`_
    Compares a bunch of different neural architectures on remote sensing for planes - find thats YOLOv5 is the best, outperforming CenterNet.  (Turns out they're using the other Centernet)

`Improving Detection in Aerial Images by Capturing Inter-Object Relationships <https://arxiv.org/pdf/2404.04140.pdf>`_
    Objects tend to be spatially corrected, but existing overhead ATR methods don't take this into account.  This paper does so by putting a transfomer on top of traditional two-stage detectors to examine regions of interest.  We though about implementing something like this for MAGI, but never did.  A cool idea to keep in our back pocket, especially if we can adapt it to non-overhead areas.

`DiffDet4SAR: Diffusion-based Aircraft Target Detection Network for SAR Images <https://arxiv.org/pdf/2404.03595.pdf>`_
    ConvNets/transformers for overhead sensing in SAR are limited by varying target size, spikiness of SAR data, and general noise.  They try to get around these problems by (1) using a  denoising diffusion process and (2) using a scattering feature enhancement to model the SAR data.  Seems to lead to improved results. 

Autonomy
--------
`Laser Learning Environment: A new environment for coordination-critical multi-agent tasks <https://arxiv.org/pdf/2404.03596.pdf>`_
    Introduces a new learning environment for mult-agent reinforcement learning.  One problem is getting stuck in a state space.  They don't have a solution, but they did find the problem.  

`PLAYER2VEC: A LANGUAGE MODELING APPROACH TO UNDERSTAND PLAYER BEHAVIOR IN GAMES <https://arxiv.org/pdf/2404.04234.pdf>`_
    Player behavior in video games can be turned into a sequence of actions and modelled with a transformer.  The authors don't really do much with this insight, but you could imagine doing something interesting, like using it to control autonomous systems. 

Theory
------
`SGD with Large Step Sizes Learns Sparse Features <https://arxiv.org/pdf/2210.05337.pdf>`_
    An investigation of how stochastic gradient descent can impose implicit regulation on neural nets, in particular large step sizes cause the network to become sparse.  Some of this is old, but its worth reading.  

`Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data <https://arxiv.org/pdf/2404.01413.pdf>`_
    Generative models trained on their own outputs tend to collapse.  Part of the reason we thought this was because new generated data was replacing old real data - if you accumulate data instead, and supplement the old data with generated data rather than substitute it, than model collapse does not occur.  There's still some error, I think - it's just bounded above instead of going to infinity.

`The Unreasonable Effectiveness Of Early Discarding After One Epoch In Neural Network Hyperparameter Optimization <https://arxiv.org/pdf/2404.04111.pdf>`_
    Apparently the best way to do hyperparameter parameterization is to train all instantiations of your network for the same number of epochs, and make a choice based on those results.  Kind of surprising none of the fancier techniques provide any real benefit over this naive approach.

`Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws <https://arxiv.org/pdf/2404.05405.pdf>`_
    From Meta.  LLMs apparently get 2 bits of information per parameter, no more and no less, even when quantized different ways.  

`No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance <https://arxiv.org/pdf/2404.04125.pdf>`_
    From Deepmind (and friends).  Zero-shot models like CLIP aren't really zero-shot - their performance instead requires exponentially more data to get linear performance increases.  Obvious implication is that these models aren't actually zero-shot and instead depend on just having oodles of training data, which makes sense intuitively.  

`Variational Stochastic Gradient Descent for Deep Neural Networks <https://arxiv.org/pdf/2404.06549.pdf>`_
    New method for gradient descent, Variational Stochastic Gradient Descent, which outperforms both ADAM and regular SGD on the examples in the paper (both image classification).  They make VSGD by forming a probabilistic model of gradient descent and use stochastic variational inference to find updates.  Apparently VSGD is a generalization of other methods such as SGD and ADAM?

Stats
-----
`On the Learnability of Out-of-distribution Detection <https://arxiv.org/pdf/2404.04865.pdf>`_
    A NeurIPS 2022 paper accepted in JLMR and republished in 2024 (maybe with signficant revisions?). "Proves" when OOD detection is theoretically impossible and when it's possible.   

Sensing
-------
`Bridging Remote Sensors with Multisensor Geospatial Foundation Models <https://arxiv.org/pdf/2404.01260.pdf>`_
    From Amazon Web Services.  Fusing together multiple modalities in remote sensing.  Does distinct embedding layers for each sensor, then hits them all with a shared encoder, and decodes on a per-sensor level.  Worth looking at for multi-modal data problems.

`A Satellite Band Selection Framework for Amazon Forest Deforestation Detection Task <https://arxiv.org/pdf/2404.02659.pdf>`_
    Uses the Univariate Margina Distribution Algorithm (UMDA) to select the "optimal" Landsat band for overhead monitoring.  Apparently, this outperforms using all of the bands, which is wild - I guess the other bands were actively harmful to inference?

`LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification <https://arxiv.org/pdf/2404.03883.pdf>`_
    Uses LiDAR to select the best hyperspectral bands using fancy self-attention encoders, then uses all of it for image classification.  Could be useful for fusion.  

FPGA
----
`GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA <https://arxiv.org/pdf/2404.07188.pdf>`_
    From DEVCOM Army Research Office.  Putting CNNS and GNNs for CV on FPGAs.  


Reasoning/Knowledge Graphs
--------------------------
`FLawN-T5: An Empirical Examination of Effective Instruction Tuning Data Mixtures for Legal Reasoning <https://arxiv.org/pdf/2404.02127.pdf>`_
    Turns out one of the reasons that legal reasoners are bad is because there isn't a good legal reasoning dataset.  This paper introduces one, finetunes a bit, and shows much better performance.  Seems kind of obvious once they point it out. 

`Chain event graphs for assessing activity-level propositions in forensic science in relation to drug traces on banknotes <https://arxiv.org/pdf/2404.02778.pdf>`_
    Legal reasoning via turning arguments into graphical models, assigning probabilites to edges, and going from there.  Doesn't really seem groundbreaking from a statistical point of view (and similar to knowledge graphs?) but a useful way to formalize intuition.  

`KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion <https://arxiv.org/pdf/2404.03893.pdf>`_
    From Amazon.  Proposes a method to explain knowledge graph completions done with knowledge graph embeddings by investigating connected subgraphs.  Makes intuitive sense and seems to improve performance in practice.  
