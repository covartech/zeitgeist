2024-01
=======

`Scalable Extraction of Training Data from (Production) Language Models <https://arxiv.org/abs/2311.17035>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ask ChatGPT to repeat a word forever. After a while it starts barfing training data.
This is now against OpenAI terms of service.

    Our attack circumvents the privacy safeguards by identifying a vulnerability in ChatGPT that causes it to escape its fine-tuning alignment procedure and fall back on its pre-training data.

`See Also <https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html>`_

------------


`NC Senate AI Panel Report <https://wraltechwire.com/2023/12/01/ai-safety-is-imperative-triangle-thought-leaders-talk-artificial-intelligence-with-senate-panel/>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Not a lot, but interesting to see who was there speaking on "our" behalf. It's Cynthia Rudin.

`See also <https://www.schumer.senate.gov/newsroom/press-releases/statements-from-the-seventh-bipartisan-senate-forum-on-artificial-intelligence>`_

------------

`NNSVG <https://alexlenail.me/NN-SVG/AlexNet.html>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Make pretty SVGs from your NN architecture.
They look nice. 

------------


`Mixtral 8x7B Explained <https://huggingface.co/blog/moe>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Mixtral is an interesting LLM company. They are French and seem to be pretty solid. This is how they do Mixture of Expert models (like GPT4). Instead of using one big model say 56GB we can actually use a mixture of 8 7B models. We end up saving a bit of VRAM space in the process and it works better. Something to keep an eye on.

`VLLM Implementation <https://github.com/vllm-project/vllm/commit/b5f882cc98e2c9c6dde7357dbac2ec0c2c57d8cd>`_


------------


`The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A" <https://arxiv.org/abs/2309.12288>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
User: Who is Tom Cruise's mother.

LLM: Mary Lee Pfiefer


User: Who's son is Mary Lee Pfiefer

LLM: ?!?!? Dunno ?!?!?!

Nobody talks about that so how could the big memorizer memorize it.

------------


`Wikifunctions <https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The Wikipedia of "functions" that can translate inputs to outputs. The open source collection of algorithms/code/functions. A bunch of string operations for now, not much going on, but eventually could be very useful for general AI.

------------

`Efficient SAM <https://yformer.github.io/efficient-sam/>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I tend to think the future of most AI is big foundational models that few people seldomly retrain. Segment Anything (SAM) is the foundational model of image segmentation and its pretty good most of the time. Now fast.

------------

`mlX <https://ml-explore.github.io/mlx/build/html/index.html>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Apple released computational library kinda like numpy or pytorch for Apple silicon. Because of the unified memory on Apple Silicon, and the inclusion of auto-grad, it's suitable as a pytorch replacement. Has LLM runtime components that make LLMs go faster than "CPU" based operations.

------------


`Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation <https://arxiv.org/abs/2311.03348>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Getting GPT4 to give you a recipe for meth is very doable. It's not a GPT problem, but's a standard LLM problem. This has big implications for Secret and Top Secret LLMs. Jailbreaking them is a security violation / spill.

------------

`Sequential Modeling Enables Scalable Learning for Large Vision Models <https://arxiv.org/abs/2312.00785>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Training a foundational vision model that can do many tasks. Uses transformers and image squences. It's a bit insane. All input must be images so if you want BBoxes you better draw an image with those boxes in it. I didn't say it was good. I said it was interesting.

`See Also <https://yutongbai.com/lvm.html>`_


`General Object Foundation Model for Images and Videos at Scale <https://arxiv.org/abs/2312.09158>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
More foundational model vision talk. This thing does it all, object detection, segmentation, tracking. Honestly it looks pretty impressive. 

`Triple Pattern Fragments <https://linkeddatafragments.org/specification/triple-pattern-fragments/>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The old school pick of the month. W3C is a bunch of committees that govern a lot of open standards. (There are those that say they lost and Google runs the internet now, and they are sorta right but,) Looking at the types of things they have come up with for abstraction and representations is interesting. Lot's of smart people. We should be more inspired by them.

------------

`Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models <https://sliders.baulab.info>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In a generative model it is very useful to have knobs to control how to modify an existing image in a specific direction. Make the face older/have glasses etc. This is a way to train those sliders.

------------


`Splatter Image: Ultra-Fast Single-View 3D Reconstruction <https://arxiv.org/abs/2312.13150>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It's like the Centernet of NERF. They make a "splatter image" that defines a 3D reconstruction, they can feed it to a Gaussian splatter renderer. From those small number channels they can render other views of that target. It's kind of awesome. It seems super useful to me, just can't quite figure out how yet.

`This paper with video <https://szymanowiczs.github.io/splatter-image>`_
`Splatting for people moving <https://shunyuanzheng.github.io/GPS-Gaussian>`_
`Splatting for avatar heads <https://yuelangx.github.io/gaussianheadavatar/>`_
`Splatting with transformers baked in <https://arxiv.org/abs/2312.09147>`_

I think this one `iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching <https://arxiv.org/abs/2312.09031>`_ is the splatting paper I was looking for as Doctrinaire.

------------


`Spiking Graph Convolutional Networks <https://arxiv.org/abs/2205.02767>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
What ever happened to Spiking Networks? Do they do anything good yet? This is for graph convolution from March 2022 and the answer is, not really. Energy efficiency?
Here is another one `Language Modeling on a SpiNNaker 2 Neuromorphic Chip <https://arxiv.org/abs/2312.09084>`_ also being a little better on energy for LSTM language models (not even LLMs). Also do some event based camera work, MAYBE just MAYBE there is something there.

------------

`Exploring Transferability for Randomized Smoothing <https://arxiv.org/abs/2312.09020>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
If you want a good model you need to train it well with tons of augmentation the first time. When you get a new task and fine tune you will remain robust to augmentation (noise) if if you don't fine tune with it. 

------------


`Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers <https://www.together.ai/blog/stripedhyena-7b>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
What's after transformers? Any ideas? Yeah there are some and sometimes they are better sometimes. It's based on `signal processing inspired sequence models <https://hazyresearch.stanford.edu/blog/2023-06-08-hyena-safari>`_ which means that sometimes you use an FFT. Miles this sounds up your alley.

------------

`NeurIPS 2023 <https://neurips2023.vizhub.ai>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The conference is in December and this is a cool visualization to see trends. Lots to explore. Maybe next month.

------------

`GenDeF: Learning Generative Deformation Field for Video Generation <https://arxiv.org/pdf/2312.04561.pdf>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Video generation via warping one image rather than generating multiple frames in a row.  Unclear how it extends to long videos, but might have some insights for analyzing videos