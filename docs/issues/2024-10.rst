The CoVar Zeitgeist: 2024-10
============================

Featured
--------

`Aligning Machine and Human Visual Representations across Abstraction Levels <https://arxiv.org/pdf/2409.06509>`_
    From Deepmind.  Attempts to improve VLMs by making them think more like humans - that is, hierarchically, from coarse to fine grained concepts. Uses a teacher model to mimic this intuition and inpart it to a student model.  Best performance in the field etc etc

`An overview of domain-specific foundation model: key technologies, applications and challenges <https://arxiv.org/pdf/2409.04267>`_
    Review paper for foundation models, what they are, how to use them, etc.  We want to/are already doing this, and this is a nice summary of the field.

`Can Unconfident LLM Annotations Be Used for Confident Conclusions? <https://arxiv.org/pdf/2408.15204>`_
    Investigates how to best use LLMs to label a large amount of unlabelled data in a manner where you can be somewhat reliably assured of the outcome. Uses "active inference" (whatever that is) to strategically select the best datapoints to have humans label to increase the LLMs performance. Could be a useful thing for us to know how to do.

`Moving from Machine Learning to Statistics: the case of Expected Points in American football <https://arxiv.org/pdf/2409.04889>`_
    Publicly available football analytics is apparently a bit of the wild west where machine learning tools are just thrown all over the place.  This paper claims that this methodology ignores some important statistical properties of the data which, when taken into account, can improve performance.  Demonstrates that understanding and properly modelling data is still important.

`SCIAGENTS: AUTOMATING SCIENTIFIC DISCOVERY THROUGH MULTI-AGENT INTELLIGENT GRAPH REASONING <https://arxiv.org/pdf/2409.05556>`_
    Researchers from MIT propose a new method for doing reasoning over knowledge graphs.  Could be useful for LitCoin/ODIN/Translator.

`Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models <https://arxiv.org/pdf/2409.07452>`_
    A new method from Fudan University that strings together multiple U-Nets, Encoders, and Decoders to turn a single image of an object into a 3D mesh of that object, and then use Gaussian Splatting to create a video.  Results seem impressive.  We might be able to leverage this for 3D object detection purposes.

LLMs
----
`Can Unconfident LLM Annotations Be Used for Confident Conclusions? <https://arxiv.org/pdf/2408.15204>`_
    Investigates how to best use LLMs to label a large amount of unlabelled data in a manner where you can be somewhat reliably assured of the outcome. Uses "active inference" (whatever that is) to strategically select the best datapoints to have humans label to increase the LLMs performance. Could be a useful thing for us to know how to do.

`In Defense of RAG in the Era of Long-Context Language Models <https://arxiv.org/pdf/2409.01666>`_
    NVIDIA proposes a new method for long-context RAGs which puts them back on top of long-context LLMs.  Short paper, but they seem to have receipts.

`Can LLMs Generate Novel Research Ideas? <https://arxiv.org/pdf/2409.04109>`_
    Group from Stanford runs a study where they compare human-generated an LLM-generated research proposals and find that the LLMs are both mroe novel and less feasible than the humans, on average.  Could be something signifcant about LLMs, could be that humans are trained to generate ideas that get funding.

VLMs
----
`Aligning Machine and Human Visual Representations across Abstraction Levels <https://arxiv.org/pdf/2409.06509>`_
    From Deepmind.  Attempts to improve VLMs by making them think more like humans - that is, hierarchically, from coarse to fine grained concepts. Uses a teacher model to mimic this intuition and inpart it to a student model.  Best performance in the field etc etc

Doctrinaire
-----------
`Future Does Matter: Boosting 3D Object Detection with Temporal Motion Estimation in Point Cloud Sequences <https://arxiv.org/pdf/2409.04390>`_
    A framework for 3D object detection framework that can learn spatio-temporal features.  Possibly of interest to us if we go this route.  Has my favorite type of paper section, the appendix that starts with "the reviewer wanted..."    

`Deep Clustering of Remote Sensing Scenes through Heterogeneous Transfer Learning <https://arxiv.org/pdf/2409.03938>`_
    A collaboration between Texas A&M and the Intelligence and Space Research at Los Alamos, this paper clusters remote sensing scenes using (1) a pretrained neural net, (2) manifold projection, and (3) Bayesian clustering techniques.  Decently effective, and feels like maybe we can leverage a similar approach for automatically detecting image backgrounds and doing context-based learning thereafter.

`Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models <https://arxiv.org/pdf/2409.07452>`_
    A new method from Fudan University that strings together multiple U-Nets, Encoders, and Decoders to turn a single image of an object into a 3D mesh of that object, and then use Gaussian Splatting to create a video.  Results seem impressive.  We might be able to leverage this for 3D object detection purposes.

Autonomy
--------

Reasoning
---------
`SCIAGENTS: AUTOMATING SCIENTIFIC DISCOVERY THROUGH MULTI-AGENT INTELLIGENT GRAPH REASONING <https://arxiv.org/pdf/2409.05556>`_
    Researchers from MIT propose a new method for doing reasoning over knowledge graphs.  Could be useful for LitCoin/ODIN/Translator.

`HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs <https://arxiv.org/pdf/2409.06692>`_
    Proposes a new fact-checking method for knowledge graphs leveraging ensemble methods.  Doubles the "best" AUC from 0.14 to 0.27.

Tracking
--------
`Gaussian Process Upper Confidence Bounds in Distributed Point Target Tracking over Wireless Sensor Networks <https://arxiv.org/pdf/2409.07652>`_
    This paper has a coathuor from DEVCOM Army Reserach  Lab.  Uses a Gaussian Process approach for point-tracking with Bayesian filtering.  Lots of pretty pictures.

Gaussian Splatting
------------------

Gotta Go Fast
-------------

Geometric Deep Learning
-----------------------

Adversarial
-----------
`LoRID: Low-Rank Iterative Diffusion for Adversarial Purification <https://arxiv.org/pdf/2409.08255>`_
    Researchers at Los Alamos develop an interative diffusion process to remove adversarial perturbations from images.  Reading the paper does kind of feel like joing a discourse halfway through, so presumably this is an area of research in the literature.

Out of Distribution
-------------------
`RESULTANT: INCREMENTAL EFFECTIVENESS ON LIKELIHOOD FOR UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION <https://arxiv.org/pdf/2409.03801>`_
    A paper that focusses specifically on hard out-of-distribution detection problems as opposed to easy ones.  Some good thoughts in here on focussing on incremental increases over a baseline.

Theory
------
`An overview of domain-specific foundation model: key technologies, applications and challenges <https://arxiv.org/pdf/2409.04267>`_
    Review paper for foundation models, what they are, how to use them, etc.  We want to/are already doing this, and this is a nice summary of the field.

`Theory, Analysis, and Best Practices for Sigmoid Self-Attention <https://arxiv.org/pdf/2409.04431>`_
    Apple investigates what happens when you use sigmoid self-attention instead of ReLu or softmax.  A bit of a lab manual, but a nice treatment of the subject.

`Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold <https://arxiv.org/pdf/2408.14608>`_
    Proposes a new method, based on some fancypants mathematics/physics, to model systems where a large amount of interacting entities evovle continuously over time.  The main application is single-cell drug screen tests, but you could see appplications to other agent-based modelling areas such as modelling warfighters.

`BREAKING NEURAL NETWORK SCALING LAWS WITH MODULARITY <https://arxiv.org/pdf/2409.05780>`_
    A research group from MIT investigates how modular neural nets can improve on normal neural nets.  They claim that regular neural nets require an exponential number of samples in task dimensionality while modular neural nets are independent.  Using this, they propose a whole bevy of improvements.

Applications
------------
`Causal effect of the infield shift in the MLB <https://arxiv.org/pdf/2409.03940>`_
    Finds that the infield shift was in fact effective at preventing runs, but especially so against left-handed batters.  Apparently there hadn't been a causal analysis of the subject, which makes the MLB's decision to ban the infield shift funny even if it was validated in hindsight.

`Moving from Machine Learning to Statistics: the case of Expected Points in American football <https://arxiv.org/pdf/2409.04889>`_
    Publicly available football analytics is apparently a bit of the wild west where machine learning tools are just thrown all over the place.  This paper claims that this methodology ignores some important statistical properties of the data which, when taken into account, can improve performance.  Demonstrates that understanding and properly modelling data is still important.

`A Framework for Predicting the Impact of Game Balance Changes through Meta Discovery <https://arxiv.org/pdf/2409.07340>`_
    A cool paper that uses RL algorithms to simulate the metagame on Pokemon Showdown, with an interest in simulating metagames after certain pokemon are banned. 

New Models
--------
`OLMoE: Open Mixture-of-Experts Language Models <https://arxiv.org/pdf/2409.02060>`_
    A 7B parameter mixture of experts model that uses only 1B parameters per input token.  Claims to outperform all similarly-sized models and even some bigger ones (shock).  Weights are available.

`Introducing OpenAI o1-preview <https://openai.com/index/learning-to-reason-with-llms/>`_
    OpenAI gets LLMs to be much better at reasoning by training them to think about things before they answer.  Simple idea, but the results are incredibly impressive.

`WHAT MAKES A MAZE LOOK LIKE A MAZE? <https://arxiv.org/pdf/2409.08202>`_
    A new VLM which has a better understanding of abstract concepts such as what a maze looks like.

Lunch and Learn
---------------
2024-09-10
    `Matryoshka Representation Learning <https://arxiv.org/pdf/2205.13147>`_
    A neat way to trade off embedding size for performance on downstream tasks - e.g., image/document retrieval/classification - without training multiple networks. This capability may be useful for multi-platform AiTR, where available bandwidth may vary depending on network conditions.