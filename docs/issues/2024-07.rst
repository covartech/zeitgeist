2024-06
=======

Featured
--------
`Large Language Models Must Be Taught to Know What They Don’t Know <https://arxiv.org/pdf/2406.08391>`_
    One of the problems with LLMs is that you have no idea if they're making stuff up or not - there is no uncertainty estimate.  This paper devises a method for LLM confidence calibration which involves finetuning the LLM in order to estimate confidence.  The finetuned LLM can then also be used to provide confidence estimates for other models.

`Interpreting the Second-Order Effects of Neurons in CLIP <https://arxiv.org/pdf/2406.04341>`_
    How to interpret what individual neurons are doing in CLIP?  Look at the effect of these neurons flowing through the model, through the attention heads, and directly to the output.  Neurons end up polysemantic (think "ships" and "cars") and a clever use of these neurons can improve model capabilities.

`Knowledge Circuits in Pretrained Transformers <https://arxiv.org/pdf/2405.17969>`_
    Examines the "computation graph" of small LLMs such as GPT2 and TinyLLAMA to uncover "knowledge circuits" that shed some sort of light on how LLMs work.  Seems interesting, with some implications towards hallucinations and in-context learning.

`Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image <https://arxiv.org/pdf/2406.04343>`_
    Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interative process to get things out of line of sight.  Seems pretty cool, and I'm astounded it works as well as it does.

`To Believe or Not to Believe Your LLM <https://arxiv.org/pdf/2406.02543>`_
    From Deepmind.  Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.  Really cool paper.  Seems morally similar to some things Miles was poking around at a few months ago.

`Neural Redshift: Random Networks are not Random Functions <https://arxiv.org/pdf/2403.02241>`_
    Neural networks have been thought to have a simplicity bias favoring simple simple solutions to problems.  This paper demonstrates that this is not an inherent feature of neural networks but instead an emergent feature of neural architecture and activation functions, with, e.g., ReLu's favoring simple responses and tanh's favoring complicated ones.  

LLM Applications
----


LLM Theory
----------
`Knowledge Circuits in Pretrained Transformers <https://arxiv.org/pdf/2405.17969>`_
    Examines the "computation graph" of small LLMs such as GPT2 and TinyLLAMA to uncover "knowledge circuits" that shed some sort of light on how LLMs work.  Seems interesting, with some implications towards hallucinations and in-context learning.

`To Believe or Not to Believe Your LLM <https://arxiv.org/pdf/2406.02543>`_
    From Deepmind.  Introduces an information-theory based approach to detect when hallucinations are occurring by measuring epistemic uncertainty about knowledge.  Really cool paper. Seems morally similar to some things Miles was poking around at a few months ago.

`Does your data spark joy? Performance gains from domain upsampling at the end of training <https://arxiv.org/pdf/2406.03476>`_
    From Databricks.  Oversampling from a particular domain at the end of training finetunes your LLM to that domain.  Seems like a cool result, then you realize it's kind of just training plus finetuning combined into one?

`LLM Dataset Inference Did you train on my dataset? <https://arxiv.org/pdf/2406.06443>`_
    Mostly inspired by attempts to enforce copyright law, this paper develops a new method for testing whether an LLM has been trained on a given dataset.

`How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad <https://arxiv.org/pdf/2406.06467>`_
    From Apple.  Investigates how well transformers can reason.  Seems interesting, but is clearly from an academic subfield that uses a lot of jargon and assumes you know what it means.
    
`Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals <https://arxiv.org/pdf/2402.11655>`_
    Another paper in the "how do LLMs work" genre, but rather than focus on any one mechanism such as induction heads or factual recall in MLP layers, this looks at a variety.  In particular, it examines how these different mechanisms compete with each other when, e.g., asked a counterfactual

`Large Language Models Must Be Taught to Know What They Don’t Know <https://arxiv.org/pdf/2406.08391>`_
    One of the problems with LLMs is that you have no idea if they're making stuff up or not - there is no uncertainty estimate.  This paper devises a method for LLM confidence calibration which involves finetuning the LLM in order to estimate confidence.  The finetuned LLM can then also be used to provide confidence estimates for other models.

Ethics
------
`The Impossibility of Fair LLMs <https://arxiv.org/pdf/2406.03198>`_
    Might be a useful reference for ASIMOV.  Looks at traditional definitions of fairness, finds limitations as they apply to LLMs, and thinks about new ones.

`Collective Constitutional AI: Aligning a Language Model with Public Input <https://arxiv.org/pdf/2406.07814>`_
    Who gets to make decisions about how LLMs should behave?  Anthropic says it should be "all of us" and develops a method for crowdsourcing this.

Doctrinaire
-----------


Autonomy
--------


Theory
------
`Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality <https://arxiv.org/pdf/2405.21060>`_
    Transformers and SSMs such as Mamba are actually very similar archtectures and have a large number of similarities that had yet to be noticed (but despite the name of the paper aren't actually the same thing?).  This paper uses these connections to propose a new SSM that is competitive with Transformers on language tasks. 

`einspace: Searching for Neural Architectures from Fundamental Operations <https://arxiv.org/pdf/2405.20838>`_
    Proposes a method to search for the best neural architecture for a given task.  Seems kind of interesting in theory, but I wonder how useful it will be in practice - a lot of architectures are "good enough" given a set of data.

`Evidence of Learned Look-Ahead in a Chess-Playing Neural Network <https://arxiv.org/pdf/2406.00877>`_
    A cool examination of how the current best neural network for playing chess operates.  Dives into the nuts and bolts of how the NN functions with regards to, e.g., planning moves.

`Interpreting the Second-Order Effects of Neurons in CLIP <https://arxiv.org/pdf/2406.04341>`_
    How to interpret what individual neurons are doing in CLIP?  Look at the effect of these neurons flowing through the model, through the attention heads, and directly to the output.  Neurons end up polysemantic (think "ships" and "cars") and a clever use of these neurons can improve model capabilities.

`Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement <https://arxiv.org/pdf/2406.07515>`_
    From Meta.  Investigates how to train models on (partially) synthetic data to avoid the model collapse phenomena.  The answer they come up with is to use reinforcement learning to select the best data, in part because it's a relatively easy task to tell between good and bad data.

`Compute Better Spent: Replacing Dense Layers with Structured Matrices <https://arxiv.org/pdf/2406.06248>`_
    Another paper in the "actually, better compute" genre.  Recommend structured matrices as replacements for dense matrices and show some promising results.

`Neural Redshift: Random Networks are not Random Functions <https://arxiv.org/pdf/2403.02241>`_
    Neural networks have been thought to have a simplicity bias favoring simple simple solutions to problems.  This paper demonstrates that this is not an inherent feature of neural networks but instead an emergent feature of neural architecture and activation functions, with, e.g., ReLu's favoring simple responses and tanh's favoring complicated ones.  

Gaussian Splatting
------------------
`SATSPLATYOLO: 3D GAUSSIAN SPLATTING-BASED VIRTUAL OBJECT DETECTION ENSEMBLES FOR SATELLITE FEATURE RECOGNITION <https://arxiv.org/pdf/2406.02533>`_
    Learns Gaussian splats from remote sensing data and then applies Yolo-3D on the resulting point cloud to do detections.  An interesting approach, though I'm not sure it's better than a CNN on imagery.

`Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image <https://arxiv.org/pdf/2406.04343>`_
    Reconstructs a 3D render of a scene from a single image using 3D Gaussians, monocular depth estimation, and an interative process to get things out of line of sight.  Seems pretty cool, and I'm astounded it works as well as it does.
    
`Trim 3D Gaussian Splatting for Accurate Geometry Representation <https://arxiv.org/pdf/2406.07499>`_
    Introduces a new method into Gaussian splatting to trim the Gaussian to enforce geometric patterns.  Seems to really improve rendering parts of pictures that can end up blurry with the usual methods.

`ICE-G: Image Conditional Editing of 3D Gaussian Splats <https://arxiv.org/pdf/2406.08488>`_
    From Google.  A method to edit a 3D Gaussian splatting render using DINO.  Probably a good reference to have on hand.

Reasoning/Knowledge Graphs
--------------------------

FPGA
----
`Scalable MatMul-free Language Modeling <https://arxiv.org/pdf/2406.02528>`_
    Apparently, matrix multiplication in LLMs is completely optional.  There are, as you might imagine, huge computational benefits to be gleaned here - in particular, this paper puts LLMs on an FPGA.

Applications
------------


New Models
----------
`U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation <https://arxiv.org/pdf/2406.02918>`_
    Implements a KAN-based NN modelled after U-Net for computer vision.  Claims that it outperforms traditional MLPs and gives results backing this up by comparing it to off-the-shelf models.  Improvement is, to be fair, only a little bit better than state of the art MLPs.

Lunch and Learn
---------------
