2024-08
=======

Featured
--------
`SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting <https://arxiv.org/pdf/2406.20055>`_
    From Deepmind.  Proposes a novel Gaussian Splatting method which can effectively ignore interfering objects.  We've noticed on EID that this can lead to weird splats, so ignoring it is quite nice.

`The Art of the Steal: Purloining Deep Learning Models Developed for an Ultrasound Scanner to a Competitor Machine <https://arxiv.org/pdf/2407.03512>`_
    If you put a proprietary DL algorithm on a device, anyone with access to the device can recreate, or "steal" the model weights of the original algorithm by using the device to label a bunch of data and training a new algorithm on that data.  This paper proposes a better way to do that which essentially replicates the performance of the original algorithm.

`Similarity Distance-Based Label Assignment for Tiny Object Detection <https://arxiv.org/pdf/2407.02394>`_
    A better method for non-max suppression for tiny object detection.  Seems to greatly improve performance of faster R-CNN on AITOD.  Worth looking into for any of our tiny object detection problems.

`Learning to (Learn at Test Time): RNNs with Expressive Hidden States <https://arxiv.org/pdf/2407.04620>`_
    New hidden state model with linear complexity in context length.  Seems to outperform both transformers and Mamba in terms of computatoinal time and results.  Impressive if true.

`Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA <https://arxiv.org/pdf/2407.02362>`_
    A new matrix multiplication method for putting neural nets on FPGAs.  Much more efficient than the baseline.

`Single Character Perturbations Break LLM Alignment <https://arxiv.org/pdf/2407.03232>`_
    LLMs tend to have safeguards built in like "don't tell people how to build bombs".  This paper got around these safeguards simply by appending a token of whitespace at the end of input prompts.  This happens because,in training, whitespace prompts the model to make lists and this overrides the "don't make bombs" instruction.

LLMs
----------
`Single Character Perturbations Break LLM Alignment <https://arxiv.org/pdf/2407.03232>`_
    LLMs tend to have safeguards built in like "don't tell people how to build bombs".  This paper got around these safeguards simply by appending a token of whitespace at the end of input prompts.  This happens because,in training, whitespace prompts the model to make lists and this overrides the "don't make bombs" instruction.

`LLM-Select: Feature Selection with Large Language Models <https://arxiv.org/pdf/2407.02694>`_
    Uses LLMs to select features for predictive models.  They claim this works as well as something like LASSO and might obviate the need for doing analysis entirey.  I'm less than convinced.  To the extent this is true it must be because you're working on a problem that exists in the literature and the LLM essentially just did a literature review for you.  

`On Large Language Models in National Security Applications <https://arxiv.org/pdf/2407.03453>`_
    Two professors from the Air Force Institute of Technology talk about the use case of LLMs in the Air Force.  I'm not sure how much is new, but it's relatively short and probably worth a read for anyone involved in BD or LLMs.  

`On scalable oversight with weak LLMs judging strong LLMs <https://arxiv.org/pdf/2407.04622>`_
    From Deepmind.  Is interested in whether a human can supervise the training of a supersmart LLM and still have it "aligned".  To test this, runs through some simulations where a "weak" LLM judges one or more "strong" LLMs and finds that, in the context of debate, a judge can align LLMs which are smarter than it is.

`Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs <https://arxiv.org/pdf/2407.04694>`_
    Are LLMs aware that they are LLMs?  If they are, does this change how they behave?  The authors investigate this and say "not really", but it's an interesting thought because roleplay is generally a way to get an LLM to unlock new capabilities.

Ethics
------

Autonomy
--------
`TREE SEARCH FOR LANGUAGE MODEL AGENTS <https://arxiv.org/pdf/2407.01476>`_
    How to get an LLM to act as an autonomous agent with decent-to-good reasoning?  Make it perform a tree search!  This seems similar to how beam search operates and could be an interesting way to empower an autonomous UAS.

Doctrinaire
-----------
`Similarity Distance-Based Label Assignment for Tiny Object Detection <https://arxiv.org/pdf/2407.02394>`_
    A better method for non-max suppression for tiny object detection.  Seems to greatly improve performance of faster R-CNN on AITOD.  Worth looking into for any of our tiny object detection problems.

Theory
------
`A Theory of Interpretable Approximations <https://arxiv.org/pdf/2406.10529>`_
    From Google.  Investigates under what circumtsances neural nets can be interpreated with rather simple decision trees.  Develops some bounds for this.  Incredibly interesting, but incredibly theoretical.

`The Art of the Steal: Purloining Deep Learning Models Developed for an Ultrasound Scanner to a Competitor Machine <https://arxiv.org/pdf/2407.03512>`_
    If you put a proprietary DL algorithm on a device, anyone with access to the device can recreate, or "steal" the model weights of the original algorithm by using the device to label a bunch of data and training a new algorithm on that data.  This paper proposes a better way to do that which essentially replicates the performance of the original algorithm.

`Analytic Convolutional Layer: A Step To Analytic Neural Network <https://arxiv.org/pdf/2407.06087>`_
    Presents a new convolutional kernel which is both computationally more efficient in some cases and more interpretable.  I'm not sure this paper really gets there, but it's worth keeping an eye on stuff like this in case interpretable neural nets ever become a thing.

Gaussian Splatting
------------------
`SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting <https://arxiv.org/pdf/2406.20055>`_
    From Deepmind.  Proposes a novel Gaussian Splatting method which can effectively ignore interfering objects.  We've noticed on EID that this can lead to weird splats, so ignoring it is quite nice.

`Segment Any 4D Gaussians <https://arxiv.org/pdf/2407.04504>`_
    Segment Anything for 4D Gaussian splatting.  Looks pretty impressive, but I struggle to think of a use case for 4D Gaussian splatting over 3D.  Maybe I lack imagination.

FPGA
----
`Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA <https://arxiv.org/pdf/2407.02362>`_
    A new matrix multiplication method for putting neural nets on FPGAs.  Much more efficient than the baseline.

Knowledge Graphs
----------------

Applications
------------

New Models
----------
`Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials <https://ai.meta.com/research/publications/meta-3d-assetgen-text-to-mesh-generation-with-high-quality-geometry-texture-and-pbr-materials/?utm_source=twitter&utm_medium=organic_social&utm_content=thread&utm_campaign=research>`_
    Meta presents a novel model for generating 3D objects from text or image inputs.  The examples look incredibly impressive.  Anyone working on recovering CAD models/3D representations of objects should take a look at this.

`Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects <https://ai.meta.com/research/publications/meta-3d-texturegen-fast-and-consistent-texture-generation-for-3d-objects/?utm_source=twitter&utm_medium=organic_social&utm_content=thread&utm_campaign=research>`_
    Meta presents a novel model for generating textures for 3D objects.  Probably supposed to work with AssetGen, this also looks suitably impressive.

`InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output <https://arxiv.org/pdf/2407.03320>`_
    Novel open-source large vision language model.  Can handle text-to-image and image-to-text.  Fairl extensive benchmarking, seems about on par with GPT-4.  Claims to be the best open source VLM.

`Learning to (Learn at Test Time): RNNs with Expressive Hidden States <https://arxiv.org/pdf/2407.04620>`_
    New hidden state model with linear complexity in context length.  Seems to outperform both transformers and Mamba in terms of computatoinal time and results.  Impressive if true.

Lunch and Learn
---------------
2024-07-02
    `Scalable MatMul-free Language Modeling<https://arxiv.org/pdf/2406.02528>`_
    (Was in last month's issue) Basically Replace the MatMul with Ternary weights (making it addition only operation) and replace the self-attention with a ternary GRU. Dramatically increases throughput / watt. Similar to this paper: `The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits <https://arxiv.org/pdf/2402.17764>`_

    `Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP <https://arxiv.org/pdf/2406.17639>`_
    (Was in last month's issue) Also brought up this paper which makes a better embedding space for text and images by tweaking the CLIP loss. Makes the embeddings relatively similar for intra-modality representation.

    ``