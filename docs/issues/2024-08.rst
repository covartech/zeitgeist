2024-08
=======

Featured
--------
`Click-Gaussian: Interactive Segmentation to Any 3D Gaussians <https://arxiv.org/pdf/2407.11793>`_
    A 3D Gaussian Splatting renderer/UI that allows the user to segment any object inside the render by clicking on it and adjusting a parameter.  We're using this (or something morally equivalent) for EID and it's pretty cool.

`ViLLa: Video Reasoning Segmentation with Large Language Model <https://arxiv.org/pdf/2407.14500>`_
    A novel vision LLM which takes a text prompt as an input and segments the relevant objects in a video, possibly with a text description as well.  Looks pretty powerful in a limited number of examples, but code is coming "soon".

`Distilling Tiny and Ultra-fast Deep Neural Networks for Autonomous Navigation on Nano-UAVs <https://arxiv.org/pdf/2407.12675>`_
    Builds a tiny CNN to function as the brain on a "nano-drone".  Lots of potential with a swarm of these things coordinating together.

`Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness <https://arxiv.org/pdf/2407.08972>`_
    ConvNets with extremely large kernels achieve comparable/superior performance compared to vision transformers.  This paper demonstrates this for robustness and investigates why.

`CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference <https://arxiv.org/pdf/2407.12736>`_
    A bunch of people from USC develop a software for putting vision transformers on FPGAs.  Lots of stuff in here that's slightly above my head, but the results look legitimate.  

`Don’t Throw Away Data: Better Sequence Knowledge Distillation <https://arxiv.org/pdf/2407.10456>`_
    From Deepmind.  The problem investigated is how to manage student-teacher cases where you have a big LLM (the teacher) that can perform a specific task (e.g. language translation) but you want a smaller model (the student) to be able to perform the same task.  Finds the most optimal method of teaching the student.  We might be able to use similar methods for cases where LLMs are big but useful in a limited way.  Maybe ODIN?

LLMs
----------
`Single Character Perturbations Break LLM Alignment <https://arxiv.org/pdf/2407.03232>`_
    LLMs tend to have safeguards built in like "don't tell people how to build bombs".  This paper got around these safeguards simply by appending a token of whitespace at the end of input prompts.  This happens because,in training, whitespace prompts the model to make lists and this overrides the "don't make bombs" instruction.

`LLM-Select: Feature Selection with Large Language Models <https://arxiv.org/pdf/2407.02694>`_
    Uses LLMs to select features for predictive models.  They claim this works as well as something like LASSO and might obviate the need for doing analysis entirey.  I'm less than convinced.  To the extent this is true it must be because you're working on a problem that exists in the literature and the LLM essentially just did a literature review for you.  

`On Large Language Models in National Security Applications <https://arxiv.org/pdf/2407.03453>`_
    Two professors from the Air Force Institute of Technology talk about the use case of LLMs in the Air Force.  I'm not sure how much is new, but it's relatively short and probably worth a read for anyone involved in BD or LLMs.  

`On scalable oversight with weak LLMs judging strong LLMs <https://arxiv.org/pdf/2407.04622>`_
    From Deepmind.  Is interested in whether a human can supervise the training of a supersmart LLM and still have it "aligned".  To test this, runs through some simulations where a "weak" LLM judges one or more "strong" LLMs and finds that, in the context of debate, a judge can align LLMs which are smarter than it is.

`Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs <https://arxiv.org/pdf/2407.04694>`_
    Are LLMs aware that they are LLMs?  If they are, does this change how they behave?  The authors investigate this and say "not really", but it's an interesting thought because roleplay is generally a way to get an LLM to unlock new capabilities.

`SLIP: Securing LLM’s IP Using Weights Decomposition <https://arxiv.org/pdf/2407.10886>`_
    Microsoft develops a method to protect LLM weights from being discovered by decomposing them into two matrices, one of which is recoverable and one of which is not.

`Don’t Throw Away Data: Better Sequence Knowledge Distillation <https://arxiv.org/pdf/2407.10456>`_
    From Deepmind.  The problem investigated is how to manage student-teacher cases where you have a big LLM (the teacher) that can perform a specific task (e.g. language translation) but you want a smaller model (the student) to be able to perform the same task.  Finds the most optimal method of teaching the student.  We might be able to use similar methods for cases where LLMs are big but useful in a limited way.  Maybe ODIN?

Ethics
------

Autonomy
--------
`Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows? <https://arxiv.org/pdf/2407.10956>`_
    From Google Deepmind.  Investigates whether automous agents are able to take our jobs yet by introducing a new benchmarking dataset.  The best performing agent gets 14%, so the answer is no.

`Distilling Tiny and Ultra-fast Deep Neural Networks for Autonomous Navigation on Nano-UAVs <https://arxiv.org/pdf/2407.12675>`_
    Builds a tiny CNN to function as the brain on a "nano-drone".  Lots of potential with a swarm of these things for recon purposes.

`ODYSSEY: Empowering Agents with Open-World Skills <https://arxiv.org/pdf/2407.15325>`_
    Establishes a framework/dataset for open-world autonomous agents in Minecraft.  Seems fun/silly at first, but might have enough similarities to, e.g., drones flying around looking for things to do to be worth keeping an eye on.

Reinforcement Learning
----------------------
`Enhancing Building Safety Design for Active Shooter Incidents: Exploration of Building Exit Parameters using Reinforcement Learning-Based Simulation <https://arxiv.org/pdf/2407.10441>`_
    Incredibly morbid, but there's something here that can be adapted to simulating armed combatants that may have a use case.

Sensing
-------
`Similarity Distance-Based Label Assignment for Tiny Object Detection <https://arxiv.org/pdf/2407.02394>`_
    A better method for non-max suppression for tiny object detection.  Seems to greatly improve performance of faster R-CNN on AITOD.  Worth looking into for any of our tiny object detection problems.

`Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial Vehicles Surveillance in Day and Night Dual Visions <https://arxiv.org/pdf/2407.12647>`_
    Bulids a bespoke model for RGB/IR sensor fusion for counter-UAS activities at day and night-time.  The results seem convincing, may be worth taking inspiration from.

`Is That Rain? Understanding Effects on Visual Odometry Performance for Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge <https://arxiv.org/pdf/2407.12663>`_
    Builds a dataset and a (small) detector for detecting whether or not it is raining outside.  We could use this for sensor fusion, or context-aware sensing.

`Training-Free Model Merging for Multi-target Domain Adaptation <https://arxiv.org/pdf/2407.13771>`_
    Investigates how to do fusion on multiple models spanning multiple domains without access to training data.  Not directly relevant to anything we're doing  (EID would have a fit about all the deep learning), but maybe useful later.

Theory
------
`A Theory of Interpretable Approximations <https://arxiv.org/pdf/2406.10529>`_
    From Google.  Investigates under what circumtsances neural nets can be interpreated with rather simple decision trees.  Develops some bounds for this.  Incredibly interesting, but incredibly theoretical.

`The Art of the Steal: Purloining Deep Learning Models Developed for an Ultrasound Scanner to a Competitor Machine <https://arxiv.org/pdf/2407.03512>`_
    If you put a proprietary DL algorithm on a device, anyone with access to the device can recreate, or "steal" the model weights of the original algorithm by using the device to label a bunch of data and training a new algorithm on that data.  This paper proposes a better way to do that which essentially replicates the performance of the original algorithm.

`Analytic Convolutional Layer: A Step To Analytic Neural Network <https://arxiv.org/pdf/2407.06087>`_
    Presents a new convolutional kernel which is both computationally more efficient in some cases and more interpretable.  I'm not sure this paper really gets there, but it's worth keeping an eye on stuff like this in case interpretable neural nets ever become a thing.

`Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness <https://arxiv.org/pdf/2407.08972>`_
    ConvNets with extremely large kernels achieve comparable/superior performance compared to vision transformers.  This paper demonstrates this for robustness and investigates why.

Gaussian Splatting
------------------
`SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting <https://arxiv.org/pdf/2406.20055>`_
    From Deepmind.  Proposes a novel Gaussian Splatting method which can effectively ignore interfering objects.  We've noticed on EID that this can lead to weird splats, so ignoring it is quite nice.

`Segment Any 4D Gaussians <https://arxiv.org/pdf/2407.04504>`_
    Segment Anything for 4D Gaussian splatting.  Looks pretty impressive, but I struggle to think of a use case for 4D Gaussian splatting over 3D.  Maybe I lack imagination.

`Click-Gaussian: Interactive Segmentation to Any 3D Gaussians <https://arxiv.org/pdf/2407.11793>`_
    A 3D Gaussian Splatting renderer/UI that allows the user to segment any object inside the render by clicking on it and adjusting a parameter.  We're using this (or something morally equivalent) for EID and it's pretty cool.

`Generalizable Human Gaussians for Sparse View Synthesi <https://arxiv.org/pdf/2407.12777>`_
    CMU and Meta develop a novel Gaussian splatting algorithm which can render an entire human using a sparse set (as few as three) pictures.  They do this by leveraging the fact that we know what humans look like, on average, and so can infer a lot from a small number of datapoints in the restricted setting of their problem.

Gotta Go Fast
-------------
`Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA <https://arxiv.org/pdf/2407.02362>`_
    A new matrix multiplication method for putting neural nets on FPGAs.  Much more efficient than the baseline.

`Fast Matrix Multiplications for Lookup Table-Quantized LLMs <https://arxiv.org/pdf/2407.10960>`_
    Another paper in the "make LLM go fast by multiply matrix fast" genre.  Like most of these it goes a bit over my head, but it seems to have quite an interesting approach, using an offline lookup table to supplement its quantized matrix mulitiplication.

`Q-Sparse: All Large Language Models can be Fully Sparsely-Activated <https://arxiv.org/pdf/2407.10969>`_
    Another method for making LLMs go fast.  Acts as a method to sparsify the model, and can be applied ontop of either full precision or 1-bit models.  Maintains performance while going much faster.

`CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference <https://arxiv.org/pdf/2407.12736>`_
    A bunch of people from USC develop a software for putting vision transformers on FPGAs.  Lots of stuff in here that's slightly above my head, but the results look legitimate.  

`A deeper look at depth pruning of LLMs <https://arxiv.org/pdf/2407.16286>`_
    A group at NVIDIA takes a look at various methods for pruning LLMs.  Finds that you can prune a third of Mistral 7B and retain the same performance.  Could be worth a look for our LLM related work.

Knowledge Graphs
----------------

Applications
------------
`A Survey of Distance-Based Vessel Trajectory Clustering: Data Pre-processing, Methodologies, Applications, and Experimental Evaluation <https://arxiv.org/pdf/2407.11084>`_
    A survey on methods for keeping track of ship trajectories.  Could be useful as a reference for monitoring sea vessels.

`Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye Images in Autonomous Driving <https://arxiv.org/pdf/2407.16647>`_
    Does ATR with fish-eye camera, finds that a deformable CNN (where the kernel depends on the shape of the object) outperforms the non-deformable version of CNNs such as ResNets and U-Nets.

New Models
----------
`Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials <https://ai.meta.com/research/publications/meta-3d-assetgen-text-to-mesh-generation-with-high-quality-geometry-texture-and-pbr-materials/?utm_source=twitter&utm_medium=organic_social&utm_content=thread&utm_campaign=research>`_
    Meta presents a novel model for generating 3D objects from text or image inputs.  The examples look incredibly impressive.  Anyone working on recovering CAD models/3D representations of objects should take a look at this.

`Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects <https://ai.meta.com/research/publications/meta-3d-texturegen-fast-and-consistent-texture-generation-for-3d-objects/?utm_source=twitter&utm_medium=organic_social&utm_content=thread&utm_campaign=research>`_
    Meta presents a novel model for generating textures for 3D objects.  Probably supposed to work with AssetGen, this also looks suitably impressive.

`InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output <https://arxiv.org/pdf/2407.03320>`_
    Novel open-source large vision language model.  Can handle text-to-image and image-to-text.  Fairl extensive benchmarking, seems about on par with GPT-4.  Claims to be the best open source VLM.

`Learning to (Learn at Test Time): RNNs with Expressive Hidden States <https://arxiv.org/pdf/2407.04620>`_
    New hidden state model with linear complexity in context length.  Seems to outperform both transformers and Mamba in terms of computatoinal time and results.  Impressive if true.

`Lite-SAM Is Actually What You Need for Segment Everything <https://arxiv.org/pdf/2407.08965>`
    A new, more computationally efficient, method of implementing SAM.  Seems to hold up in terms of results, but examples are sparse.

`LookupViT: Compressing visual information to a limited number of tokens <https://arxiv.org/pdf/2407.12753>`_
    From Deepmind.  There's lots of tokens in images which have very low information content - this paper compresses input tokens to a fixed number of tokens as a method of getting rid of the extraneous tokens.  Improves computational burdens and (sometimes) performance.

`ViLLa: Video Reasoning Segmentation with Large Language Model <https://arxiv.org/pdf/2407.14500>`_
    A novel vision LLM which takes a text prompt as an input and segments the relevant objects in a video, possibly with a text description as well.  Looks pretty powerful in a limited number of examples, but code is coming "soon".

`AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection <https://arxiv.org/pdf/2407.15795>`_
    A novel CLIP variant that functions as a zero-shot anomaly detector/segmenter.  Takes text as an input and semgents whatever was described in the text.  Potentially useful as an "off-the-shelf" tool for a lot of functions.  Maybe a Grounded SAM replacement?

New LLMs
--------
`Codestral Mamba <https://mistral.ai/news/codestral-mamba/>`_
    Mistral drops another LLM, this time based on Mamba and with an Apache 2.0 license.  They say it's good but this particular link is light on resources.

`GPT-4o mini: advancing cost-efficient intelligence <https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/>`_
    A new GPT model which is very small and very cheap yet better than all GPT models across a range of tasks, being outperformed only by GPT-4

`Mistral NeMo <https://mistral.ai/news/mistral-nemo/>`_
    A "drop-in replacement for Mistral 7B", this looks pretty good.  A context window of 128K is the standout here, but it shows some decent results in what is a short blog post.

`The Llama 3 Herd of Models <https://ai.meta.com/research/publications/the-llama-3-herd-of-models/>`_
    Llama 3.1 is out of the gates, with 8B, 70B, and 405B(!!) models. Comes with a 92 page lab report  which probably has some good info.

`Large Enough <https://mistral.ai/news/mistral-large-2407/>`_
    Mistral somehow gets a response to Llama 3 the day after Llamma 3 drops, with Mistral Large 2.  They claim its better than Llama 3.  Who knows, both are too big for us.
    
Lunch and Learn
---------------
2024-07-02
    `Scalable MatMul-free Language Modeling <https://arxiv.org/pdf/2406.02528>`_
    (Was in last month's issue) Basically Replace the MatMul with Ternary weights (making it addition only operation) and replace the self-attention with a ternary GRU. Dramatically increases throughput / watt. Similar to this paper: `The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits <https://arxiv.org/pdf/2402.17764>`_

    `Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP <https://arxiv.org/pdf/2406.17639>`_
    (Was in last month's issue) Also brought up this paper which makes a better embedding space for text and images by tweaking the CLIP loss. Makes the embeddings relatively similar for intra-modality representation.

2024-07-09
    `On Scalable Oversight with weak LLMs judging strong LLMs <https://arxiv.org/abs/2407.04622v1>`_
    Deepmind: What happens when you ask a judge to choose the best answer in 3 scenarios: 2 debaters try to convice the judge, 1 consultant converses with the judge, and we ask the judge directly. Oh, and the debaters, consultants, and judges are all LLMs. The judges are also weaker models than the debaters/consultants. They found that debate is better than consulting; however, the judge used is lowkey too smart here.