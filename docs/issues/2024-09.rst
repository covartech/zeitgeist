2024-09
=======

Featured
--------
`MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction <https://arxiv.org/pdf/2407.21635>`_
    Uses relation transformers to do multi-agent tracking in basketball data.  This kind of makes sense since tracking involves sequences and transformers are good at that.

`Large Language Monkeys: Scaling Inference Compute with Repeated Sampling <https://arxiv.org/pdf/2407.21787>`_
    Generating accurate answers is hard, but verifying an answer is (sometimes) easy.  If you are living in a world where verifying an answer is easy, you can have an LLM generate a ton of answers and find th correct one.  Greatly improves performance.

`Disentangling Dense Embeddings with Sparse Autoencoders <https://arxiv.org/pdf/2408.00657>`_
    If you have dense embeddings, you can hit them with a sparse autoencoder and have sparse embeddings that maintain semantic fidelity.  Feels like there is something useful here, but can't quite put my finger on what.

`Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation <https://arxiv.org/pdf/2408.00744>`_
    Does some fancy footwork to turn CLIP models into something that can more or less accurately handle the open-vocabulary segmentation task.  Results seem impresive.

`SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code <https://arxiv.org/pdf/2403.01248>`_
    Google release a new LLM agent which can take natural language desriptions of a scene, generate Python scripts to render them in blender, and then iteratively refine the rendered scene.  Could be useful for some of our synthetic data generation.

LLMs
----
`Large Language Monkeys: Scaling Inference Compute with Repeated Sampling <https://arxiv.org/pdf/2407.21787>`_
    Generating accurate answers is hard, but verifying an answer is (sometimes) easy.  If you are living in a world where verifying an answer is easy, you can have an LLM generate a ton of answers and find th correct one.  Greatly improves performance.

`LLM Critics Help Catch LLM Bugs <https://arxiv.org/pdf/2407.00215>`_
    OpenAI proposes a new training method for LLMs to replace RLHF - use other LLMs to help grade the output of an LLM in a training process. Not directly applicable for us unless we want to train an LLM from scratch, but another paper using the idea of "generate some stuff with an LLM and then evaluate for downstream use" which seems to work well and might be relevant.

`SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code <https://arxiv.org/pdf/2403.01248>`_
    Google release a new LLM agent which can take natural language desriptions of a scene, generate Python scripts to render them in blender, and then iteratively refine the rendered scene.  Could be useful for some of our synthetic data generation.

VLMs
----

Doctrinaire
-----------

Autonomy
--------
`NOLO: Navigate Only Look Once <https://arxiv.org/pdf/2408.01384>`_
    Develops a transformer model to control navigation on a drone based on input video/images.

Reinforcement Learning
----------------------

Fusion
------

Tracking
--------
`MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction <https://arxiv.org/pdf/2407.21635>`_
    Uses relation transformers to do multi-agent tracking in basketball data.  This kind of makes sense since tracking involves sequences and transformers are good at that.

Gaussian Splatting
------------------

Gotta Go Fast
-------------

Theory
------
`Disentangling Dense Embeddings with Sparse Autoencoders <https://arxiv.org/pdf/2408.00657>`_
    If you have dense embeddings, you can hit them with a sparse autoencoder and have sparse embeddings that maintain semantic fidelity.  Feels like there is something useful here, but can't quite put my finger on what.

`Autoencoders in Function Space <https://arxiv.org/pdf/2408.01362>`_
    Develops a variational autoencoder which functions directly on function space.  The imagined applications included computer vision, with image pixels being viewed as a pixelization of a functional space. Shows some promise on inpainting/superresolution problems.  Very theoretical, though

Applications
------------

New LLMs
--------
`Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma <https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/>`_
    Google adds three new additions to the Gemma 2B family.  They claim its the best thing on the market, etc etc.  `Lab report <https://arxiv.org/pdf/2408.00118>`_
    
Lunch and Learn
---------------
