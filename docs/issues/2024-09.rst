2024-09
=======

Featured
--------
`Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures <https://www.arxiv.org/pdf/2407.09468>`_
    Cool overview with lots of pictures. Doing ML in non-vector spaces is basically what we need to do.

`Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing <https://arxiv.org/pdf/2404.01223>`_
    Combines 3D Gaussian splats with VLMS and physics-based models to enable text-based scened decomposition and to simulate physics-based dynamics in a 3D Gaussian splat.  Duct-tapes a whole bunch of models together to get to a cool looking result

`Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models <https://gonenhila.github.io/files/Semantic_Leakage.pdf>`_
    Finds a new way LLMs can go horribly wrong, called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny's job is, it will say that Kenny is a school bus driver because school busses are yellow. 

`Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation <https://arxiv.org/pdf/2408.00744>`_
    Does some fancy footwork to turn CLIP models into something that can more or less accurately handle the open-vocabulary segmentation task.  Results seem impresive.

`Out-of-Distribution Learning with Human Feedback <https://arxiv.org/pdf/2408.07772>`_
    How do you deal with out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from "wild data" (a great name), uses human feedback to label them, and then trains to both classify and identify OOD objects.  It's a cool method, and one we might be able to use in a "how to use the minimun amount of labels to best improve model performance" sort of way.

`Bye-Bye, Bye Advantage: Estimating the competitive impact of rest differential in the National Football League <https://arxiv.org/pdf/2408.10867>`_
    Important paper for all you fantasy football fans. Employs a Bayesian State Space model to model investigate the effects of having a bye week on a teams performance.  Finds a rather strong effect before the 2011 Collective Bargaining Agreement, but much less of an effect afterwards.

LLMs
----
`Large Language Monkeys: Scaling Inference Compute with Repeated Sampling <https://arxiv.org/pdf/2407.21787>`_
    Generating accurate answers is hard, but verifying an answer is (sometimes) easy.  If you are living in a world where verifying an answer is easy, you can have an LLM generate a ton of answers and find th correct one.  Greatly improves performance.

`LLM Critics Help Catch LLM Bugs <https://arxiv.org/pdf/2407.00215>`_
    OpenAI proposes a new training method for LLMs to replace RLHF - use other LLMs to help grade the output of an LLM in a training process. Not directly applicable for us unless we want to train an LLM from scratch, but another paper using the idea of "generate some stuff with an LLM and then evaluate for downstream use" which seems to work well and might be relevant.

`SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code <https://arxiv.org/pdf/2403.01248>`_
    Google release a new LLM agent which can take natural language desriptions of a scene, generate Python scripts to render them in blender, and then iteratively refine the rendered scene.  Could be useful for some of our synthetic data generation.

`Self-Taught Evaluators <https://arxiv.org/pdf/2408.02666>`_
    From META FAIR.  Looks to improve evaluators using only synthetic data - no human labelling, no RLHF.  This sounds crazy, but training a judge in this way, without any human-labelled data, and using it to make Lamma 3 better actually made Llama 3 substantially better.

`Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters <https://arxiv.org/pdf/2408.03314>`_
    Deepmind investigates how best to use a finite amount of test-time compute to get an LLM to give the best answer.  Results are nuanced and the paper seems worth reading.

`SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models <https://arxiv.org/pdf/2407.09025>`_
    Microsoft figures out how to give a spreadsheet to an LLM. Not really a problem we have, but somewhat interesting in that, represented data effectively to an LLM is not always super straightforward.

`The Geometry of Categorical and Hierarchical Concepts in Large Language Models <https://arxiv.org/pdf/2406.01506>`_
    If you ask an LLM to make a graph, or you just analyze them concepts understood by the graph, they have similar structure. Somewhat interesting that the implicit structure of the “understanding” of the LLM has graph like characteristics, hierarchy and relationships.

`Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models <https://gonenhila.github.io/files/Semantic_Leakage.pdf>`_
    Finds a new way LLMs can go horribly wrong, called semantic leakage, where two unrelated concepts get linked together in creative ways.  For instance, if you tell an LLM that Kenny likes the color yellow and then ask it what Kenny's job is, it will say that Kenny is a school bus driver because school busses are yellow. 

`To Code, or Not To Code? Exploring Impact of Code in Pre-training <https://arxiv.org/pdf/2408.10914>`_
    From Cohere.  Investigates the effects of including code in the training data for your LLM, and finds some pretty surprising results - mostly that including code helps improve the performance of your LLM on other tasks such as NLP reasoning and world knowledge.  Begs the question of "why", but they don't really answer that.

VLMs
----
`LONGVILA: SCALING LONG-CONTEXT VISUAL LANGUAGE MODELS FOR LONG VIDEOS <https://arxiv.org/pdf/2408.10188>`_
    NVIDIA continues to chip away at VLMs, this time giving VILA extra long context length.  Lots of good stuff in here about training, datasets, performance, etc.

Doctrinaire
-----------
`MESHANYTHING V2: ARTIST-CREATED MESH GENERATION WITH ADJACENT MESH TOKENIZATION <https://arxiv.org/pdf/2408.02555>`_
    A model which takes "anything" (point clouds, Gaussian Splats, images, text, etc) and generates 3D meshes of the described object.  Could be useful.

`4D Contrastive Superflows are Dense 3D Representation Learners <https://arxiv.org/pdf/2407.06190>`_
    Cross train vision and lidar to make a pointcloud. Uses a vision transform and claims state of the art performance. Could potentially inspire some of our work to make 3D scenes from vision only.

`Trends, Applications, and Challenges in Human Attention Modelling <https://arxiv.org/pdf/2402.18673>`_
    Our job with AiTR is ultimately to aid human perception. Should we really be drawing boxes? With the new glow work coming up I think we should focus on understanding human attention so that we can better direct it.

`SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views <https://arxiv.org/pdf/2408.10195>`_
    Proposes a method to take one to a small number of unposed images of an object, and create a 3D mesh in 20 seconds.  Seems decent, lots of pictures of videogame characters.

`MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model <https://arxiv.org/pdf/2408.10198>`_
    A novel method for 2D to 3D, sparse image to 3D, and text to 3D generation.  The abstract kind of reads like a madlibs of methods, but the results seem decent.

Autonomy
--------
`NOLO: Navigate Only Look Once <https://arxiv.org/pdf/2408.01384>`_
    Develops a transformer model to control navigation on a drone based on input video/images.

Reinforcement Learning
----------------------

Fusion
------

Tracking
--------
`MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction <https://arxiv.org/pdf/2407.21635>`_
    Uses relation transformers to do multi-agent tracking in basketball data.  This kind of makes sense since tracking involves sequences and transformers are good at that.

Gaussian Splatting
------------------
`Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing <https://arxiv.org/pdf/2404.01223>`_
    Combines 3D Gaussian splats with VLMS and physics-based models to enable text-based scened decomposition and to simulate physics-based dynamics in a 3D Gaussian splat.  Duct-tapes a whole bunch of models together to get to a cool looking result

`3D Gaussian Editing with A Single Image <https://arxiv.org/pdf/2408.07540>`_
    Develops a method that allows you to take a Gaussian splat, compress it to one image, modify that one image (photoshop?), and then generate a novel Gaussian splat corresponding to the changed image.  Seems cool, not sure what the use case it.

`WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting <https://arxiv.org/pdf/2408.08206>`_
    Novel 3D Gaussian Splatting approach for underwater scenes.  Generalizes to foggy/rainy scenes on dry land.  Could be a good tool in a toolbox

Gotta Go Fast
-------------
`CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications <https://arxiv.org/pdf/2408.03703>`_
    How to put vision transformers on an iPhone.  Hilariously, they cite a paper from 2009 showing vision transformer results?? This must be a typo?

`Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters <https://arxiv.org/pdf/2408.04093>`_
    Does some real fancy footwork to get "asymptotically" faster computational results for some part of transformers.  Code available.

`How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model <https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/>`_
    NVIDIA takes Llama-3.1 8B and turns it into a 4B parameter model with minimal decrease in performance somehow.  I guess NVIDIA is just better at this than Meta?

`FPCA: FIELD-PROGRAMMABLE PIXEL CONVOLUTIONAL ARRAY FOR EXTREME-EDGE INTELLIGENCE <https://arxiv.org/pdf/2408.10233>`_
    I don't know what an FPCA is, but it seems morally similar to an FPGA.  Worth keeping an eye on?

Theory
------
`Disentangling Dense Embeddings with Sparse Autoencoders <https://arxiv.org/pdf/2408.00657>`_
    If you have dense embeddings, you can hit them with a sparse autoencoder and have sparse embeddings that maintain semantic fidelity.  Feels like there is something useful here, but can't quite put my finger on what.

`Autoencoders in Function Space <https://arxiv.org/pdf/2408.01362>`_
    Develops a variational autoencoder which functions directly on function space.  The imagined applications included computer vision, with image pixels being viewed as a pixelization of a functional space. Shows some promise on inpainting/superresolution problems.  Very theoretical, though

`Pre-training and in-context learning IS Bayesian inference a la De Finetti <https://arxiv.org/pdf/2408.03307>`_
    A very funny paper that argues pre-training and in-context learning is Bayesian inference because of De Finetti's theorem.  Not useful, but worth it for the comedy.

`Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures <https://www.arxiv.org/pdf/2407.09468>`_
    Cool overview with lots of pictures. Doing ML in non-vector spaces is basically what we need to do.

`Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 <https://arxiv.org/pdf/2408.05147>`_
    Deepmind explores some ways that sparse autoencoders are useful, with a look at the Gemma 2 family of models.  Seems like a decent overview

`Your Classifier Can Be Secretly a Likelihood-Based OOD Detector <https://arxiv.org/pdf/2408.04851>`_
    Another paper in the "classifiers do OOD detection" bin.  Results seem decently convincing?

`Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness <https://arxiv.org/pdf/2408.05446>`_
    From Deepmind.  Proposes a novel, ensemble-based method for robustness to adversarial attacks.  Worth keeping an eye on if this stuff is ever of interest.

`Out-of-Distribution Learning with Human Feedback <https://arxiv.org/pdf/2408.07772>`_
    How do you deal with out-of-distribution (OOD) data?  This paper proposes a method to detect the most important OOD datapoints from "wild data" (a great name), uses human feedback to label them, and then trains to both classify and identify OOD objects.  It's a cool method, and one we might be able to use in a "how to use the minimun amount of labels to best improve model performance" sort of way.

Applications
------------
`Do grant proposal texts matter for funding decisions? A field experiment <https://link.springer.com/article/10.1007/s11192-024-04968-7>`_
    A dutch study finds that an abstract and CV hold as much weight as a full proposal. Your representation, connections, and elevator pitch are what matter.  I wonder if this generalizes to other countries/institutions?

`Bye-Bye, Bye Advantage: Estimating the competitive impact of rest differential in the National Football League <https://arxiv.org/pdf/2408.10867>`_
    Important paper for all you fantasy football fans. Employs a Bayesian State Space model to model investigate the effects of having a bye week on a teams performance.  Finds a rather strong effect before the 2011 Collective Bargaining Agreement, but much less of an effect afterwards.

New Models
--------
`Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma <https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/>`_
    Google adds three new additions to the Gemma 2B family.  They claim its the best thing on the market, etc etc.  `Lab report <https://arxiv.org/pdf/2408.00118>`_

`Apple Intelligence Foundation Language Models <https://arxiv.org/pdf/2407.21075>`_
    Apple's lab report on its foundation models.  Probably something interesting here if you want to read it.

`Imagen 3 <https://arxiv.org/pdf/2408.07009>`_
    Text to image generation diffusion model from Google.  Maybe there's a way to do synthetic data generation with this?

`LLaVA-OneVision: Easy Visual Task Transfer <https://arxiv.org/pdf/2408.03326>`_
    ByteDance releases a family of open LLMs that "push the performance boundaries" in some computer vision tasks.  Using anything released by ByteDane is presumably a hard no for government work, but it comes with a blog detailing development that might be worth a read.

`Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model <https://arxiv.org/pdf/2408.11039>`_
    Meta's new multi-model foundation model.  Can take text and images as part of the same input, as well as generating images.  Can handle complex(ish) instructions for image editting.

`Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models <https://arxiv.org/pdf/2408.10189>`_
    Figures out a way to distill a tranformer down to a SSM model.  Actually seems like a kind of cool process.Claims to be the best of its kind on the open source market, etc etc

Lunch and Learn
---------------
2024-08-06
    `Large Language Monkeys: Scaling Inference Compute with Repeated Sampling <https://arxiv.org/pdf/2407.21787>`_
    Generating accurate answers is hard, but verifying an answer is (sometimes) easy.  If you are living in a world where verifying an answer is easy, you can have an LLM generate a ton of answers and find th correct one.  Greatly improves performance.
