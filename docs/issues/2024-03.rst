
2024-03
=======

LLMs
----
`SWARMBRAIN: EMBODIED AGENT FOR REAL-TIME STRATEGY GAME STARCRAFT II VIA LARGE LANGUAGE MODELS <https://arxiv.org/pdf/2401.17749.pdf>`_
    LLMs play SC2.  Two stage model: (1) is an "Overmind" where multiple LLMs are blended together to create macro strategy in the same way the Zerg are supposed to function and (2) a "Swarm ReflexNet" which controls individual units.  Cool way of setting up an agent, and can beat the SC2 computer set to "Hard" model 76% of the time.  I'm not sure how impressive that is.  

`LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law <https://arxiv.org/pdf/2402.00795.pdf>`_
    LLMs can, when given a time series representing some physical thing generated by a dynamical system (e.g. double pendulum), learn the underlying physical dynamics well enough to predict the future of the time series.  I'm kind of unclear on how much "learning underlying principles" vs "predicting future of time series" is going on here.

`POKELLMON: A Human-Parity Agent for Pokemon Battles with Large Language Models <https://arxiv.org/pdf/2402.01118.pdf>`_
    LLMs play Pokemon Showdown.  Has a couple of techniques to bring in outside knowledge to keep the LLM in line.  Gets about a 50% winrate on the pokemon showdown ladder.  Given that pokemon showdown uses ELO this is going to be expected unless its either the worst or best player online - the actual ELO values is more important.

`SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures <https://arxiv.org/pdf/2402.03620.pdf>`_
    Deepmind.  Proposes SELF-DISCOVER, a method to get LLMs to reason which outperforms Chain-of-Thought.  For a given task, it first proposes a reasoning structure appropriate to that task and then uses the structure to solve the task.  Performs best on tasks requiring world knowledge instead of algorithmic tasks such as MATH.  Might be something we can implement.

`Secret Collusion Among Generative AI Agents <https://arxiv.org/pdf/2402.07510.pdf>`_
    Deepmind.  Letting LLMs talk to each other in a simulated environment is a popular research technique - but, what happens if the LLMs secretly talk/collude with each other without letting us know?  Currently not a problem, but GPT-4 is getting there

`V-STaR: Training Verifiers for Self-Taught Reasoners <https://arxiv.org/pdf/2402.06457.pdf>`_
    Deepmind and Microsoft.  LLMs do reasoning.  Let the LLM generate a bunch of solutions to a problem and have a verifier that picks the right one.  This approach trains the verifier on incorrect as well as correct approaches, which seems to improve performance.

`Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents <https://arxiv.org/pdf/2402.12327.pdf>`_
    If you put LLMs in a room and let them talk to each other in a multi-agent setting, they will develop collaborative strategies wiithout explicit instructions to do so.  The paper claims that this means that they can mimic human behaviors, in part because its a spontaneous development, but maybe it just means that there's some amount of game theory/the simulation scenarios used in the training data?

`Gemma: Introducing new state-of-the-art open models <https://blog.google/technology/developers/gemma-open-models/>`_
    Google's releasing a new open LLM.  Has 2B and 7B model weights available for download, claims state-of-the-art performance, etc.  I'm not sure if they're letting anyone use it for free, but it sort of seems like it?

`COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING <https://arxiv.org/pdf/2402.14020.pdf>`_
    A sequence of random chinese characters can get LLAMA to rickroll you, and other adventures in jailbreaking LLMs.  This paper argues the problem is much broader, and much worse, than we thought.  I recommend checking some of these out.

`OmniPred: Language Models as Universal Regressors <https://arxiv.org/pdf/2402.14547.pdf>`_
    Deepmind.  LLMs as universal regressors over any (x,y) dataset.  The advantage seems to be that it sweeps a lot of the encoding/objective stuff under the hood, but I'm not sure this is a good thing - you should probably be thinking about those instead of letting the computer do it.  Worth reading.

`Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs <2402.14740.pdf (arxiv.org)>`_
    Finetuning LLMs to give desired behaviors has traditionally been though of as a difficult reinforcement learning problem (?).  This paper argues that the reinforcement learning is actually quite simple and can proceed using some approaches straight ouf of the 90s.

`The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits <https://arxiv.org/pdf/2402.17764.pdf>`_
    LLMs getting up and running on 1.58 bits ({-1,0,1}) to increase computational performance.  Claims to be as effective as 16 bit transformers.  If this is true, I suppose the takeaway is that the extra bits don't add much?

Autonomy
--------
`Grandmaster-Level Chess Without Search <https://arxiv.org/pdf/2402.04494.pdf>`_
    Deepmind.  A decision transformer (I think?) plays chess without explicit heuristics or move search.  It does pretty well! - but calling it grandmaster level might be stretch since it only cross that threshold in blitz against humans, a situation which maximizes the computer's advantage

`Deep Learning Based Situation Awareness for Multiple Missiles Evasion <https://arxiv.org/pdf/2402.10101.pdf>`_
    What do you do when your drone is getting shot at by multiple missiles?  This paper uses deep learning to learn what is happening and guide decision making.

`Contingency Planning Using Bi-level Markov Decision Processes for Space Missions <https://arxiv.org/pdf/2402.16342.pdf>`_
    NASA talks about how the autonomous methods they're using to manage their rovers, using VIPER as an example.

Theory
------
`Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise <2402.01567.pdf (arxiv.org)>`_
    Recasts optimizers as online learners and finds that " Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL)."  This paper is worth reading if you want to understand how Adam works.

`The boundary of neural network trainability is fractal <2402.06184.pdf (arxiv.org)>`_
    Have you ever tried to optimze hyperparameters in a neural net?  Well, bad news - the boundary between convergent and divergent zones are fractal.  Maybe don't go looking for bright spots on the boundary.  Worth a look just for the pretty gifs.

`On Limitations of the Transformer Architecture <https://arxiv.org/pdf/2402.08164.pdf>`_
    Hallucinations happen because transformers have difficulty composing functions (e.g.  birthday of Chopin's father) if the domain is large enough, though this persists in small domains as well.  This is why mathematical tasks that are compositions can pose difficulties.  Worth a read.

`Chain of Thought Empowers Transformers to Solve Inherently Serial Problems <https://arxiv.org/pdf/2402.12875.pdf>`_
    Chain-of-thought helps LLMS, but why?  This paper provides some theoretical bounds and argues that it helps with serial tasks.  That sort of seemed intuitively obvious, but I guess it's good to get it backed up.  

Doctrinaire
-----------
`Shrub of a thousand faces: an individual segmentation from satellite images using deep learning <https://arxiv.org/pdf/2401.17985.pdf>`_
    Doctrinaire frooom spaaaace but for shrubs.  Somehow they have better quality data for shrubs than we have for MAGI?  Integrates on the ground data and spatial data, seems to develop its own architecture for detecting/segmenting juniper shrubs.  

`Tiered approach for rapid damage characterisation of infrastructure enabled by remote sensing and deep learning technologies <https://arxiv.org/ftp/arxiv/papers/2401/2401.17759.pdf>`_
    Assess infrastructure damage from space using a three-tiered approach.  Applied to a case study in Ukraine.  Seems like a direction we could take MAGI in if there's interest.

`ON THE MODELLING OF SHIP WAKES IN S-BAND SAR IMAGES AND AN APPLICATION TO SHIP IDENTIFICATION <https://arxiv.org/pdf/2402.04066.pdf>`_
    Develops a simulator for S-Band SAR data to simulate ships wakes.  Train a deep learning model on this data for ship classification.  The synthetic to real data pipeline somewhat indicates that this is something that we could implement, whether in this setting or a different one. 

`Arbitrary Scale Super-Resolution Assisted Lunar Crater Detection in Satellite Images <https://arxiv.org/pdf/2402.05068.pdf>`_
    Up-scale low resolution images for crater detection on the moon.  Has two subcomponents - one upscales (at multiple scales!) and one detects.  Seems interesting, and maybe applicable to work where resolution is low (eg MAGI) but I'm not sure how it increases the amount of information in the image?

`Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment <https://arxiv.org/pdf/2402.09816.pdf>`_
    CLIP for remote sensing. Finetunes CLIP, transforms info from pictures (multiple modalities?) into CLIP space to improve perfromance.  Morally similar to Doctrinaire.  Goes for scene classification instead of object detection/classificaiton. Might be worth exploring something like this

`Delving into Dark Regions for Robust Shadow Detection <https://arxiv.org/pdf/2402.13631.pdf>`_
    Shadow segmentation in fairly high resolution imagery.  Two-stage process, one which looks at the whole image and picks things, and a second which does shadow analysis locally.  This makes sense, since shadows can have different values in different regions.

`BENCHCLOUDVISION: A BENCHMARK ANALYSIS OF DEEP LEARNING APPROACHES FOR CLOUD DETECTION AND SEGMENTATION IN REMOTE SENSING IMAGERY <https://arxiv.org/pdf/2402.13918.pdf>`_
    Compares a bunch of methods for cloud segmentation on landsat and sentinel data.  Very relevant tool to have for MAGI or other remote sensing projects.

`Intelligent Known and Novel Aircraft Recognition - A Shift from Classification to Similarity Learning for Combat Identification <https://arxiv.org/pdf/2402.16486.pdf>`_
    Overhead ATR for combat identification of airplanes from Pakistani and Saudi Arabian researchers.  Uses an embedder to embed input images into some space, and then uses metrics inside of this space to do classification/novel aircraft identification.  Cool approach, though somewhat uninterpretable - this is the same problem as the October demo for MAGI, but different on the technical details.

Knowledge Graphs
----------------
`SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph <https://arxiv.org/pdf/2402.04627.pdf>`_
    How to turn natural language questions into SPARQL queries for use in knowledge graphs?  Could be useful for any of our KG projects.

Fusion
------
`INTERPRETABLE MULTI-SOURCE DATA FUSION THROUGH LATENT VARIABLE GAUSSIAN PROCESS <https://arxiv.org/pdf/2402.04146.pdf>`_
    GE Aeorspace Research presents a frameowrk for multi-source data fusion using latent variable Gaussian Processes.  They show off a bit on simulated examples and on "Thermal Aging Behavior of FeCrAl Alloys" and "Magnetic Behavior of SmCoFe Alloys".  Seems somewhat regression based - not sure how to apply to our work but seems cool.   

FPGA
----
`Accelerating Local Laplacian Filters on FPGAs <https://arxiv.org/pdf/2402.12407.pdf>`_
    Faster way to do things on FPGAs.  Out of my wheelhouse, but cogent for CoVar.

Stats
-----
`Diffusive Gibbs Sampling <https://arxiv.org/pdf/2402.03008.pdf>`_
    Gibbs sampling often has trouble with bimodal (or multimodal) posterior distributions.  Diffusive Gibbs sampling gets around this by leveraging stuff from diffusion models: "auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces"

`IMPROVED PREDICTION OF FUTURE USER ACTIVITY IN ONLINE A/B TESTING <https://arxiv.org/pdf/2402.03231.pdf>`_
    Develops a novel Bayesian nonparametric method to estimate quantity of new customers and number of times they will be observed in A/B testing environments.  Inference is done via empirical Bayes.  Appears to outperform competitors.

`Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits <https://arxiv.org/pdf/2402.05878.pdf>`_
    Deepmind. New strategy for fixed-budget Bayesian multi-arm bandits.  This is sort of what we want to do to dynamically jump between CAD models in doctrinaire-stuff, so worth keeping an eye on.  

`Horseshoe Priors for Sparse Dirichlet-Multinomial Models <https://arxiv.org/pdf/2402.09583.pdf>`_
    Polson has another paper in the "weird and novel priors for Dirichlet distributions" category.  Looks to be able to handle sparse count data pretty well, which was a large limitaiton of Dirichlet-multinomial models.

`Allowing Growing Dimensional Binary Outcomes via the Multivariate Probit Indian Buffet Process <https://arxiv.org/pdf/2402.13384.pdf>`_
    David Dunson is proposing a novel Bayesian nonparametric model.  Used for modelling presence of large amounts of species in ecology studies - we might be able to massage this into something relevant for ODIN, ie differing amounts of units, or for ATR in modelling likely presence given detected presence.

`Estimating Unknown Population Sizes Using the Hypergeometric Distribution <https://arxiv.org/pdf/2402.14220.pdf>`_
    How to estimate total number of things given a set of observations? Not terribly complicated stats, but develops some hypergeometric distribution methods.  Could be useful for guessing distribution of adversary forces given some observatoins.

`Information-Theoretic Safe Bayesian Optimization <https://arxiv.org/pdf/2402.15347.pdf>`_
    How to do Baysian Optimization where there's some unknown "safe zone" you can't enter for safety reasons.  Introduces latent variables to indicate whether you're safe.  Applications to autonomy?

`SEQUENTIAL DESIGN FOR SURROGATE MODELING IN BAYESIAN INVERSE PROBLEMS <https://arxiv.org/pdf/2402.16520.pdf>`_
    Basically what it says on the tin.  The more cogent part for CoVar is how they do their sequential design - this feels morally similar to, say, how you'd go about making a drone select the next best view.

Applications
---------

`Estimating individual contributions to team success in women’s college volleyball <https://arxiv.org/pdf/2402.01083.pdf>`_
    Models individual performance in the 2022 NCAA womens basketball season using a Markov Chain to simulate the progression of a game and a generalized linear mixed effects model to model individual contributions.  Seems to be a step forward for the field. 

`A Virtual Solar Wind Monitor for Mars with Uncertainty Quantification using Gaussian Processes <https://arxiv.org/pdf/2402.01932.pdf>`_
    Uses Gaussian Process Regression to esitmate solar winds from the MAVEN mission.  Sparse spatiotemporal data.  Not exactly a novel method, but a cool application.

`The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer <https://arxiv.org/ftp/arxiv/papers/2402/2402.04898.pdf>`_
    When should soccer players play and when should they sit?  This paper implements a Markov Decision Process which balances risk of injury vs win probability for all of its players.  Cool stats, but maybe there's some ethical questions floating around here about what to leave to computers

Position Papers
---------------
`Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI <https://arxiv.org/pdf/2402.00809.pdf>`_
    20(ish) authors, including Yee Whye Teh, think that Bayesian deep learning has great potential to solve a diverse array of problems, particularly in combination with foundation models.  Seems more like a "work needs to be done, but if its done there's a lot of potential" than "this is ready to go off the shelf".  Worth keeping an eye on the field.

`Mission Critical – Satellite Data is a Distinct Modality in Machine Learning <Mission Critical – Satellite Data is a Distinct Modality in Machine Learning (arxiv.org)>`_
    Position paper with some names from big universities/companies arguing that satellite data is a domain unto itself deserving of its own techniques and methods.  This tracks with what we've been doing on MAGI, but maybe points to growing interest from academia/industry

`Position Paper: Challenges and Opportunities in Topological Deep Learning <https://arxiv.org/pdf/2402.08871.pdf>`_
    Position paper on Topological Deep Learning.  Could be useful for anything involving CAD models/object estimation.  Good resource if you're interested in the field.
    
Datasets
---------
`Vehicle Perception from Satellite <https://arxiv.org/pdf/2402.00703.pdf>`_
    New dataset for vehicle detection from satellites just dropped.  408 videos with 9296 images for a total of 128,801 vehicles.  

`UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery <2402.05773.pdf (arxiv.org)>`_
    Raindrops stuck on your UAV's camera?  This paper has a method to remove it - and a public dataset.  Seems to be mostly synthetic. 

`Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning  <2402.05281.pdf (arxiv.org)>`_
    Propose a deep-learning model to simulate effects of underwater imagery.  Basically a "filter" to add to existing imagery.  Has a publicly available dataset.

`MAJOR TOM: EXPANDABLE DATASETS FOR EARTH OBSERVATION <https://arxiv.org/pdf/2402.12095.pdf>`_
    European Space Agency. A framework for molding multiple EO remote sensing datasets together.  Kind of similar to cvr grid.  Will release when paper is accepted, they promise.

`Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment <2402.12320.pdf (arxiv.org)>`_
    Out of Mizzou - are these the UAS people?  Proposes using a "landmark anchor node" to locate soldiers on the battlefield.  Has a dataset and a method.  Not publicly available?

`Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset <https://arxiv.org/pdf/2402.14804.pdf>`_
    LLMs were getting too good at existing math datasets, so these authors proposed a new one which includes more diverse problem types.  LLMs do a lot worse on this one.

`MTARSI <https://github.com/PremaKathiresanVasagam/MTARSI>`_
    Overhead ATR pictures of planes
