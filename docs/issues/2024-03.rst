
2024-03
=======

LLMs
----
`SWARMBRAIN: EMBODIED AGENT FOR REAL-TIME STRATEGY GAME STARCRAFT II VIA LARGE LANGUAGE MODELS <https://arxiv.org/pdf/2401.17749.pdf>`_
    LLMs play SC2.  Two stage model: (1) is an "Overmind" where multiple LLMs are blended together to create macro strategy in the same way the Zerg are supposed to function and (2) a "Swarm ReflexNet" which controls individual units.  Cool way of setting up an agent, and can beat the SC2 computer set to "Hard" model 76% of the time.  I'm not sure how impressive that is.  

`LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law <https://arxiv.org/pdf/2402.00795.pdf>`_
    LLMs can, when given a time series representing some physical thing generated by a dynamical system (e.g. double pendulum), learn the underlying physical dynamics well enough to predict the future of the time series.  I'm kind of unclear on how much "learning underlying principles" vs "predicting future of time series" is going on here.

`POKELLMON: A Human-Parity Agent for Pokemon Battles with Large Language Models <https://arxiv.org/pdf/2402.01118.pdf>`_
    LLMs play Pokemon Showdown.  Has a couple of techniques to bring in outside knowledge to keep the LLM in line.  Gets about a 50% winrate on the pokemon showdown ladder.  Given that pokemon showdown uses ELO this is going to be expected unless its either the worst or best player online - the actual ELO values is more important.

`SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures <https://arxiv.org/pdf/2402.03620.pdf>`_
    Deepmind.  Proposes SELF-DISCOVER, a method to get LLMs to reason which outperforms Chain-of-Thought.  For a given task, it first proposes a reasoning structure appropriate to that task and then uses the structure to solve the task.  Performs best on tasks requiring world knowledge instead of algorithmic tasks such as MATH.  Might be something we can implement.

`Secret Collusion Among Generative AI Agents <https://arxiv.org/pdf/2402.07510.pdf>`_
    Deepmind.  Letting LLMs talk to each other in a simulated environment is a popular research technique - but, what happens if the LLMs secretly talk/collude with each other without letting us know?  Currently not a problem, but GPT-4 is getting there

`V-STaR: Training Verifiers for Self-Taught Reasoners <https://arxiv.org/pdf/2402.06457.pdf>`_
    Deepmind and Microsoft.  LLMs do reasoning.  Let the LLM generate a bunch of solutions to a problem and have a verifier that picks the right one.  This approach trains the verifier on incorrect as well as correct approaches, which seems to improve performance.

Reinforcement Learning
----------------------
`Grandmaster-Level Chess Without Search <https://arxiv.org/pdf/2402.04494.pdf>`_
    Deepmind.  A decision transformer (I think?) plays chess without explicit heuristics or move search.  It does pretty well! - but calling it grandmaster level might be stretch since it only cross that threshold in blitz against humans, a situation which maximizes the computer's advantage

Theory
------
`Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise <2402.01567.pdf (arxiv.org)>`_
    Recasts optimizers as online learners and finds that " Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL)."  This paper is worth reading if you want to understand how Adam works.

`The boundary of neural network trainability is fractal <2402.06184.pdf (arxiv.org)>`_
    Have you ever tried to optimze hyperparameters in a neural net?  Well, bad news - the boundary between convergent and divergent zones are fractal.  Maybe don't go looking for bright spots on the boundary.  Worth a look just for the pretty gifs.

`On Limitations of the Transformer Architecture <https://arxiv.org/pdf/2402.08164.pdf>`_
    Hallucinations happen because transformers have difficulty composing functions (e.g.  birthday of Chopin's father) if the domain is large enough, though this persists in small domains as well.  This is why mathematical tasks that are compositions can pose difficulties.  Worth a read.

Remote Sensing
-----------
`Shrub of a thousand faces: an individual segmentation from satellite images using deep learning <https://arxiv.org/pdf/2401.17985.pdf>`_
    Doctrinaire frooom spaaaace but for shrubs.  Somehow they have better quality data for shrubs than we have for MAGI?  Integrates on the ground data and spatial data, seems to develop its own architecture for detecting/segmenting juniper shrubs.  

`Tiered approach for rapid damage characterisation of infrastructure enabled by remote sensing and deep learning technologies <https://arxiv.org/ftp/arxiv/papers/2401/2401.17759.pdf>`_
    Assess infrastructure damage from space using a three-tiered approach.  Applied to a case study in Ukraine.  Seems like a direction we could take MAGI in if there's interest.

`ON THE MODELLING OF SHIP WAKES IN S-BAND SAR IMAGES AND AN APPLICATION TO SHIP IDENTIFICATION <https://arxiv.org/pdf/2402.04066.pdf>`_
    Develops a simulator for S-Band SAR data to simulate ships wakes.  Train a deep learning model on this data for ship classification.  The synthetic to real data pipeline somewhat indicates that this is something that we could implement, whether in this setting or a different one. 

`Arbitrary Scale Super-Resolution Assisted Lunar Crater Detection in Satellite Images <https://arxiv.org/pdf/2402.05068.pdf>`_
    Up-scale low resolution images for crater detection on the moon.  Has two subcomponents - one upscales (at multiple scales!) and one detects.  Seems interesting, and maybe applicable to work where resolution is low (eg MAGI) but I'm not sure how it increases the amount of information in the image?


Knowledge Graphs
----------------
`SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph <https://arxiv.org/pdf/2402.04627.pdf>`_
    How to turn natural language questions into SPARQL queries for use in knowledge graphs?  Could be useful for any of our KG projects.

Fusion
------
`INTERPRETABLE MULTI-SOURCE DATA FUSION THROUGH LATENT VARIABLE GAUSSIAN PROCESS <https://arxiv.org/pdf/2402.04146.pdf>`_
    GE Aeorspace Research presents a frameowrk for multi-source data fusion using latent variable Gaussian Processes.  They show off a bit on simulated examples and on "Thermal Aging Behavior of FeCrAl Alloys" and "Magnetic Behavior of SmCoFe Alloys".  Seems somewhat regression based - not sure how to apply to our work but seems cool.   

Stats
-----
`Diffusive Gibbs Sampling <https://arxiv.org/pdf/2402.03008.pdf>`_
    Gibbs sampling often has trouble with bimodal (or multimodal) posterior distributions.  Diffusive Gibbs sampling gets around this by leveraging stuff from diffusion models: "auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces"

`IMPROVED PREDICTION OF FUTURE USER ACTIVITY IN ONLINE A/B TESTING <https://arxiv.org/pdf/2402.03231.pdf>`_
    Develops a novel Bayesian nonparametric method to estimate quantity of new customers and number of times they will be observed in A/B testing environments.  Inference is done via empirical Bayes.  Appears to outperform competitors.

`Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits <https://arxiv.org/pdf/2402.05878.pdf>`_
    Deepmind. New strategy for fixed-budget Bayesian multi-arm bandits.  This is sort of what we want to do to dynamically jump between CAD models in doctrinaire-stuff, so worth keeping an eye on.  

Applications
---------

`Estimating individual contributions to team success in women’s college volleyball <https://arxiv.org/pdf/2402.01083.pdf>`_
    Models individual performance in the 2022 NCAA womens basketball season using a Markov Chain to simulate the progression of a game and a generalized linear mixed effects model to model individual contributions.  Seems to be a step forward for the field. 

`A Virtual Solar Wind Monitor for Mars with Uncertainty Quantification using Gaussian Processes <https://arxiv.org/pdf/2402.01932.pdf>`_
    Uses Gaussian Process Regression to esitmate solar winds from the MAVEN mission.  Sparse spatiotemporal data.  Not exactly a novel method, but a cool application.

`The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer <https://arxiv.org/ftp/arxiv/papers/2402/2402.04898.pdf>`_
    When should soccer players play and when should they sit?  This paper implements a Markov Decision Process which balances risk of injury vs win probability for all of its players.  Cool stats, but maybe there's some ethical questions floating around here about what to leave to computers

Position Papers
---------------
`Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI <https://arxiv.org/pdf/2402.00809.pdf>`_
    20(ish) authors, including Yee Whye Teh, think that Bayesian deep learning has great potential to solve a diverse array of problems, particularly in combination with foundation models.  Seems more like a "work needs to be done, but if its done there's a lot of potential" than "this is ready to go off the shelf".  Worth keeping an eye on the field.

`Mission Critical – Satellite Data is a Distinct Modality in Machine Learning <Mission Critical – Satellite Data is a Distinct Modality in Machine Learning (arxiv.org)>`_
    Position paper with some names from big universities/companies arguing that satellite data is a domain unto itself deserving of its own techniques and methods.  This tracks with what we've been doing on MAGI, but maybe points to growing interest from academia/industry

`Position Paper: Challenges and Opportunities in Topological Deep Learning <https://arxiv.org/pdf/2402.08871.pdf>`_
    Position paper on Topological Deep Learning.  Could be useful for anything involving CAD models/object estimation.  Good resource if you're interested in the field.
    
Datasets
---------
`Vehicle Perception from Satellite <https://arxiv.org/pdf/2402.00703.pdf>`_
    New dataset for vehicle detection from satellites just dropped.  408 videos with 9296 images for a total of 128,801 vehicles.  

`UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery <2402.05773.pdf (arxiv.org)>`_
    Raindrops stuck on your UAV's camera?  This paper has a method to remove it - and a public dataset.  Seems to be mostly synthetic. 

`Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning  <2402.05281.pdf (arxiv.org)>`_
    Propose a deep-learning model to simulate effects of underwater imagery.  Basically a "filter" to add to existing imagery.  Has a publicly available dataset.